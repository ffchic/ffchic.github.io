<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#ff8c00"><meta name="author" content="天 听"><meta name="copyright" content="天 听"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>爬虫的相关知识 | Salmon</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.22/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_ed8vp4atwoj.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link id="light-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.20.0/themes/prism-solarizedlight.css" media="(prefers-color-scheme: light)"><link id="dark-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.20.0/themes/prism-solarizedlight.css" media="(prefers-color-scheme: dark)"><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#ff8c00"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"godhearing.cn","root":"/","title":["天","听","の","blog"],"version":"1.4.0","mode":"time","copycode":true,"page":{"isPost":true},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"/data/sentences.json"},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["255, 222, 173","255, 250, 240","240, 255, 255\t"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="alternate" href="/atom.xml" title="Salmon" type="application/atom+xml"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta name="description" content="1.虚拟环境 虚拟环境就是一个隔离的python环境，不同的项目应该使用不同的虚拟环境(可以使用同一个虚拟环境)   虚拟环境不会导致环境之间的污染  1.1  虚拟环境管理模块virtualenvwrapper 安装 pip install virtualenvwrapper-win virtualenvwrapper的使用 查看所有虚拟环境：lsvirtualenv 创建虚拟环境：mkvirt">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫的相关知识">
<meta property="og:url" content="https://godhearing.cn/2019/08/01/pa-chong-de-xiang-guan-zhi-shi/index.html">
<meta property="og:site_name" content="Salmon">
<meta property="og:description" content="1.虚拟环境 虚拟环境就是一个隔离的python环境，不同的项目应该使用不同的虚拟环境(可以使用同一个虚拟环境)   虚拟环境不会导致环境之间的污染  1.1  虚拟环境管理模块virtualenvwrapper 安装 pip install virtualenvwrapper-win virtualenvwrapper的使用 查看所有虚拟环境：lsvirtualenv 创建虚拟环境：mkvirt">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-08-01T14:32:00.000Z">
<meta property="article:modified_time" content="2019-08-01T14:32:00.000Z">
<meta property="article:author" content="天 听">
<meta property="article:tag" content="python">
<meta property="article:tag" content="spider">
<meta name="twitter:card" content="summary"><script src="/js/ui/mode.js"></script>
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-solarizedlight.css" type="text/css"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><canvas id="trianglifyContainer"></canvas><script defer src="https://cdn.jsdelivr.net/npm/trianglify@4/dist/trianglify.bundle.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  const pattern = trianglify({
    width: 800,
    height: 600,
    cellSize: 65,
    palette: ["YlGnBu", "GnBu", "Purples", "Blues","Reds","Oranges"],
  });
  const canvasOpts = {
    applyCssScaling: false
  }
  document.body.appendChild(pattern.toCanvas(trianglifyContainer, canvasOpts));
});</script><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="天 听"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="天 听"></a><div class="site-author-name"><a href="/about/">天 听</a></div><a class="site-name" href="/about/site.html">Salmon</a><sub class="site-subtitle">天听的小窝</sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">130</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">24</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">40</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://www.baidu.com/" title="通向一个万事都能解决的地方"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Salmon-x" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=247102977" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://twitter.com/godhearing1" title="Twitter" target="_blank" style="color:#1da1f2"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-twitter-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://haoye0822@gmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="最喜欢的女孩子们" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="toc-number">1.</span> <span class="toc-text">1.虚拟环境</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E6%A8%A1%E5%9D%97"><span class="toc-number">1.0.1.</span> <span class="toc-text">1.1  虚拟环境管理模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E7%8E%AF%E5%A2%83%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">1.0.2.</span> <span class="toc-text">1.2  环境一致性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%9F%A5%E7%9C%8B%E5%8C%85%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF"><span class="toc-number">1.0.3.</span> <span class="toc-text">1.3  查看包的详细信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E6%89%93%E5%8C%85"><span class="toc-number">1.0.4.</span> <span class="toc-text">1.4  打包</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%88%AC%E8%99%AB"><span class="toc-number">2.</span> <span class="toc-text">2.爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E7%88%AC%E8%99%AB%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">2.0.1.</span> <span class="toc-text">2.1  爬虫的概念</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-1-pyinstaller"><span class="toc-number">2.0.1.1.</span> <span class="toc-text">2.1.1  pyinstaller</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB"><span class="toc-number">2.0.2.</span> <span class="toc-text">2.2  通用爬虫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-robots%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.0.3.</span> <span class="toc-text">2.3  robots协议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB"><span class="toc-number">2.0.4.</span> <span class="toc-text">2.4  聚焦爬虫</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-requests%E6%A8%A1%E5%9D%97"><span class="toc-number">3.</span> <span class="toc-text">2.5  requests模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-OSI%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.0.1.</span> <span class="toc-text">2.6  OSI七层模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-TCP-IP%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.0.2.</span> <span class="toc-text">2.7  TCP&#x2F;IP五层模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-8-TCP%E5%92%8CUDP"><span class="toc-number">3.0.3.</span> <span class="toc-text">2.8 TCP和UDP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-ARP%E5%8D%8F%E8%AE%AE"><span class="toc-number">3.0.4.</span> <span class="toc-text">2.9  ARP协议</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-9-1-ssh"><span class="toc-number">3.0.4.1.</span> <span class="toc-text">2.9.1  ssh</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-9-2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%9B%E5%BB%BA%E7%9A%84%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3"><span class="toc-number">3.0.4.2.</span> <span class="toc-text">2.9.2  服务器创建的默认端口</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-9-3-http%E4%B8%8EHTTPS%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">3.0.4.3.</span> <span class="toc-text">2.9.3  http与HTTPS协议的区别</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%AF%B7%E6%B1%82"><span class="toc-number">4.</span> <span class="toc-text">3.请求</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E8%AF%B7%E6%B1%82%E8%BF%87%E7%A8%8B"><span class="toc-number">4.0.1.</span> <span class="toc-text">3.1  请求过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95"><span class="toc-number">4.0.2.</span> <span class="toc-text">3.2  请求方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E8%AF%B7%E6%B1%82%E4%BD%93"><span class="toc-number">4.0.3.</span> <span class="toc-text">3.3  请求头和请求体</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E5%8F%8D%E7%88%AC%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8F%8D%E5%8F%8D%E7%88%AC%E7%AD%96%E7%95%A5"><span class="toc-number">4.0.4.</span> <span class="toc-text">3.4  反爬机制与反反爬策略</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%93%8D%E5%BA%94"><span class="toc-number">5.</span> <span class="toc-text">4.响应</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8A%B6%E6%80%81%E7%A0%81"><span class="toc-number">5.0.1.</span> <span class="toc-text">常见的状态码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E5%93%8D%E5%BA%94%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E5%BD%A2%E5%BC%8F"><span class="toc-number">5.0.2.</span> <span class="toc-text">4.1  响应数据的几种形式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-uuid"><span class="toc-number">5.0.3.</span> <span class="toc-text">4.2  uuid</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">6.</span> <span class="toc-text">5.正则表达式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-%E5%85%83%E5%AD%97%E7%AC%A6"><span class="toc-number">6.0.1.</span> <span class="toc-text">5.1  元字符</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-%E5%AD%97%E7%AC%A6%E7%BB%84"><span class="toc-number">6.0.2.</span> <span class="toc-text">5.2  字符组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-%E9%87%8F%E8%AF%8D"><span class="toc-number">6.0.3.</span> <span class="toc-text">5.3  量词</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-%E8%BE%B9%E7%95%8C%E4%BF%AE%E9%A5%B0"><span class="toc-number">6.0.4.</span> <span class="toc-text">5.4   边界修饰</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-%E5%88%86%E7%BB%84"><span class="toc-number">6.0.5.</span> <span class="toc-text">5.5   分组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-%E8%B4%AA%E5%A9%AA%E4%B8%8E%E9%9D%9E%E8%B4%AA%E5%A9%AA"><span class="toc-number">6.0.6.</span> <span class="toc-text">5.6  贪婪与非贪婪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-re"><span class="toc-number">6.0.7.</span> <span class="toc-text">5.7  re</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-requests%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">6. requests高阶应用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86"><span class="toc-number">7.0.1.</span> <span class="toc-text">6.1  文件处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-%E4%BC%9A%E8%AF%9D%E7%BB%B4%E6%8C%81"><span class="toc-number">7.0.2.</span> <span class="toc-text">6.2  会话维持</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-ssl%E8%AF%81%E4%B9%A6%E9%AA%8C%E8%AF%81"><span class="toc-number">7.0.3.</span> <span class="toc-text">6.3  ssl证书验证</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE"><span class="toc-number">7.0.4.</span> <span class="toc-text">6.4  代理设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-%E8%B6%85%E6%97%B6%E8%AE%BE%E7%BD%AE"><span class="toc-number">7.0.5.</span> <span class="toc-text">6.5  超时设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-6-UA%E6%A3%80%E6%B5%8B"><span class="toc-number">7.0.6.</span> <span class="toc-text">6.6  UA检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-7-cookie%E7%9A%84%E5%A4%84%E7%90%86-session"><span class="toc-number">7.0.7.</span> <span class="toc-text">6.7  cookie的处理(session)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-lxm%E5%BA%93"><span class="toc-number">8.</span> <span class="toc-text">7.lxm库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">8.0.1.</span> <span class="toc-text">7.1  xpath基本语法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-%E5%B1%9E%E6%80%A7%E5%8C%B9%E9%85%8D"><span class="toc-number">8.0.2.</span> <span class="toc-text">7.2  属性匹配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-%E6%8C%89%E5%BA%8F%E9%80%89%E6%8B%A9"><span class="toc-number">8.0.3.</span> <span class="toc-text">7.3  按序选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-4-%E6%B5%81%E7%A8%8B"><span class="toc-number">8.0.4.</span> <span class="toc-text">7.4  流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-5-%E8%A1%A5%E5%85%85"><span class="toc-number">8.0.5.</span> <span class="toc-text">7.5  补充</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%8A%A8%E6%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">9.</span> <span class="toc-text">8.动态数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-selenium"><span class="toc-number">9.0.1.</span> <span class="toc-text">8.1  selenium</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-selenium%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%93%8D%E4%BD%9C"><span class="toc-number">9.0.2.</span> <span class="toc-text">8.2  selenium安装与配置与操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#iframe%E6%A0%87%E7%AD%BE%E8%B7%B3%E8%BD%AC"><span class="toc-number">9.0.3.</span> <span class="toc-text">iframe标签跳转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-3-%E5%AD%90%E7%BA%A7%E9%A1%B5%E9%9D%A2%E5%92%8C%E7%88%B6%E7%BA%A7%E9%A1%B5%E9%9D%A2"><span class="toc-number">9.0.4.</span> <span class="toc-text">8.3  子级页面和父级页面</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-4-%E9%98%B2%E6%A3%80%E6%B5%8B"><span class="toc-number">9.0.5.</span> <span class="toc-text">8.4  防检测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB"><span class="toc-number">10.</span> <span class="toc-text">9.多线程爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C"><span class="toc-number">10.0.1.</span> <span class="toc-text">9.1  并发与并行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-%E7%A4%BA%E4%BE%8B"><span class="toc-number">10.0.2.</span> <span class="toc-text">9.2  示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%97%A0%E5%A4%B4%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8EBS4"><span class="toc-number">11.</span> <span class="toc-text">10.无头浏览器与BS4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-%E6%97%A0%E5%A4%B4%E6%B5%8F%E8%A7%88%E5%99%A8"><span class="toc-number">11.0.1.</span> <span class="toc-text">10.1  无头浏览器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-2-BS4%E8%AF%AD%E6%B3%95"><span class="toc-number">11.0.2.</span> <span class="toc-text">10.2  BS4语法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E5%BF%AB%E4%BB%A3%E7%90%86%E7%BD%91%E7%AB%99%E7%9A%84%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95"><span class="toc-number">12.</span> <span class="toc-text">11.快代理网站的模拟登录</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-1-%E7%AC%AC%E4%B8%89%E6%96%B9%E6%89%93%E7%A0%81%E5%B9%B3%E5%8F%B0"><span class="toc-number">12.0.1.</span> <span class="toc-text">11.1  第三方打码平台</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Scrapy%E6%A1%86%E6%9E%B6"><span class="toc-number">13.</span> <span class="toc-text">12.Scrapy框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-1-%E5%AE%89%E8%A3%85"><span class="toc-number">13.0.1.</span> <span class="toc-text">12.1  安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-number">13.0.2.</span> <span class="toc-text">12.2  创建项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-3-%E8%BF%90%E8%A1%8C%E9%A1%B9%E7%9B%AE"><span class="toc-number">13.0.3.</span> <span class="toc-text">12.3  运行项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-4-%E9%A1%B9%E7%9B%AE%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">13.0.4.</span> <span class="toc-text">12.4  项目基本架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-5-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-number">13.0.5.</span> <span class="toc-text">12.5  核心组件与数据流向</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">13.0.5.1.</span> <span class="toc-text">核心组件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-number">13.0.5.2.</span> <span class="toc-text">数据流向</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-6-%E7%BB%84%E4%BB%B6%E5%88%86%E6%9E%90"><span class="toc-number">13.0.6.</span> <span class="toc-text">12.6   组件分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE"><span class="toc-number">13.0.7.</span> <span class="toc-text">12.7  保存数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-8-%E7%AE%A1%E9%81%93-Item-Pipline-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">13.0.8.</span> <span class="toc-text">12.8  管道(Item Pipline)的使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-9-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="toc-number">13.0.9.</span> <span class="toc-text">12.9  整体流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-91-%E6%89%8B%E5%8A%A8%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="toc-number">13.0.10.</span> <span class="toc-text">12.91  手动发送请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-92-%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">13.0.11.</span> <span class="toc-text">12.92  中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AF%A1%E6%94%B9UA"><span class="toc-number">13.0.11.1.</span> <span class="toc-text">篡改UA</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AF%A1%E6%94%B9ip"><span class="toc-number">13.0.11.2.</span> <span class="toc-text">篡改ip</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#selenium%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">13.0.11.3.</span> <span class="toc-text">selenium的使用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-93-meta%E4%BC%A0%E5%80%BC"><span class="toc-number">13.0.12.</span> <span class="toc-text">12.93  meta传值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-94-%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">13.0.13.</span> <span class="toc-text">12.94   全站数据爬取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-95-%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-number">13.0.14.</span> <span class="toc-text">12.95  增量式爬虫</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">14.</span> <span class="toc-text">13.MongoDB数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#13-1-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C"><span class="toc-number">14.0.1.</span> <span class="toc-text">13.1  数据库操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-2-%E5%A2%9E%E5%8A%A0%E6%95%B0%E6%8D%AE"><span class="toc-number">14.0.2.</span> <span class="toc-text">13.2  增加数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-3-%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span class="toc-number">14.0.3.</span> <span class="toc-text">13.3  查询数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-4-%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE"><span class="toc-number">14.0.4.</span> <span class="toc-text">13.4  修改数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-5-%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE"><span class="toc-number">14.0.5.</span> <span class="toc-text">13.5  删除数据</span></a></li></ol></li></ol></li></ol></div></div></div><div class="tag-cloud"><div class="tag-cloud-tags"><a href="/tags/Book/" style="font-size: 21px; color: #80b45d">Book</a> <a href="/tags/Casbin/" style="font-size: 12px; color: #ffc20e">Casbin</a> <a href="/tags/Celery/" style="font-size: 16.5px; color: #bfbb36">Celery</a> <a href="/tags/Django/" style="font-size: 30px; color: #00a6ac">Django</a> <a href="/tags/Docker/" style="font-size: 25.5px; color: #40ad85">Docker</a></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://godhearing.cn/2019/08/01/pa-chong-de-xiang-guan-zhi-shi/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="天 听"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Salmon"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">爬虫的相关知识</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2019-08-01 22:32:00" itemprop="dateCreated datePublished" datetime="2019-08-01T22:32:00+08:00">2019-08-01</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/spider/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">spider</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/python/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">python</span></a><a class="tag" href="/tags/spider/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">spider</span></a></span></div><div class="post-author"><span class="author-name">天听</span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#ff8c00;"><h2 id="1-虚拟环境"><a href="#1-虚拟环境" class="headerlink" title="1.虚拟环境"></a>1.虚拟环境</h2><blockquote>
<p>虚拟环境就是一个隔离的python环境，不同的项目应该使用不同的虚拟环境(可以使用同一个虚拟环境)</p>
</blockquote>
<blockquote>
<p>虚拟环境不会导致环境之间的污染</p>
</blockquote>
<h4 id="1-1-虚拟环境管理模块"><a href="#1-1-虚拟环境管理模块" class="headerlink" title="1.1  虚拟环境管理模块"></a>1.1  虚拟环境管理模块</h4><p><code>virtualenvwrapper</code></p>
<p>安装</p>
<p><code>pip install virtualenvwrapper-win</code></p>
<p><code>virtualenvwrapper</code>的使用</p>
<p>查看所有虚拟环境：<code>lsvirtualenv</code></p>
<p>创建虚拟环境：<code>mkvirtualenv 环境名</code></p>
<p>激活虚拟环境：<code>workon 环境名</code></p>
<p>查看当前虚拟环境下的模块：</p>
<ol>
<li>进入当前虚拟环境</li>
<li>pip list</li>
</ol>
<p>退出虚拟环境:<code>deactivate</code></p>
<p>虚拟环境中安装模块:<code>pycharm</code>中,选中虚拟环境，然后添加模块,也可以通过<code>pip install</code>安装</p>
<p>删除虚拟环境：<code>rmvirtualenv 环境名</code></p>
<blockquote>
<p><code>pycharm</code>如果没有直接显示虚拟环境，则<br> settings中选解释器处<code>show all</code> ,加号添加，创建的虚拟环境在<br> <code>C:\Users\22742\Envs\</code>目录下，选中Scripts下的<code>python.exe</code></p>
</blockquote>
<h4 id="1-2-环境一致性"><a href="#1-2-环境一致性" class="headerlink" title="1.2  环境一致性"></a>1.2  环境一致性</h4><blockquote>
<p>保证开发与生产环境一致，需要将模块等同一致</p>
</blockquote>
<blockquote>
<p>在开发机的虚拟环境中，运行命令:<br>生成模块和其版本<code>pip freeze &gt; requirements.txt</code></p>
<p>将<code>requirements</code>中生成的模块版本进行安装<br><code>pip listall -r ./requirements.txt</code></p>
</blockquote>
<h4 id="1-3-查看包的详细信息"><a href="#1-3-查看包的详细信息" class="headerlink" title="1.3  查看包的详细信息"></a>1.3  查看包的详细信息</h4><p><code>pip show 包名</code></p>
<h4 id="1-4-打包"><a href="#1-4-打包" class="headerlink" title="1.4  打包"></a>1.4  打包</h4><p><code>pyinstall -F XXX.py</code></p>
<hr>
<h2 id="2-爬虫"><a href="#2-爬虫" class="headerlink" title="2.爬虫"></a>2.爬虫</h2><h4 id="2-1-爬虫的概念"><a href="#2-1-爬虫的概念" class="headerlink" title="2.1  爬虫的概念"></a>2.1  爬虫的概念</h4><blockquote>
<p>爬虫又称网页蜘蛛或者网页机器人</p>
<p>模拟人操作客户端，向服务器发起网络请求，抓取数据的自动化程序和脚本</p>
</blockquote>
<blockquote>
<p>通用爬虫是通过抓取数据实现检索服务</p>
</blockquote>
<blockquote>
<p>爬虫分为聚焦爬虫和通用爬虫</p>
<p>自动化，数据量较小时可以人工获取数据，但往往在公司中爬取的量都在百万级千万级，所以要程序自动化获取数据</p>
</blockquote>
<blockquote>
<p>B/S架构：<code>Browser</code>/<code>Server</code>，类似淘宝，没有第三方中转，客户端和服务器直接交互</p>
<p>C/S：<code>Client</code>/<code>server</code>，类似微信，将微信后端作为中转站，和其他人对话时，需要在中转站传话</p>
</blockquote>
<h5 id="2-1-1-pyinstaller"><a href="#2-1-1-pyinstaller" class="headerlink" title="2.1.1  pyinstaller"></a>2.1.1  <code>pyinstaller</code></h5><p><code>pyinstaller</code>可以将python文件编译成一个程序，类似go语言的编译</p>
<h4 id="2-2-通用爬虫"><a href="#2-2-通用爬虫" class="headerlink" title="2.2  通用爬虫"></a>2.2  通用爬虫</h4><p>百度，360，搜狐等搜索引擎</p>
<p>原理：</p>
<ol>
<li>抓取网页</li>
<li>采集数据</li>
<li>数据处理</li>
<li>提供检索服务</li>
</ol>
<p>通用爬虫抓取新网站的方式</p>
<ol>
<li>主动提交<code>url</code></li>
<li>设置友情连接</li>
<li>百度会和<code>DNS</code>服务商合作，抓取新网站</li>
</ol>
<p>检索排名:</p>
<ol>
<li>竞价排名</li>
<li>根据<code>PageRank</code>值，访问量、点击量   (<code>SEO</code>)</li>
</ol>
<h4 id="2-3-robots协议"><a href="#2-3-robots协议" class="headerlink" title="2.3  robots协议"></a>2.3  robots协议</h4><p><code>robots.txt</code>:如果不想让百度爬取，可以编写<code>robots.txt</code>,这个协议只是口头上的协议，自己写的爬虫程序<strong>不需要遵从</strong></p>
<h4 id="2-4-聚焦爬虫"><a href="#2-4-聚焦爬虫" class="headerlink" title="2.4  聚焦爬虫"></a>2.4  聚焦爬虫</h4><blockquote>
<p>根据特定的需求，抓取指定的数据</p>
</blockquote>
<p>思路：</p>
<blockquote>
<p>代替浏览器上网</p>
<ol>
<li><code>url</code>，发起请求，获取响应</li>
<li>解析内容，提取数据</li>
<li>将数据存储到本地，数据持久化</li>
</ol>
</blockquote>
<h2 id="2-5-requests模块"><a href="#2-5-requests模块" class="headerlink" title="2.5  requests模块"></a>2.5  requests模块</h2><pre class="language-python" data-language="python"><code class="language-python">#导包
import requests
url = '*****'
#res是获取的响应数据
res = requests.get(url)</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#响应数据的获取方式
1.文本形式：res.text
2.json形式：res.json()
3.流形式：res.content</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#数据持久化(mysql入库)
#1.导包
import pymysql
#2.创建链接
conn = pymysql.connect(host='127.0.0.1',port=3306,user='root',password='root',charset='utf8',database='***')
#3.创建游标
cursor = conn.cursor()
#4.构造sql语句
sql  = 'insert into *** values(数据)'
#5.执行sql语句
try:
    cursor.execute(sql)
    #提交事务
    conn.commit()
except Exception as e:
    print(e)
    #回滚
    conn.rollback()</code></pre>

<p><code>params</code>参数</p>
<blockquote>
<p>get方式传参的拼接，将参数拼接到目标<code>url</code>中</p>
</blockquote>
<h4 id="2-6-OSI七层模型"><a href="#2-6-OSI七层模型" class="headerlink" title="2.6  OSI七层模型"></a>2.6  <code>OSI</code>七层模型</h4><p>应用层：</p>
<pre class="language-none"><code class="language-none">https/http/ftp

http协议：明文传输，端口80
https协议：加密传输，端口443</code></pre>

<p>表示层</p>
<p>会话层</p>
<p>传输层：<code>UDP/TCP</code></p>
<p>网络层：<code>IP</code></p>
<p>数据链路层：<code>ARP</code></p>
<p>物理层：<strong>以太网协议</strong></p>
<h4 id="2-7-TCP-IP五层模型"><a href="#2-7-TCP-IP五层模型" class="headerlink" title="2.7  TCP/IP五层模型"></a>2.7  <code>TCP/IP</code>五层模型</h4><p>应用层：<code>https/http/ftp/ssh/Sftp/</code></p>
<p>传输层：<code>UDP/TCP</code></p>
<p>网络层：<code>IP</code></p>
<p>数据链路层：<code>ARP</code></p>
<p>物理层：<strong>以太网协议</strong></p>
<h4 id="2-8-TCP和UDP"><a href="#2-8-TCP和UDP" class="headerlink" title="2.8 TCP和UDP"></a>2.8 <code>TCP</code>和<code>UDP</code></h4><p>TCP协议是一种面向连接的，可靠的，基于字节流的传输通信协议</p>
<ol>
<li>有序性：数据包编号，判断数据包的正确次序</li>
<li>正确性：使用checksum函数检查数据包是否损坏，发送和接收时都会计算<strong>校验和</strong></li>
<li>可靠性：发送端由超时重发，并有确认机制识别错误和数据的丢失</li>
<li>可控性：<strong>滑动窗口协议</strong>与<strong>拥塞控制算法</strong>控制数据包的发送速度</li>
</ol>
<p><code>UDP</code>协议是用户数据报协议，面向无连接的传输层协议，传输<code>相对于TCP</code>来说，不可靠</p>
<ol>
<li>无连接：数据可能丢失或损坏</li>
<li>报文小，传输速度快</li>
<li>吞吐量大的网络传输，可以在一定成都上承受数据丢失</li>
</ol>
<h4 id="2-9-ARP协议"><a href="#2-9-ARP协议" class="headerlink" title="2.9  ARP协议"></a>2.9  <code>ARP</code>协议</h4><p>通过<code>IP</code>获取目标计算机的mac地址的协议</p>
<blockquote>
<p>交换机不能识别<code>IP</code>地址</p>
</blockquote>
<h5 id="2-9-1-ssh"><a href="#2-9-1-ssh" class="headerlink" title="2.9.1  ssh"></a>2.9.1  ssh</h5><blockquote>
<p>远程登录会话</p>
</blockquote>
<h5 id="2-9-2-服务器创建的默认端口"><a href="#2-9-2-服务器创建的默认端口" class="headerlink" title="2.9.2  服务器创建的默认端口"></a>2.9.2  服务器创建的默认端口</h5><pre class="language-none"><code class="language-none">ftp:21
ssh:22
mySQL:3306
MongoDB:27017
Redis:6379</code></pre>

<h5 id="2-9-3-http与HTTPS协议的区别"><a href="#2-9-3-http与HTTPS协议的区别" class="headerlink" title="2.9.3  http与HTTPS协议的区别"></a>2.9.3  <code>http</code>与<code>HTTPS</code>协议的区别</h5><ol>
<li><code>https</code>协议需要到ca申请证书，因而需要一定费用，现阶段国内各大厂商也提供免费的证书</li>
<li><code>http</code>是超文本传输协议，信息是铭文传输，<code>https</code>则是具有安全性的<code>ssl</code>加密传输协议</li>
<li><code>http</code>和<code>https</code>使用的是完全不同的连接方式，端口号也不一样,前者是80，后者是443</li>
<li><code>http</code>的连接很简单，是无状态的，<code>https</code>协议是由<code>ssl+http</code>协议构建的可进行加密传输，身份认证的网络协议，比<code>http</code>协议安全(尽管<code>HTTPS</code>安全，但是传输的效率没有<code>http</code>高)</li>
</ol>
<hr>
<h2 id="3-请求"><a href="#3-请求" class="headerlink" title="3.请求"></a>3.请求</h2><p>url:请求的网址，即<strong>统一资源定位符</strong>，它可以唯一确定我门向请求的资源</p>
<h4 id="3-1-请求过程"><a href="#3-1-请求过程" class="headerlink" title="3.1  请求过程"></a>3.1  请求过程</h4><blockquote>
<p>客户端，通常指(web浏览器或<code>APP</code>)向服务器发起请求，服务器接收到的请求进行处理，并向客户端发起响应</p>
</blockquote>
<blockquote>
<p>请求由客户端向服务器发出的，可以分为四部分：请求方法(<code>Request Method</code>),请求网址(<code>Request.URL</code>),请求头(<code>Request Headers</code>) 请求体(<code>Request Body</code>)</p>
</blockquote>
<h4 id="3-2-请求方法"><a href="#3-2-请求方法" class="headerlink" title="3.2  请求方法"></a>3.2  请求方法</h4><p>常见的有八种</p>
<pre class="language-none"><code class="language-none">GET：请求页面，并返回内容
POST：用于提交表单数据或者文件等，数据包含在请求体中
PUT：从客户端向服务器传送的数据取代指定文档中的内容
DELETE：请求服务器删除指定的页面
HEAD：类似于GET请求，只不过返回的响应中没有具体的内容，用于获取头
CONNECT：把服务器当跳板，让服务器代替客户端访问其他网页
OPTIONS：允许客户端查看服务器的性能
TRACE：会回显服务器收到的请求，主要用于测试或诊断</code></pre>

<blockquote>
<p>GET和POST请求的区别</p>
<ol>
<li>GET请求中的参数包含在URL里，数据可以在URL中看到，而POST请求的URL一般不会包含这些数据</li>
<li>GET请求提交的数据最多只有1024字节，而POST方法没有限制</li>
<li>POST比 GET<strong>相对安全</strong></li>
</ol>
</blockquote>
<h4 id="3-3-请求头和请求体"><a href="#3-3-请求头和请求体" class="headerlink" title="3.3  请求头和请求体"></a>3.3  请求头和请求体</h4><p>Accept：请求报头域，用于指定客户端可接收哪些类型的信息</p>
<p>Cookie：页常用复数形式<code>Cookies</code>，这是网站为了辨别用户进行会话跟踪而存在用户本地的数据，它的主要功能时维持当前访问会话，cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上cookies并将其发送给服务器，服务器通过cookies识别出是我们自己，并且查出当前是登录状态，所以返回的数据是登录之后网页内容</p>
<p><code>Referer</code>:此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如来源统计，防盗链处理等</p>
<p>User-Agent：简称UA，它是一个页数的字段串头，可以使服务器识别客户使用的操作系统及版本，浏览器及版本等信息，做爬虫时加上此信息，可以伪装浏览器</p>
<p>x-requested-with：<code>XMHttpRequest</code>  代表ajax请求</p>
<p>Accept-Language：指定客户端可接受的语言类型</p>
<p>Accept-Encoding：指定客户端可接受的内容编码</p>
<p>Content-type：也叫互联网媒体类型(Internet Media Type)或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息,例：text/html代表HTML格式，image/<code>gif</code>代表<code>GIF</code>图片，<code>application/json</code>代表JSON类型</p>
<blockquote>
<p>请求体一般承载的内容时POST请求中的表单数据，GET请求没有请求体，为空</p>
</blockquote>
<h4 id="3-4-反爬机制与反反爬策略"><a href="#3-4-反爬机制与反反爬策略" class="headerlink" title="3.4  反爬机制与反反爬策略"></a>3.4  反爬机制与反反爬策略</h4><pre class="language-python" data-language="python"><code class="language-python">#反爬机制：
为了不让数据泄露，设置了各种阻碍，这就是反爬机制

#反反爬策略
针对网站的反爬机制，采取不同策略
1.脚本：直接忽略
2.scrapy框架：修改配置文件，让爬虫不遵守robots协议</code></pre>

<hr>
<h2 id="4-响应"><a href="#4-响应" class="headerlink" title="4.响应"></a>4.响应</h2><blockquote>
<p>响应是由服务端返回给客户端的，可 以分为三部分：响应状态码，响应头，响应体</p>
</blockquote>
<blockquote>
<p>响应体，响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码，我们要爬虫请求网页后，要解析的内容就是响应体</p>
</blockquote>
<h4 id="常见的状态码"><a href="#常见的状态码" class="headerlink" title="常见的状态码"></a>常见的状态码</h4><pre class="language-none"><code class="language-none">200：成功
301：永久重定向
302：临时重定向
400：错误的请求
401：未授权
403：服务器拒绝此请求
404：未找到
500：服务器内部错误
501：服务器不具备完成请求的功能
502：错误的网关，服务器走位网关或代理，从上游服务器收到无效响应
504：网关超时，服务器作为网关或代理，但是没有及时从上游服务器收到请求
505：HTTP版本不支持</code></pre>

<blockquote>
<p>状态码不能完全代表相应状态，部分网站的状态码是自定义的，一切以响应数据为准</p>
</blockquote>
<h4 id="4-1-响应数据的几种形式"><a href="#4-1-响应数据的几种形式" class="headerlink" title="4.1  响应数据的几种形式"></a>4.1  响应数据的几种形式</h4><pre class="language-python" data-language="python"><code class="language-python">res = requests.get(url='https://www.guidaye.com/cp/')
res.text &gt;&gt;&gt;将响应对象转化为str类型
res.json()  &gt;&gt;&gt; 将响应对象转化为python中的dict类型，形式(类json)
res.content  &gt;&gt;&gt;流形式(数据流，图片就是流形式)</code></pre>

<blockquote>
<p>如果响应数据中文乱码，可以用<code>content.decode('utf-8')</code>来解决</p>
</blockquote>
<h4 id="4-2-uuid"><a href="#4-2-uuid" class="headerlink" title="4.2  uuid"></a>4.2  uuid</h4><blockquote>
<p>通用唯一标识符，时间戳，命名空间，随机数，伪随机数来保证生成ID的唯一性</p>
<p>python的<code>uuid</code>模块提供<code>UUID</code>类和函数<code>uuid1()</code>, <code>uuid3()</code>,<code> uuid4()</code>,<code> uuid5()</code> 来生成1, 3, 4, 5各个版本的<code>UUID</code><br>( 需要注意的是: python中没有**<code>uuid2()</code>**这个函数)</p>
</blockquote>
<pre class="language-none"><code class="language-none">uuid1：基于时间戳
uuid3：基于名字的MD5散列值
uuid4：基于随机数，有一定重复概率
uuid5：基于名字的SHA=1散列值</code></pre>

<hr>
<h2 id="5-正则表达式"><a href="#5-正则表达式" class="headerlink" title="5.正则表达式"></a>5.正则表达式</h2><h4 id="5-1-元字符"><a href="#5-1-元字符" class="headerlink" title="5.1  元字符"></a>5.1  元字符</h4><pre class="language-python" data-language="python"><code class="language-python">.:任意字符，换行符除外
\d:任意数字
\w:任意数字字母下划线
\s:空白符

#如果是大写的s，w，d，代表'非'</code></pre>

<h4 id="5-2-字符组"><a href="#5-2-字符组" class="headerlink" title="5.2  字符组"></a>5.2  字符组</h4><pre class="language-python" data-language="python"><code class="language-python">[a-z]:
[A-Z]:
[0-9]:

[^...]
#匹配非其中元素，举例：[^abc]---&gt;匹配除了abc之外的字符</code></pre>

<h4 id="5-3-量词"><a href="#5-3-量词" class="headerlink" title="5.3  量词"></a>5.3  量词</h4><pre class="language-python" data-language="python"><code class="language-python">*:匹配0次或多次
+:匹配1次或多次
?:匹配0次或1次  #非贪婪匹配
{m}:m次
{m,}:至少m次
{m,n}:m-n次
{,n}:最多n次</code></pre>

<h4 id="5-4-边界修饰"><a href="#5-4-边界修饰" class="headerlink" title="5.4   边界修饰"></a>5.4   边界修饰</h4><blockquote>
<p>^匹配开始</p>
<p>$匹配结尾</p>
</blockquote>
<h4 id="5-5-分组"><a href="#5-5-分组" class="headerlink" title="5.5   分组"></a>5.5   分组</h4><pre class="language-python" data-language="python"><code class="language-python">import re
s = "&lt;a href='asdsdjfiohssdbfkjsdbkjsd'&gt;"

res = re.findall(r"href='(.*?)'&gt;",s)</code></pre>



<h4 id="5-6-贪婪与非贪婪"><a href="#5-6-贪婪与非贪婪" class="headerlink" title="5.6  贪婪与非贪婪"></a>5.6  贪婪与非贪婪</h4><blockquote>
<p>贪婪，尽可能多的匹配</p>
<p>非贪婪，尽可能往少了匹配</p>
</blockquote>
<h4 id="5-7-re"><a href="#5-7-re" class="headerlink" title="5.7  re"></a>5.7  re</h4><p><code>re.findall(r'正则表达式','str')</code>,结果是一个列表，匹配整个字符串</p>
<p><code>re.search(r'正则表达式','str')</code>匹配到第一个结果就返回，返回的是一个对象，使用<code>group</code>取值</p>
<p><code>re.match(r'正则表达式','str')</code>从字符串开始进行匹配，返回一个对象，使用<code>group</code>取值，如果未匹配到，返回None </p>
<p><code>re.complie</code>将正则表达式编译为<strong>对象</strong>，在需要按正则表达式匹配是可以在直接使用该对象调用以上方法</p>
<hr>
<h2 id="6-requests高阶应用"><a href="#6-requests高阶应用" class="headerlink" title="6. requests高阶应用"></a>6. requests高阶应用</h2><h4 id="6-1-文件处理"><a href="#6-1-文件处理" class="headerlink" title="6.1  文件处理"></a>6.1  文件处理</h4><pre class="language-python" data-language="python"><code class="language-python">import requests
#打开文件，注意要以rb形式打开
f = open('chn.jpg','rb')
files = {
    'file':f
}
res = requests.post(url='***',files = files)</code></pre>

<blockquote>
<p>文件也是一种数据，所以，可以通过<code>files</code>参数来进行文件的上传</p>
</blockquote>
<h4 id="6-2-会话维持"><a href="#6-2-会话维持" class="headerlink" title="6.2  会话维持"></a>6.2  会话维持</h4><pre class="language-python" data-language="python"><code class="language-python">from requests import Session
#1.实例化一个对象
session = Session()
#2.url
url = '*****'
#3.session.get()或者session.post(url=url.headers=headers)
res = session.post(url=url.headers=headers)</code></pre>

<h4 id="6-3-ssl证书验证"><a href="#6-3-ssl证书验证" class="headerlink" title="6.3  ssl证书验证"></a>6.3  <code>ssl</code>证书验证</h4><p><code>https</code>是<code>http</code>的安全版本，<code>HTTPS</code>在<code>http</code>的基础上多了一个<code>ssl</code>安全套接层</p>
<blockquote>
<p>requests提供了证书验证的功能，当发起HTTP请求时，模块会检查<code>SSL</code>证书，但检查的行为可以用verify参数来控制</p>
</blockquote>
<p>添加了一个参数<code>verify=false   ---&gt;不检查ssl证书</code>,如果等于<code>True</code>，则检查<code>SSL</code>证书</p>
<pre class="language-python" data-language="python"><code class="language-python">#ssl证书验证

#添加一个verify=false参数，禁止证书验证
import requests
url = '******'
#阻止抛出警告
requests.packages.urllib3.disable_warinings()

res = requests.get(url=url,verify=false)</code></pre>

<blockquote>
<p>简单来说，在爬取网站时，有可能网站的证书是有问题的，这时如果使用<code>requests</code>模块去请求时，会报错，所以需要<code>ssl</code>证书验证</p>
</blockquote>
<h4 id="6-4-代理设置"><a href="#6-4-代理设置" class="headerlink" title="6.4  代理设置"></a>6.4  代理设置</h4><blockquote>
<p>代理<code>IP</code>是指在请求的过程中使用非本机<code>ip</code>进行请求，避免大数据量频繁请求的过程中出现<code>IP</code>封禁，限制数据的爬取</p>
</blockquote>
<p>透明代理<code>ip</code>：服务器知道你使用了代理，服务器能够获取爬虫真实的<code>ip</code></p>
<p>匿名代理<code>ip</code>：服务器知道你使用了代理，服务器不能获取爬虫真实的<code>ip</code></p>
<p>高匿代理<code>ip</code>：服务器不知道使用了代理，服务器不能获取爬虫真实<code>ip</code></p>
<blockquote>
<p>代理类别：基于接口的，基于隧道的</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">反爬：ip封禁---&gt;使用代理ip

import requests
url = '*********'
proxies = {
    #或者是https
    'http':'http://ip地址:端口号',
    #无论是http还是https，后面一定是http
    'https':'http://ip地址:端口号'
}
res = requests.get(url=url,proxies = proxies)</code></pre>

<h4 id="6-5-超时设置"><a href="#6-5-超时设置" class="headerlink" title="6.5  超时设置"></a>6.5  超时设置</h4><p>添加了一个参数，以<strong>秒</strong>计量<code>timeout=0.1</code></p>
<pre class="language-python" data-language="python"><code class="language-python">#添加timeout参数，秒数
import requests
res=requests.get(url=url,timeout=0.1)</code></pre>

<blockquote>
<p>给予爬虫与服务器连接的时间限定，设置一个时间，在指定的时间内完成了正常的连接，不报错，如果没有完成，就会报错</p>
<p>requests模块发送请求可以设置超时时间，在超时时间内未得到响应，便会抛出异常</p>
<p>好处：一方面减少了请求的阻塞时间，一方面，可以进行异常处理，执行相应的操作</p>
</blockquote>
<blockquote>
<p>如果规定时间完成了和服务器连接，之后爬取数据的时间并不算在超时设置的时间内</p>
</blockquote>
<h4 id="6-6-UA检测"><a href="#6-6-UA检测" class="headerlink" title="6.6  UA检测"></a>6.6  <code>UA</code>检测</h4><p><code>UA</code>是用户的身份表示，可以表示用户的系统及浏览器信息</p>
<blockquote>
<p>在请求过程中，添加headers参数</p>
</blockquote>
<h4 id="6-7-cookie的处理-session"><a href="#6-7-cookie的处理-session" class="headerlink" title="6.7  cookie的处理(session)"></a>6.7  cookie的处理(session)</h4><blockquote>
<p>在同一个关联网页中，为了保存登录状态和各种信息，可以通过cookie来保持</p>
</blockquote>
<blockquote>
<p>三种方法</p>
<ol>
<li>手动在headers中添加cookie的键值对</li>
<li><code>cookiejar</code>对象</li>
<li>自动封装cookie的类：<code>Session</code></li>
</ol>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#cookie的处理
	#1.url = 'https://www.baidu.com/'
        headers = {
            'Cookie':'BIDUPSID=B63BDB40304991E9FF3159864CC9C302; PSTM=1586308511; BAIDUID=B63BDB40304991E9CC4E4ECFFCFFB23D:FG=1; BD_UPN=12314753; BDUSS=VWNmZu',
            'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
        }
        res = requests.get(url=url,headers=headers)
        
    #2.cookiejar对象
    from requests.cookies import RequestCookieJar
   	1.首先需要获取Cookies
    Cookie = ***********
    2.实例化一个jar对象
    jar = RequestsCookieJar()
    3.处理Cookies，封装进jar对象中
    for i in Cookie.split(','):
        #再次分割，分成dict的键值，每分割一次添加一次
        k,v = i.split('=',1)
        jar.set(k,v)
    
    #3.Session类，会话维持
    from requests import Session
    1.实例化一个对象
    session = Session()
    2.url
    url = '*****'
    3.session.get()或者session.post(url=url.headers=headers)
    res = session.post(url=url.headers=headers)</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">from requests import Session

session = Session()

data = {
    'username':"天听",
    'password':'123456'
}

res = session.post(url=url,headers=headers,data=data)</code></pre>

<hr>
<h2 id="7-lxm库"><a href="#7-lxm库" class="headerlink" title="7.lxm库"></a>7.<code>lxm</code>库</h2><blockquote>
<p>从响应数据中抽取出目标数据的过程，就叫做数据解析<br>数据解析：<br><code>re</code>,<code>xpath</code>,<code>BS4</code>,<code>Pyquery</code></p>
</blockquote>
<blockquote>
<p>DOM树与<code>xpath</code>解析原理</p>
<p>HTML页面标签存在层级关系，即DOM树，在获取目标数据时可以根据网页层级关系定位标签，再获取标签的文本或属性</p>
<p><code>xpath</code>解析原理：根据DOM节点的结构关系，进行定位</p>
</blockquote>
<h4 id="7-1-xpath基本语法"><a href="#7-1-xpath基本语法" class="headerlink" title="7.1  xpath基本语法"></a>7.1  <code>xpath</code>基本语法</h4><p><code>.</code>:当前节点</p>
<p><code>/</code>:根节点</p>
<p><code>//</code>:代表任意位置</p>
<p><code>.//</code>:从当前节点向下的任意位置匹配</p>
<p><code>nodename</code>:<strong>节点名</strong>定位</p>
<p><code>nodename[@attribute='value']</code>:根据节点的属性进行定位</p>
<p><code>@attribue</code>：获取节点的属性值，比如获取a标签的<code>href</code>属性，直接可以<code>/a/@href</code></p>
<p><code>text()</code>:获取节点的文本<br><code>//div[@class='asdds']/p/text()</code></p>
<h4 id="7-2-属性匹配"><a href="#7-2-属性匹配" class="headerlink" title="7.2  属性匹配"></a>7.2  属性匹配</h4><ul>
<li><p>单属性多值匹配：当节点的一个属性有多个值时，根据其中一个进行定位，使用<code>contai ns</code>函数</p>
<pre class="language-python" data-language="python"><code class="language-python">'//div[contains(@class,"属性值")]'</code></pre>
</li>
<li><p>多属性匹配：用节点的多个属性共同定位节点<code>and</code></p>
<pre class="language-python" data-language="python"><code class="language-python">'//div[@class="asds" and @name="adsadasd"]'</code></pre>

</li>
</ul>
<h4 id="7-3-按序选择"><a href="#7-3-按序选择" class="headerlink" title="7.3  按序选择"></a>7.3  按序选择</h4><ul>
<li><p>索引定位：[6] </p>
<blockquote>
<p><strong>注意，索引从1开始，跟python有区别</strong></p>
</blockquote>
</li>
<li><p>位置函数：<code>position</code><br>例：<code>/li[position()&gt;2]</code></p>
</li>
<li><p><code>last()</code>函数：定位最后一个，<code>last()-1</code>代表倒数第二个</p>
</li>
</ul>
<h4 id="7-4-流程"><a href="#7-4-流程" class="headerlink" title="7.4  流程"></a>7.4  流程</h4><p>加载本地<code>html</code>，需要有<code>etree.HTMLParser</code>参数，注意要加括号<br>例：<code>tree = etree.parser('./xpath.html',etree.HTMLParser())</code></p>
<p>加载网页<code>html</code>,直接使用HTML<br>例：<code>tree = etree.HTML()</code></p>
<p>然后些<code>xpath</code>语法<br><code>tree.xpath('//ul[@class="pli"]/li/div/a/img/@src')</code></p>
<p><code>xpath</code>获得的结果是一个列表2  </p>
<pre class="language-python" data-language="python"><code class="language-python">#编码流程
from lxml import etree
res = requests.get(...)
tree = etree.HTML(res.text) #etree加载的是响应数据的文本形式
tree.xpath('xpath表达式')</code></pre>



<h4 id="7-5-补充"><a href="#7-5-补充" class="headerlink" title="7.5  补充"></a>7.5  补充</h4><pre class="language-python" data-language="python"><code class="language-python">res1 = tree.xpath("//div[@id='007']/text()")

res2 = tree.xpath("//div[@id='007']//text()")

'''
res1展示的是以divid007为根节点的结果，其div下的其他标签不显示
res2展示的是以divid007任意位置的结果，其div下的其他标签内容也一同显示
'''</code></pre>

<hr>
<h2 id="8-动态数据加载"><a href="#8-动态数据加载" class="headerlink" title="8.动态数据加载"></a>8.动态数据加载</h2><blockquote>
<p>网页HTML上，有些数据是通过<code>js</code>代码填充，所以如果直接使用爬虫，只会爬取到一个标签，并没有其中的元素</p>
<p>requests模块和<code>scrapy</code>框架在发起请求爬取数据的过程中，不能执行<code>js</code>代码</p>
</blockquote>
<h4 id="8-1-selenium"><a href="#8-1-selenium" class="headerlink" title="8.1  selenium"></a>8.1  selenium</h4><blockquote>
<p>selenium是一个web端自动化测试框架，可以通过代码来控制浏览器，比如打开关闭，点击等行为</p>
<p>作用：帮助抓取动态加载的数据，避免反爬</p>
</blockquote>
<h4 id="8-2-selenium安装与配置与操作"><a href="#8-2-selenium安装与配置与操作" class="headerlink" title="8.2  selenium安装与配置与操作"></a>8.2  selenium安装与配置与操作</h4><blockquote>
<ol>
<li><code>Chrome</code>浏览器</li>
<li><code>selenium</code>框架：<code>pip install selenium</code></li>
</ol>
<p>3.驱动程序：下载<br>   <code>http://npm.taobao.org/mirrors/chromedriver/</code><br>   查看浏览器版本<br>   选择对应的版本</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">编码流程：
	#首先需要将下载的chromedriver.exe放到代码文件夹下
    #导包
    from seleniumi import webdriver
    
    
    #调用chromedriver.exe
    bro = webdriver.Chrome('./chromedriver.exe')
    
    #访问
    bro.get('https://www.iqiyi.com/')
    
    #获取网页源代码
	(对象)bro.page_source   ---&gt;字符串</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#如何获取网页的元素
#根据标签内属性定位，一般用id定位
find_element_by_id('id')
find_element_by_name('name')
find_element_by_class_name('class')#根据class属性定位
find_element_by_xpath()#根据xpath定位节点
find_element_by_css_selector()#css选择器
find_element_by_link_text()#根据超链接文本定位
find_element_by_partial_link_text()#根据超链接文本的一部分定位

#执行js脚本
execute_script(js)

#节点交互操作：
1.输入内容：对象.send_keys()

2.清空内容：对象.clear()

3.点击操作：对象.click()

4.退出浏览器：对象.quit()

#获取网页的数据
获取元素属性：get_attribute()
获取元素文本：get_text()
获取元素位置：element.location
获取元素尺寸：element.size
获取网页源码：browser.page_source(*****)


#执行js脚本
js = 'window.scrollTo(0,300)'#向下滚动300距离

js = 'window.scrollTo(0,document.body.scrollHeight)'#滚动到底部


对象.execute_script(js)</code></pre>

<h4 id="iframe标签跳转"><a href="#iframe标签跳转" class="headerlink" title="iframe标签跳转"></a><code>iframe</code>标签跳转</h4><p><code>switch_to.frname('frameid')</code></p>
<p><code>switch_to.default_content()</code></p>
<p>实例：</p>
<pre class="language-python" data-language="python"><code class="language-python">from selenium import webdriver
from time import sleep
from selenium.webdriver.chrome.options import Options

options = Options()

options.add_experimental_option('excludeSwitches',['enable-automation'])

#调用chromedriver.exe
bro = webdriver.Chrome('./chromedriver.exe',options=options)

bro.get('https://www.baidu.com/')

#根据id--&gt;kw获取input输入框
input_tag = bro.find_element_by_id('kw')

#根据id--&gt;su获取百度一下点击按钮
button_baidu = bro.find_element_by_id('su')

#输入框输入
input_tag.send_keys('黑洞')

#点击
button_baidu.click()

#睡眠两秒之后清空输入框
sleep(2)
input_tag.clear()

input_tag.send_keys('抖动')
button_baidu.click()
sleep(3)
input_tag.clear()


input_tag.send_keys('翻转')
button_baidu.click()
sleep(3)
input_tag.clear()

bro.quit()</code></pre>

<h4 id="8-3-子级页面和父级页面"><a href="#8-3-子级页面和父级页面" class="headerlink" title="8.3  子级页面和父级页面"></a>8.3  子级页面和父级页面</h4><blockquote>
<p>HTML页面嵌套另一个HTML页面</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#子页面的跳转
switch_to.frame('id')

#跳转到父级页面
switch_to_default.conent()</code></pre>

<blockquote>
<p>注意，selenium默认操作父级页面</p>
</blockquote>
<h4 id="8-4-防检测"><a href="#8-4-防检测" class="headerlink" title="8.4  防检测"></a>8.4  防检测</h4><pre class="language-python" data-language="python"><code class="language-python">from selenium.webdriver.chrome.options import Options
options = Options()
options.add_experimental_option('excludeSwitches',['enable-automation'])
bro = webdriver.Chrome('./chromedriver.exe',options=options)</code></pre>

<hr>
<h2 id="9-多线程爬虫"><a href="#9-多线程爬虫" class="headerlink" title="9.多线程爬虫"></a>9.多线程爬虫</h2><blockquote>
<p>在爬取数据量大的数据时，耗费时间较长，为了提高效率，可采用多线程爬虫，提高效率，但是，多线程不是原子性，操纵数据可能会导致数据紊乱，不安全</p>
</blockquote>
<h4 id="9-1-并发与并行"><a href="#9-1-并发与并行" class="headerlink" title="9.1  并发与并行"></a>9.1  并发与并行</h4><blockquote>
<p>并行：在同一时刻，多个任务同时执行</p>
<p>并发：在同一时间段内，多个任务同时执行<br>并发时，一般采用了时间片轮转法，即在一个时间段内，给每个程序添加一个时间片，也可以叫做进度条，进度条走完，下一个程序再继续运行，不过时间片轮转时，停顿时间非常的短，所以会造成多个任务同时运行的错误</p>
</blockquote>
<h4 id="9-2-示例"><a href="#9-2-示例" class="headerlink" title="9.2  示例"></a>9.2  示例</h4><pre class="language-python" data-language="python"><code class="language-python">from threading import Thread
from queue import Queue
import requests
from lxml import etree
import pymongo
from threading import Lock

#负责爬取的类
class SpiderThread(Thread):
    def __init__(self, name, url_queue, data_queue):
        super().__init__()
        self.name = name
        self.url_queue = url_queue
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
        }
        self.data_queue = data_queue

    def run(self):
        base_url = 'http://xiaohua.zol.com.cn/lengxiaohua/%s.html'
        while 1:
            try:
                page = self.url_queue.get(block=False)
                print('%s正在爬取数据' % self.name )
                res = requests.get(url=base_url % page, headers=self.headers)
                self.data_queue.put(res.text)
            except:
                break

#负责解析的类
class ParseThread(Thread):
    def __init__(self, name, data_queue, lock):
        super().__init__()
        self.name = name
        self.data_queue = data_queue
        self.lock = lock

    def run(self):
        # 调用解析方法
        while 1:
            try:
                html = self.data_queue.get(block=False)
                print('%s 正在解析数据' % self.name)
                self.parse(html)
            except:
                break

    def parse(self, html):
        tree = etree.HTML(html)
        li_list = tree.xpath('//li[@class="article-summary"]')
        for li in li_list:
            title = li.xpath('.//span[@class="article-title"]/a/text()')
            content = ''.join(li.xpath('.//div[@class="summary-text"]//text()'))
            if title and content:
                data = {
                    'title': title[0],
                    'content': content
                }
                with self.lock:
                    self.save(data)

    def save(self, data):
        # 简历连接
        conn = pymongo.MongoClient()
        db = conn.lilong
        table = db.liuyueyang
        table.insert_one(data)</code></pre>

<hr>
<h2 id="10-无头浏览器与BS4"><a href="#10-无头浏览器与BS4" class="headerlink" title="10.无头浏览器与BS4"></a>10.无头浏览器与<code>BS4</code></h2><h4 id="10-1-无头浏览器"><a href="#10-1-无头浏览器" class="headerlink" title="10.1  无头浏览器"></a>10.1  无头浏览器</h4><blockquote>
<p>什么是无头浏览器（headless browser），简单来说是一种<strong>没有界面</strong>的浏览器。既然是浏览器那么浏览器该有的东西它都应该有，只是看不到界面而已。我们日常使用浏览器的步骤为：启动浏览器、打开一个网页、进行交互。而无头浏览器指的是我们使用脚本来执行以上过程的浏览器，能模拟真实的浏览器使用场景。</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">from selenium import webdriver
from selenium .webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--dissble-gpu')
bro = webdriver.Chrome(chrome_options = chrome_options)
bro.get(******)
print(bro.page_source)</code></pre>

<h4 id="10-2-BS4语法"><a href="#10-2-BS4语法" class="headerlink" title="10.2  BS4语法"></a>10.2  <code>BS4</code>语法</h4><blockquote>
<p>编码流程：</p>
<ol>
<li>导包<code>from bs4 import BeautifulSoup</code></li>
<li>实例化对象，传两个参数，一个文本，一个解析器，一般为<code>lxml</code><br><code>suop = BeautifulSoup(res.text,'lxml')</code></li>
<li>选择器解析</li>
</ol>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#bs4编码流程
from bs4 import BeautifulSoup
suop = BeautifulSoup(res.text,'lxml')
tag = soup.select('css选择器表达式')
tag = soup.节点() #节点选择器
tag = soup.findall() # 方法选择器</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#节点选择器
from bs4 import BeautifulSoup
soup = BeautifulSoup(res.text,'lxml')
tag = soup.a  #取a标签，只取一个</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#方法选择器
find_all(name,attrs,text,limit):
    soup.findall(name='***')  #根据节点名字定位
    soup.findall(attrs={'属性名(scr,class等)':'值'})#根据属性定位，多个属性时，一个即可定位
    soup.findall(text=res.compile(r'***'))#根据节点文本定位，返回文本
    soup.findall(name='***',limit=2)#只返回两个结果
    
find(name,attrs,text,limit):区别于find_all，find返回的是一个对象结果
    
find_all(name=节点名,{attrs:属性值})返回的是一个列表</code></pre>

<pre class="language-python" data-language="python"><code class="language-python">#css选择器
属性选择器：
1.根据节点名定位标签：标签选择器
soup.select('***(title,a,p等)')
2.根据节点的class属性定位：css选择器
soup.select('.***')
3.根据id定位
soup.select('#***')
4.嵌套选择：
ss = soup.select('ul')#得到的是一个列表
for i in ss:
    print(i.select("li"))
5.层级选择器
soup.select('div &gt; ul &gt; li') #单层级选择器，按照顺序找到直属li
soup.select('div li') #多层级选择器，包含了div下的所有li

#获取节点的文本或属性
obj.string:获取直接子文本，如果节点内有平行的节点，则结果是None
obj.get_text()：获取子孙节点的所有文本
obj['***(属性)']：获取节点属性</code></pre>

<hr>
<h2 id="11-快代理网站的模拟登录"><a href="#11-快代理网站的模拟登录" class="headerlink" title="11.快代理网站的模拟登录"></a>11.快代理网站的模拟登录</h2><pre class="language-python" data-language="python"><code class="language-python">from requests import Session

#实例化session对象
session = Session()
#登录的url
url = 'https://www.kuaidaili.com/login/'
#构造数据
data = {
    'next': '',
    'kf5_return_to': '',
    'username': '2274201339@qq.com',
    'passwd': 'o66.'
}
#浏览器头
headers = {
    "User-Agent":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'
}

#用post请求将数据传进去，此时，已经模拟了登录，访问这个网站的其他网页时，会保存登录状态
res = session.post(url=url,headers=headers,data=data)

#访问时，可以获得到返回的数据，用户名在其中，模拟登录完成
ret = session.get(url='https://www.kuaidaili.com/api/checkuser/',headers=headers)
print(ret.json())</code></pre>

<h4 id="11-1-第三方打码平台"><a href="#11-1-第三方打码平台" class="headerlink" title="11.1  第三方打码平台"></a>11.1  第三方打码平台</h4><blockquote>
<p>我们在模拟登录时，时常会遇到一些验证码代码无法准确的识别不同的验证码，这时，就用到了打码平台，它会将图片上的字符或者数字转成字符串返回给你</p>
</blockquote>
<blockquote>
<p>流程：</p>
<ol>
<li>下载验证码图片</li>
<li>传给第三方打码平台</li>
<li>进行识别，识别完成之后，传回<code>ret</code></li>
<li>把<code>ret</code>拿回，构造数据</li>
</ol>
</blockquote>
<blockquote>
<p>超级鹰平台</p>
</blockquote>
<h2 id="12-Scrapy框架"><a href="#12-Scrapy框架" class="headerlink" title="12.Scrapy框架"></a>12.<code>Scrapy</code>框架</h2><blockquote>
<p><code>scrapy</code>是基于<code>twisted</code>的异步框架</p>
</blockquote>
<h4 id="12-1-安装"><a href="#12-1-安装" class="headerlink" title="12.1  安装"></a>12.1  安装</h4><blockquote>
<p>首先需要安装相应的依赖库</p>
<p><code>lxml</code>、<code>wheel</code>、<code>pywin32</code></p>
<p><code>twisted</code>此依赖安装时，需要从<a target="_blank" rel="noopener" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">地址</a>下载相应的版本,然后通过<code>pip</code>来安装</p>
</blockquote>
<blockquote>
<p>最后安装<code>scrapy</code>框架</p>
</blockquote>
<h4 id="12-2-创建项目"><a href="#12-2-创建项目" class="headerlink" title="12.2  创建项目"></a>12.2  创建项目</h4><blockquote>
<p>创建项目<br><code>scrapy startproject 项目名</code></p>
</blockquote>
<blockquote>
<p>创建爬虫文件<br><code>scrapy genspider 爬虫名 域名</code></p>
</blockquote>
<h4 id="12-3-运行项目"><a href="#12-3-运行项目" class="headerlink" title="12.3  运行项目"></a>12.3  运行项目</h4><p><code>scrapy  crawl 爬虫名</code></p>
<h4 id="12-4-项目基本架构"><a href="#12-4-项目基本架构" class="headerlink" title="12.4  项目基本架构"></a>12.4  项目基本架构</h4><p>└── <code>day01(A)</code>  外层项目目录<br>        └── <code>day01(A)</code>  内层项目目录<br>            └── <code>spiders</code>  放置爬虫的包<br>                └── <code>__init__.py</code><br>                └── <code>tianting.py</code>  爬虫文件   </p>
<p>​            └── <code>items.py</code>   定义要爬取的数据字段<br>​            └── <code>middlewares.py</code>  中间件<br>​            └── <code>piplines.py</code>  管道<br>​            └── <code>settings.py</code>  配置文件(爬虫配置)<br>​        └── <code>scrapy.cfg</code>  配置文件,跟部署相关</p>
<h4 id="12-5-核心组件与数据流向"><a href="#12-5-核心组件与数据流向" class="headerlink" title="12.5  核心组件与数据流向"></a>12.5  核心组件与数据流向</h4><h5 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h5><blockquote>
<p>五大核心组件</p>
<p>1.引擎(<code>Scrapy Engine</code>)：<br>整个框架的调度，负责各个组件之间的通信与数据的传递</p>
<p>2.爬虫(<code>Spiders</code>)：<br>定义爬取行为和解析规则</p>
<p>3.调度器(<code>Scheduler</code>)：<br>负责调度所有请求</p>
<p>4.下载器(<code>Downloader</code>)：<br>负责爬取页面(与互联网交互，爬取页面的)</p>
<p>5.管道(<code>Item Pipeline</code>)：<br>负责数据持久化</p>
</blockquote>
<blockquote>
<p>由于引擎负责各个组件之间的调度，所以，所有的组件在相互传递时，都需要经过引擎，比如，爬虫解析后需要将<code>req</code>给引擎，然后引擎再给调度器，<strong>注意</strong>，此时，调度器<strong>无法</strong>越过引擎去直接调用下载器，所以，需要将<code>req</code>再次返回给引擎，由引擎来调度下载器和互联网交互</p>
</blockquote>
<h5 id="数据流向"><a href="#数据流向" class="headerlink" title="数据流向"></a>数据流向</h5><blockquote>
<p>依据请求的生命周期</p>
</blockquote>
<p>​                【调度器】</p>
<p>​                    ↑        ↓<br>​                <code>2.req  3.req  4.req</code></p>
<p>【管道】    【引擎】        →    【下载器】 <code>5.req</code>    → <code>internet</code><br>                                ←<code>7.res</code>            ←<code>6.res</code>    </p>
<p>​                        ↑      ↓<br>​                    <code>1.req  8.res</code></p>
<p>​                【爬虫】</p>
<blockquote>
<p>9.爬虫经过引擎到达管道 </p>
</blockquote>
<h4 id="12-6-组件分析"><a href="#12-6-组件分析" class="headerlink" title="12.6   组件分析"></a>12.6   组件分析</h4><ul>
<li><p>爬虫组件</p>
</li>
<li><blockquote>
<p><code>xpath</code>选择器，<code>extract_first</code>返回第一个数据，如果不加<code>first</code>，则是返回所有数据</p>
</blockquote>
</li>
</ul>
<blockquote>
<p><code>scrapy.Spider</code>:Spider爬虫类，自建的爬虫类必须继承这个</p>
<p>将数据item在组件中传递，<strong>注意</strong>不是return，而是<code>yield</code></p>
</blockquote>
<blockquote>
<p>实例化item,两种方法，一种是实例化对象，另一种是直接<code>yield 类名(字段名=值)</code></p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">class BlogSpider(scrapy.Spider):
    #爬虫名，爬虫唯一的身份标识，不可重复
    name = 'blog'
    #域名的限定，限制了爬虫的范围，可以注释
    # allowed_domains = ['baidu.com']
    #起始url，当项目启动，自动对这个url发起请求
    start_urls = ['http://baidu.com/']
    #parse是默认的解析回调方法，如果发送一个请求，未指定回调解析，默认调用parse
    def parse(self,response):
        li_list = response.xpath('//ul[@id="menu-list"]/li')
        for li in li_list:
            item = Test01Item()
            #名字
            item['title'] = li.xpath('.//h2/a/@title').extract_first()
            #简介
            item['brief'] = ''.join(li.xpath('./text()').extract()).replace('\n','')
            #时间
            item['date'] = re.findall(r'\d+-\d+-\d+', li.xpath('.//p/text()').extract_first())[0]
            #链接
            item['link'] = li.xpath('.//h2/a/@href').extract_first()
            yield item
            
            #第二种实例化方法
            yield Test01Item(title=title)</code></pre>

<ul>
<li>item</li>
</ul>
<blockquote>
<p>Item 是保存爬取数据的容器，它的使用方法和字典类似,Item 多了额外的保护机制，可以避免拼写错误或者定义字段错误。<br>建 <code>Item</code>需要继承<code> scrapy.Item</code>类，并且定义类型为<code> scrapy.Field</code>字段 </p>
</blockquote>
<blockquote>
<p>注意！！items只能够通过字典的方式进行访问和添加</p>
</blockquote>
<p>在爬虫组件中使用item容器，需要以下4步：</p>
<ol>
<li>导包，将items导入</li>
<li>实例化items中的类对象</li>
<li>通过键值对的字典形式将数据添加进items</li>
<li>通过yield将items返回</li>
</ol>
<pre class="language-python" data-language="python"><code class="language-python">class Test01Item(scrapy.Item):
    title = scrapy.Field()
    jianjie = scrapy.Field()
    
#爬虫组件中：
item = Test01Item()
            #名字
item['title'] = li.xpath('.//h2/a/@title').extract_first()
    #简介
item['brief'] = ''.join(li.xpath('./text()').extract()).replace('\n','')
    #时间
item['date'] = re.findall(r'\d+-\d+-\d+', li.xpath('.//p/text()').extract_first())[0]
    #链接
item['link'] = li.xpath('.//h2/a/@href').extract_first()
    yield item</code></pre>

<ul>
<li>管道(<code>pipelines</code>)</li>
</ul>
<blockquote>
<p>在管道中，主要实现数据的保存，在自定义类中，实现process_item方法，参数有item，spider</p>
</blockquote>
<blockquote>
<p>如果爬虫组件中，传出了item，那么就需要将item走过这个process_item方法</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#数据库的连接
conn = pymongo.MongoClient('localhost',27017)
db = conn.Longtan
table = db.ss
#自定义类
class TextPipline:
#实现process_item方法,item就是爬虫组件中传来的item
    def process_item(self,item,spider):
        table.insert_one(dict(item))
        return item</code></pre>

<ul>
<li>settings</li>
</ul>
<blockquote>
<p>需要修改的是<code>ROBOTSTXT_OBEY</code>,<code>UA</code>,</p>
<pre class="language-python" data-language="python"><code class="language-python">ITEM_PIPELINES = {
   'test01.pipelines.TextPipline': 300,
}</code></pre>
</blockquote>
<h4 id="12-7-保存数据"><a href="#12-7-保存数据" class="headerlink" title="12.7  保存数据"></a>12.7  保存数据</h4><p>保存到<code>json</code>中</p>
<pre class="language-python" data-language="python"><code class="language-python">scrapy crawl 爬虫名 -o blog_data.json</code></pre>

<blockquote>
<p>另外，也可以每一个Item输出一行<code>JSON</code>,输出后缀改为<code>jl</code>，命令:<code>scrapy crawl 爬虫名 -o blog_data.jl</code></p>
</blockquote>
<p>此外，输出还支持<code>csv、xml、pickle、marshal</code>，还支持了远程<code>ftp、s3</code>等输出</p>
<blockquote>
<p>注意，<code>ftp</code>输出需要正确配置用户名、密码、地址、输出路径，否则会报错</p>
</blockquote>
<h4 id="12-8-管道-Item-Pipline-的使用"><a href="#12-8-管道-Item-Pipline-的使用" class="headerlink" title="12.8  管道(Item Pipline)的使用"></a>12.8  管道(<code>Item Pipline</code>)的使用</h4><blockquote>
<p>注意，<code>pipline</code>是需要被注册的</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">1. 在settings中，解封或者重新在底下写上

ITEM_PIPELINES = {
   'test01.pipelines.TextPipline': 300,
	#项目名.pipelines.自定义的管道名:
}
# 300：代表着优先级，数字越小，代表着优先级越高</code></pre>



<blockquote>
<p>当<code>Item</code>生成后，它会自动被送到管道进行处理，我们常用管道来实现以下:</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取数据，检查爬取字段</li>
<li>查重并丢弃重复内容</li>
<li>将爬取结果保存到数据库中</li>
</ul>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">class Test01Pipeline:
    def process_item(self, item, spider):
        return item</code></pre>

<h4 id="12-9-整体流程"><a href="#12-9-整体流程" class="headerlink" title="12.9  整体流程"></a>12.9  整体流程</h4><blockquote>
<ol>
<li>需要在items中，定义要爬取的字段</li>
<li>在爬虫组件中，定义要爬取的<code>url</code>和解析规则</li>
<li>在<code>settings</code>中，配置相关的参数</li>
<li>在<code>pipeline</code>中，自定义类,实现<code>process_item</code>方法<br>在爬虫组件生成的item，通过了<code>yield</code>传递到<code>process_item</code>方法中</li>
<li>将数据保存在<code>MongoDB、mysql</code>等数据库中,也可以将数据保存到<code>json</code>等文件里</li>
</ol>
</blockquote>
<h4 id="12-91-手动发送请求"><a href="#12-91-手动发送请求" class="headerlink" title="12.91  手动发送请求"></a>12.91  手动发送请求</h4><blockquote>
<p><code>srcapy</code>框架在启动时，会自动对起始<code>url</code>发起请求，是因为爬虫组件继承的<code>scrapy.Spider</code>实现了一个<code>start_requests</code>方法，所以，想要手动发送请求，需要自己在类中实现此方法，这样，便不会再自动继承父类的方法，而是从自己类中实现方法</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#手动发送get请求
#callback参数，是指定回调，如果不指定此参数，就会默认回调到parse
yield scrapy.Request(url=url,callback=self.parse)

#手动发送post请求
yield scrapy.FormRequest(url=next_page,formdata=data)</code></pre>

<h4 id="12-92-中间件"><a href="#12-92-中间件" class="headerlink" title="12.92  中间件"></a>12.92  中间件</h4><h5 id="篡改UA"><a href="#篡改UA" class="headerlink" title="篡改UA"></a>篡改<code>UA</code></h5><blockquote>
<p>需要用到的模块<code>fake-useragent</code>，用<code>pip</code>安装</p>
</blockquote>
 <pre class="language-python" data-language="python"><code class="language-python">#在middlewares中，TestIpUaDownloaderMiddleware下载器中间件中，
#导包
from fake_useragent import UserAgent

#实例化对象
ua = UserAgent()

def process_request(self, request, spider):
    request.headers['User-Agent'] = ua.random
    return None

ua.random
#生成的ua是随机的</code></pre>

<blockquote>
<p>注意，需要把<code>settings</code>中的<code>DOWNLOADER_MIDDLEWARES</code>打开</p>
</blockquote>
<h5 id="篡改ip"><a href="#篡改ip" class="headerlink" title="篡改ip"></a>篡改<code>ip</code></h5><pre class="language-python" data-language="python"><code class="language-python">def process_request(self, request, spider):
    #篡改ip
    request.meta['proxy'] = 'http://ip:port'
    return None</code></pre>

<h5 id="selenium的使用"><a href="#selenium的使用" class="headerlink" title="selenium的使用"></a><code>selenium</code>的使用</h5><blockquote>
<ol>
<li>拦截res响应，在<code>process_response</code>方法中</li>
<li>使用<code>selenium</code>抓取数据</li>
<li>构建一个响应对象</li>
<li>替换原有的响应对象</li>
</ol>
</blockquote>
<blockquote>
<p>注意！如果在中间件中实例化对象，那么，每次请求都会实例化对象，比较耗费资源，所以，最好在爬虫组件中实例化</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#爬虫组件中
from selenium import webdriver
#构建响应对象
from scrapy.http import HttpResponse

bro = webdriver.Chrome(executable_path='设置绝对路径')

#中间件中，使用方法process_response自带的spider参数，从爬虫组件可以直接传过去
bro = spider.bro
bro.get(url=response.url)
#设置滚动一屏脚本
js = 'window.scrollTo(0,document.body.scrollHeight)'
#执行js脚本
bro.execute_script(js)
#网页源码
html = bro.page_source

#自己构建响应对象
myres = HttpResponse(url=response.url,body=html,encoding='utf-8',request=request)

#替换原来的响应对象
return myres</code></pre>

<blockquote>
<p>根据需要添加判断是否使用<code>selenium</code></p>
</blockquote>
<h4 id="12-93-meta传值"><a href="#12-93-meta传值" class="headerlink" title="12.93  meta传值"></a>12.93  <code>meta</code>传值</h4><blockquote>
<p>在爬虫组件中，如果回调解析方法之间相互传值，可以使用<code>meta</code></p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">def parse(self,response):
    ...
    yield scrapy.Request(url=url,callback=self.da_parse,meta={'title':title})
    
def da_parse(self,response):
    title = response.meta['title']</code></pre>

<h4 id="12-94-全站数据爬取"><a href="#12-94-全站数据爬取" class="headerlink" title="12.94   全站数据爬取"></a>12.94   全站数据爬取</h4><blockquote>
<p>全站数据爬取就是爬取多页时自动帮助翻页，省去了写代码的时间</p>
</blockquote>
<blockquote>
<p>创建爬虫文件时，在<code>genspider</code>后，添加<code>-t crawl</code>再添加爬虫名和域名，这样，爬虫会成为一个全站爬虫</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">#链接提取器，根据给定的规则进行链接的提起
link = LinkExtractor(allow=r'Items/')
    rules = (
        #规则解析器，根据给定的规则解析数据，follow是递归提取页码，就是逐级访问url，对各个url来追踪访问
        Rule(link,callback='parse_item', follow=True),
    )</code></pre>

<h4 id="12-95-增量式爬虫"><a href="#12-95-增量式爬虫" class="headerlink" title="12.95  增量式爬虫"></a>12.95  增量式爬虫</h4><blockquote>
<p>增量式就是自写爬虫监控目标网站，如果目标有更新的数据，则将更新的数据爬取下来，已经爬取的数据，则通过去重的方式来忽略</p>
</blockquote>
<blockquote>
<p>而这其中，又分为两种去重方式，分别是根据<code>url</code>去重和数据指纹去重</p>
</blockquote>
<pre class="language-python" data-language="python"><code class="language-python">from redis import Redis
conn = Redis('127.0.0.1',6379)

def detail_parse(self,response):
    name = li.xpath('//text()')
    content = li.xpath('//text()')
    #连起来生成指纹
    data = name+content
    fp = md5(data.encode('utf-8')).hexdigest()
    #添加进redis的集合中
    conn.sadd('ggsfp',fp)
    #如果返回结果为0，则证明已经添加过，如果为1，则没有添加过</code></pre>

<p>对应<code>day14</code>视频</p>
<hr>
<h2 id="13-MongoDB数据库"><a href="#13-MongoDB数据库" class="headerlink" title="13.MongoDB数据库"></a>13.<code>MongoDB</code>数据库</h2><h4 id="13-1-数据库操作"><a href="#13-1-数据库操作" class="headerlink" title="13.1  数据库操作"></a>13.1  数据库操作</h4><blockquote>
<ol>
<li>创建数据库并切换至该数据库下<br><code>use 库名</code></li>
<li>查看当前数据库<br><code>db</code></li>
<li>查看所有数据库，只能显示<strong>非空</strong>的数据库<br><code>show dbs</code></li>
<li>创建表<br><code>db.creaeteCollection('表名')</code></li>
<li>插入数据,如果没有这个表，会自动创建<br><code>db.表名.insert({'字段':'值})</code></li>
<li>查询所有数据<br><code>db.表名.find()</code></li>
<li>查看所有表<br><code>show tables</code></li>
<li>删除表<br><code>db.表名.drop()</code></li>
<li>删除库<br><code>db.表名.dropDatabase()</code></li>
</ol>
</blockquote>
<h4 id="13-2-增加数据"><a href="#13-2-增加数据" class="headerlink" title="13.2  增加数据"></a>13.2  增加数据</h4><blockquote>
<p>字段多少是单独的那条数据，不一致也没有关系</p>
<p>例如：</p>
<p>{‘name’:’玛卡巴卡’}—{‘name’:’海绵宝宝’,’age’:200}</p>
</blockquote>
<ol>
<li><code>db.表名.insertOne({'字段名':'值})</code></li>
<li><code>db.表名.insert({'字段':'值})</code></li>
<li><code>db.表名.insertMany([{'字段':'值},{'字段':'值}])</code></li>
</ol>
<h4 id="13-3-查询数据"><a href="#13-3-查询数据" class="headerlink" title="13.3  查询数据"></a>13.3  查询数据</h4><ul>
<li>简单查询<br><code>db.表名.find()</code></li>
<li>分页，返回2条查询数据<br><code>db.表名.find({}).limit(2)</code></li>
<li>排序，*<strong>1**<em>是升序，</em></strong>-1***是降序<br><code>db.表名.find().sort({'price':1})</code></li>
</ul>
<blockquote>
<p>等值查询</p>
</blockquote>
<ul>
<li><p>等值查询（按照条件查询）<br><code>db.表名.find({'price':4.5})</code></p>
</li>
<li><p>非等值查询（范围查询）</p>
</li>
<li><p><code>({'price':{'$lt':500,'$gt':100}})#小于500，大于100</code></p>
</li>
<li><p><strong><em>或</em></strong>查询<br><code>db.表名.find({$or:[{'price':{'$lt':5.5}},{'price':{$gt:500}} ]})</code></p>
</li>
<li><pre class=" language-python"><code class="language-python"><span class="token operator">></span>   <span class="token operator">==</span> $gt  <span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'price'</span><span class="token punctuation">:</span><span class="token punctuation">{</span>$gt<span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#价格大于100的</span>
<span class="token operator">&lt;</span>   <span class="token operator">==</span> $lt  <span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'price'</span><span class="token punctuation">:</span><span class="token punctuation">{</span>$lt<span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#价格小于100的</span>
<span class="token operator">&lt;=</span>  <span class="token operator">==</span> $lte <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">>=</span>  <span class="token operator">==</span> $gte <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">!=</span>  <span class="token operator">==</span> $ne  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre>
</li>
</ul>
<blockquote>
<p>模糊查询</p>
</blockquote>
<ul>
<li><code>db.表名.find({'字段名':{$regex:'正则表达式'}})</code></li>
</ul>
<h4 id="13-4-修改数据"><a href="#13-4-修改数据" class="headerlink" title="13.4  修改数据"></a>13.4  修改数据</h4><p><code>db.表名.update({'price':2.5},{$set:{'price':500}})</code></p>
<h4 id="13-5-删除数据"><a href="#13-5-删除数据" class="headerlink" title="13.5  删除数据"></a>13.5  删除数据</h4><p><code>db.表名.remove({'name':'玛卡巴卡'})</code></p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>天听</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://godhearing.cn/2019/08/01/pa-chong-de-xiang-guan-zhi-shi/" title="爬虫的相关知识">https://godhearing.cn/2019/08/01/pa-chong-de-xiang-guan-zhi-shi/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless otherwise stated.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2019/08/05/python-shu-ju-jie-gou-yu-suan-fa/" rel="prev" title="python数据结构与算法"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">python数据结构与算法</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2019/04/17/paypal-kua-jing-zhi-fu/" rel="next" title="PayPal跨境支付"><span class="post-nav-text">PayPal跨境支付</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="https://beian.miit.gov.cn/" target="_blank">冀ICP备2021002475号-1</a></div><div class="copyright"><span>&copy; 2017 – 2022 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 天 听</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v5.4.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.4.0</span></div><div class="live_time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2017-07-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " 天 " + passHour + " 小时 " + passMinute + " 分 " + passSecond + " 秒";
}
blog_live_time();
</script></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#ff8c00" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="Search"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="/js/search/local-search.js" defer></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="开始面向搜索引擎编程" value=""></div><div id="local-search-result"></div></div></div></body></html>