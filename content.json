{"meta":{"title":"天听的技术博客","subtitle":"天听","description":"","author":"天 听","url":"http://godhearing.cn","root":"/"},"pages":[{"title":"404","date":"2018-09-30T09:25:30.000Z","updated":"2021-02-09T10:20:18.000Z","comments":true,"path":"/404.html","permalink":"http://godhearing.cn/404.html","excerpt":"","text":""},{"title":"关于我","date":"2021-02-09T10:18:55.000Z","updated":"2021-03-11T07:03:12.000Z","comments":true,"path":"about/index.html","permalink":"http://godhearing.cn/about/index.html","excerpt":"","text":"Me代号天听，英文名salmon 积极向上，每天充满干劲🤭 偶尔也会抓狂到想掐人🤬 在下年满十八，遵纪守法，爱国爱家，不碰烟酒，善养鸡鸭，多素少荤，爱吃西瓜。 爱好：发呆、旅游、听人讲故事、给人讲故事、一切有趣的东西、摸鱼 喜欢：山楂糖、LOL 讨厌：热闹到爆的任何地方，太可怕啦ヽ(*。&gt;Д&lt;)o゜ 运动：跑步、跳绳、俯卧撑 恢复巅峰状态，指日可待，O(∩_∩)O 梦想：心理大师、技术大牛、优秀的LOL职业电竞选手、 最爱的音乐：吹梦到西州 最喜欢的反派：小丑 最喜欢的一句话：真相，从来就难以求寻，深渊中看见的光不一定是出口，还有可能是魔鬼的虹膜 最喜欢的轻音乐：The Truth That You Leave"},{"title":"站点","date":"2021-02-09T10:18:55.000Z","updated":"2021-03-11T07:01:38.000Z","comments":true,"path":"about/site.html","permalink":"http://godhearing.cn/about/site.html","excerpt":"","text":"哈？！站点？"},{"title":"","date":"2021-03-16T12:17:38.086Z","updated":"2021-02-09T11:00:02.000Z","comments":true,"path":"albums/index.html","permalink":"http://godhearing.cn/albums/index.html","excerpt":"","text":"type: albums albums: - caption: 凛冬将至 url: /albums/sunset.html cover: https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=514535248,792512535&amp;fm=15&amp;gp=0.jpg desc: 黑"},{"title":"categories","date":"2020-10-30T11:18:17.000Z","updated":"2021-03-09T10:34:06.000Z","comments":false,"path":"categories/index.html","permalink":"http://godhearing.cn/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2020-10-30T11:23:28.000Z","updated":"2020-11-19T11:05:04.000Z","comments":true,"path":"friends/index.html","permalink":"http://godhearing.cn/friends/index.html","excerpt":"","text":""},{"title":"","date":"2021-03-16T12:17:38.175Z","updated":"2021-02-09T11:57:22.000Z","comments":true,"path":"data/sentences.json","permalink":"http://godhearing.cn/data/sentences.json","excerpt":"","text":"[{\"content\":\"我们还会一起待多久..比永远更久\",\"from\":\"千珏\"},{\"content\":\"明天，只是一个希望，不是一个承诺\",\"author\":\"千珏\"},{\"content\":\"在被一套套完全相反的理论说的无法取舍的时候，哪个是真正让你感动的，对你来说，哪个就是对的。\",\"from\":\"天听\"},{\"content\":\"那些逃离死亡的人，其生命，早已停滞不前。\",\"from\":\"千珏\"},{\"content\":\"只有弱者才会恐惧阴影，于是恐惧便压倒了他们\",\"from\":\"该隐\"},{\"content\":\"世界即不黑，也不白，只是一道，精致的...灰\",\"from\":\"卡密尔\"},{\"content\":\"若不是你爱的人住在里面，这个世界也不过如此\",\"from\":\"天听\"},{\"content\":\"在你成功光芒的照耀下，人们便会忘记你手段的黑暗\",\"from\":\"天听\"},{\"content\":\"已经被逼上绝路，唯一的办法，就是置之死地而后生\",\"from\":\"天听\"},{\"content\":\"值得期待的路只有前方\",\"from\":\"天听\"},{\"content\":\"善于伪装的人，往往需要格外小心\",\"from\":\"天听\"},{\"content\":\"谎言的代价并不是我们会把谎言当成事实，真正危险的是，如果我们听了太多谎言，就再也认不清事实了。\",\"from\":\"天听\"},{\"content\":\"手段并不能说明什么，目的才能分辨正邪\",\"from\":\"天听\"},{\"content\":\"正是因为我见过太多黑暗，所以我才更渴望光明\",\"from\":\"天听\"},{\"content\":\"向黑暗中走去的人，或许会被黑暗吞噬，或许会化作明灯\",\"from\":\"天听\"},{\"content\":\"人和蝎子最大的区别在于，蝎子往往亮出自己的狠毒，而人则会将之隐藏\",\"from\":\"天听\"},{\"content\":\"天空其实是无色的，但眼睛欺骗了你。灵魂其实是无欲的，但感情欺骗了你\",\"from\":\"天听\"},{\"content\":\"规则的指定往往是用来掩盖游戏的本质，用来迷惑那些执着于胜负的人\",\"from\":\"天听\"},{\"content\":\"如果你感到寸步难行，也许是耀眼的光明蒙蔽了你的眼睛\",\"from\":\"天听\"},{\"content\":\"能者成霸道之势，强者隐百转之谋\",\"from\":\"天听\"}]"},{"title":"contact","date":"2020-10-30T11:20:56.000Z","updated":"2020-11-19T11:05:04.000Z","comments":true,"path":"contact/index.html","permalink":"http://godhearing.cn/contact/index.html","excerpt":"","text":""},{"title":"我的小伙伴们","date":"2021-03-16T12:17:38.179Z","updated":"2021-03-09T09:41:12.000Z","comments":true,"path":"links/index.html","permalink":"http://godhearing.cn/links/index.html","excerpt":"","text":""},{"title":"喜欢的女孩子","date":"2021-03-16T12:17:38.177Z","updated":"2021-02-09T10:55:54.000Z","comments":true,"path":"girls/index.html","permalink":"http://godhearing.cn/girls/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-10-30T11:19:26.000Z","updated":"2021-03-09T10:33:48.000Z","comments":false,"path":"tags/index.html","permalink":"http://godhearing.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"GORM入门","slug":"GORM","date":"2021-06-23T04:03:51.149Z","updated":"2021-06-23T09:34:18.362Z","comments":true,"path":"2021/06/23/gorm/","link":"","permalink":"http://godhearing.cn/2021/06/23/gorm/","excerpt":"","text":"前言 GORM，是Go语言中的ORM(对象关系映射)，什么是ORM，是通过使用描述对象和数据库之间映射的元数据，将程序中的对象与关系数据库相互映射 坦白来说，就是一个简略sql语句的工具，每个人写sql语句习惯不同，不一定每个人都能看得懂，但是，换成orm后，统一的格式，无论在业务交接上还是便捷度上，都有很大的提升。 推荐去官网阅读文档，中文文档非常简便，我也不想搬过来了，就简简单单的使用一下。 官网 安装go get -u gorm.io/gorm 直接go get安装即可，GORM 官方支持的数据库类型有： MySQL, PostgreSQL, SQlite, SQL Server，本文章就以mysql为例。 连接数据库import ( \"fmt\" \"gorm.io/driver/mysql\" \"gorm.io/gorm\" ) func main() { // 参考 https://github.com/go-sql-driver/mysql#dsn-data-source-name 获取详情 dsn := \"账号:密码@tcp(IP:端口号)/数据库名字?charset=utf8mb4&amp;parseTime=True&amp;loc=Local\" db, err := gorm.Open(mysql.Open(dsn), &amp;gorm.Config{}) fmt.Println(db) fmt.Println(err) } 定义模型就使用结构体即可，不过需要一点，实现一个TableName接口，否则他会在映射时表名多加一个s。比如有个表user，只有id和name两个字段 type User struct { ID uint Name string } func (User) TableName() string { return \"user\" } 如果不写这个TableName的话，默认会映射成为users表。 CreateGorm是我见过最简单的orm了，只需要将数据指针地址传过去，就ok import ( \"fmt\" \"gorm.io/driver/mysql\" \"gorm.io/gorm\" ) type User struct { ID uint Name string } func (User) TableName() string { return \"user\" } func main() { user := User{ID: 0, Name: \"Salmon\"} dsn := \"root:root@tcp(127.0.0.1:3306)/test01?charset=utf8mb4&amp;parseTime=True&amp;loc=Local\" db, err := gorm.Open(mysql.Open(dsn), &amp;gorm.Config{}) if err!= nil{ panic(err) } result := db.Create(&amp;user) fmt.Println(user.ID) } user.ID // 返回插入数据的主键 result.Error // 返回 error result.RowsAffected // 返回插入记录的条数 ReadGORM 提供了 First、Take、Last 方法，以便从数据库中检索单个对象。当查询数据库时它添加了 LIMIT 1 条件，且没有找到记录时，它会返回 ErrRecordNotFound 错误。 // 获取第一条记录（主键升序） db.First(&amp;user) // SELECT * FROM users ORDER BY id LIMIT 1; // 获取一条记录，没有指定排序字段 db.Take(&amp;user) // SELECT * FROM users LIMIT 1; // 获取最后一条记录（主键降序） db.Last(&amp;user) // SELECT * FROM users ORDER BY id DESC LIMIT 1; result := db.First(&amp;user) result.RowsAffected // 返回找到的记录数 result.Error // returns error // 条件查询 db.Where(\"id=?\", 19).Find(&amp;users) // 检查 ErrRecordNotFound 错误 errors.Is(result.Error, gorm.ErrRecordNotFound) 主键检索 db.First(&amp;user, 10) // SELECT * FROM users WHERE id = 10; db.First(&amp;user, \"10\") // SELECT * FROM users WHERE id = 10; db.Find(&amp;users, []int{1,2,3}) // SELECT * FROM users WHERE id IN (1,2,3); 查询所有，使用Find方法，并且，接收值为数组。 var users []User //users := make([]User, 10) dsn := \"root:root@tcp(127.0.0.1:3306)/test01?charset=utf8mb4&amp;parseTime=True&amp;loc=Local\" db, err := gorm.Open(mysql.Open(dsn), &amp;gorm.Config{}) if err!= nil{ panic(err) } db.Find(&amp;users) fmt.Println(users) Update更新单个列，当使用 Update 更新单个列时，你需要指定条件，否则会返回 ErrMissingWhereClause 错误，查看 Block Global Updates 获取详情。当使用了 Model 方法，且该对象主键有值，该值会被用于构建条件 db.Model(User{}).Where(\"id=?\", 19).Update(\"role\",\"Salmon\") fmt.Println(users) // db.Model(&amp;users).Where(\"id=?\", 19).Update(\"role\",\"godhearing\") 更新多列 Updates 方法支持 struct 和 map[string]interface{} 参数。当使用 struct 更新时，默认情况下，GORM 只会更新非零值的字段 db.Model(&amp;users).Where(\"id=?\", 19).Updates(User{ID: 20,Role: \"godhearing\"}) 只更新某些字段(Select)，忽略某些字段(Omit) db.Model(&amp;users).Where(\"id=?\", 20).Select(\"role\").Updates(User{ID: 21,Role: \"龙\"}) db.Model(&amp;users).Where(\"id=?\", 20).Omit(\"role\").Updates(User{ID: 21,Role: \"龙\"}) Delete根据主键删除 db.Delete(&amp;User{}, 22) 条件删除 db.Where(\"role = ?\", \"asdasd\").Delete(&amp;users)","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"}],"author":"天听"},{"title":"Go编译遇到xxx/go.mod malformed record data 问题","slug":"编译遇到xxxgo.mod malformed record data 问题","date":"2021-06-22T08:39:08.227Z","updated":"2021-06-22T09:39:11.035Z","comments":true,"path":"2021/06/22/bian-yi-yu-dao-xxxgo.mod-malformed-record-data-wen-ti/","link":"","permalink":"http://godhearing.cn/2021/06/22/bian-yi-yu-dao-xxxgo.mod-malformed-record-data-wen-ti/","excerpt":"","text":"前言 最近在慢慢转语言，在go语言下，编译时或者mod下，会偶尔莫名出现 xxx/go.mod malformed record data的问题 我理解是这样的，go 1.13 可以设置多个代理，在某个下载失败时，他会换代理下载代理设置：https://goproxy.io,direct 就是连个代理直接下载，或者使用goproxy.io下载当使用直接下载时，会通过默认代理“sum.golang.org”，这个代理需要FQ验证，所以会失败，切换goproxy.io 是报的这个错误，Google 上查可能是跟这个代理bug 有关 解决方案关掉它的默认代理 go env -w GOSUMDB=off 由于目前转go，暂时可以用这个进行解决，不太明确是否有后患，记录一下","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"},{"name":"bug","slug":"bug","permalink":"http://godhearing.cn/tags/bug/"}],"author":"天听"},{"title":"微服务架构初识","slug":"微服务架构初识","date":"2021-06-17T07:14:36.634Z","updated":"2021-06-17T08:07:46.244Z","comments":true,"path":"2021/06/17/wei-fu-wu-jia-gou-chu-shi/","link":"","permalink":"http://godhearing.cn/2021/06/17/wei-fu-wu-jia-gou-chu-shi/","excerpt":"","text":"前言 什么是微服务，微服务是使用一套小服务来开发单个应用的方式，每个服务运行在独立的进程里，一般采用轻量级的通讯机制互联，并且它们可以通过自动化的方式部署 什么叫微？ 单一功能 代码少，不是，而且代码多 架构变的复杂了 微服务是设计思想，不是量的体现 微服务特点 单一职责，此时项目专注于登录和注册 轻量级的通信，通信与平台和语言无关，http是轻量的，例如java的RMI属于重量的 隔离性，数据隔离 有自己的数据 技术多样性 互联网架构互联网的架构演变之路，是从一开始的单体架构，到垂直架构，再到SOA架构，直至微服务架构。 单体架构 所有功能放一个项目里 应用和数据库服务器可能部在一起，分开部 优点： 简单，高效，小型项目 缺点： 扛不住 技术栈受限 垂直架构 将大项目架构拆分成一个一个单体架构 优点： 不至于像单体无限扩大 缺点： 有瓶颈 成本高 SOA架构，面向服务的编程 ESB，比较传统的中间件技术 优点： 代码提高重用性，ESB接口解耦 针对不同服务，做不同数据层和部署 缺点： ESB比较重量级 对于开发人员来说，系统层和服务层界限模糊 微服务架构 每个功能抽取成一个一个的服务 微服务之间访问是轻量级的，RPC 微服务架构的优势： 独立性 使用者容易理解 技术栈灵活 高效团队 缺点： 额外的工作，服务的拆分 保证数据一致性 增加了沟通成本","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"},{"name":"微服务","slug":"微服务","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"author":"天听"},{"title":"gin框架","slug":"gin","date":"2021-06-15T04:00:29.978Z","updated":"2021-06-16T06:41:45.046Z","comments":true,"path":"2021/06/15/gin/","link":"","permalink":"http://godhearing.cn/2021/06/15/gin/","excerpt":"","text":"前言 Gin是一个golang的微框架，封装比较优雅，API友好，源码注释比较明确，具有快速灵活，容错方便等特点 对于golang而言，web框架的依赖要远比Python，Java之类的要小。自身的net/http足够简单，性能也非常不错 借助框架开发，不仅可以省去很多常用的封装带来的时间，也有助于团队的编码风格和形成规范 安装要安装Gin软件包，需要安装Go并首先设置Go工作区。 1.首先需要安装Go（需要1.10+版本），然后可以使用下面的Go命令安装Gin。 go get -u github.com/gin-gonic/gin 2.将其导入代码中： import “github.com/gin-gonic/gin” 3.（可选）导入net/http。例如，如果使用常量，则需要这样做http.StatusOK。 import “net/http” 需要注意的是，直接下载gin有可能会报错，因为毕竟这不是国内的地址，可以通过以下命令来更改源：go env -w GOPROXY=https://goproxy.cn,direct 下载完毕后，去go工程目录下的/pkg/mod/github.com/gin-gonic/查看是否有gin，如果名字带着版本什么的，改成gin就好了，否则你在引入的时候还有可能会找不到这个包。 测试一下新建一个文件main.go package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { // 1.创建路由 r := gin.Default() // 2.绑定路由规则，执行的函数 // gin.Context，封装了request和response r.GET(\"/\", func(c *gin.Context) { c.String(http.StatusOK, \"hello Salmon!\") }) // 3.监听端口，默认在8080 // Run(\"里面不指定端口号默认为8080\") r.Run(\":8020\") } 直接运行或者编译 [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET / --> main.main.func1 (3 handlers) [GIN-debug] Listening and serving HTTP on :8020 ok，启动完毕，然后我们来访问一下http://127.0.0.1:8020/，显示hello Salmon!，大功告成。 path参数 path参数可以根据:xxx来定义，通过Param取值，或者通过Params.Get的方式来取值。 package main import ( \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" ) func main() { r := gin.Default() r.GET(\"/user/:name/:com\", func(c *gin.Context) { ok,err := c.Params.Get(\"name\") // fmt.Println(c.Param(\"name\")) fmt.Println(ok, err) c.JSON(http.StatusOK,gin.H{\"message\":\"你好，tt \"}) }) r.Run(\"0.0.0.0:8020\") } 查询参数 查询参数，又叫query参数，一般是拼接在url后面的参数，通过?开始，不同的查询参数之间，用&amp;来拼接，例如:xxx.com/?id=1111&amp;name=salmon, func main() { r := gin.Default() r.GET(\"/user\", func(c *gin.Context) { req := c.DefaultQuery(\"name\",\"天听\") fmt.Println(req) c.JSON(http.StatusOK,gin.H{\"msg\":req}) }) r.Run(\"0.0.0.0:8020\") } 可以通过DefaultQuery或者Query来取值，Query自不必说，DefaultQuery是有带一个默认值的。参数不存在，返回默认值，Query()若不存在，返回空串。 表单参数 表单传输为post请求，http常见的传输格式为四种： application/json application/x-www-form-urlencoded application/xml multipart/form-data 表单参数可以通过PostForm()方法获取，该方法默认解析的是x-www-form-urlencoded或from-data格式的参数 我们先定义一个表单 &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;title>Salmon&lt;/title> &lt;/head> &lt;body> &lt;form action=\"http://localhost:8020/form\" method=\"post\" action=\"application/x-www-form-urlencoded\"> 用户名：&lt;input type=\"text\" name=\"username\" placeholder=\"请输入你的用户名\"> &lt;br> 密&amp;nbsp;&amp;nbsp;&amp;nbsp;码：&lt;input type=\"password\" name=\"userpassword\" placeholder=\"请输入你的密码\"> &lt;br> &lt;input type=\"submit\" value=\"提交\"> &lt;/form> &lt;/body> &lt;/html> 然后使用PostForm或者DefaultPostForm来取值。 上传单个文件 multipart/form-data格式用于文件上传 gin文件上传与原生的net/http方法类似，不同在于gin把原生的request封装到c.Request中 还是先定义一个html &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;title>Document&lt;/title> &lt;/head> &lt;body> &lt;form action=\"http://localhost:8080/upload\" method=\"post\" enctype=\"multipart/form-data\"> 上传文件:&lt;input type=\"file\" name=\"file\" > &lt;input type=\"submit\" value=\"提交\"> &lt;/form> &lt;/body> &lt;/html> 后台： package main import ( \"github.com/gin-gonic/gin\" \"net/http\" ) func main() { r := gin.Default() //限制上传最大尺寸 r.MaxMultipartMemory = 8 &lt;&lt; 20 r.POST(\"/upload\", func(c *gin.Context) { file, err := c.FormFile(\"file\") if err != nil { c.String(500, \"上传图片出错\") } // c.JSON(200, gin.H{\"message\": file.Header.Context}) c.SaveUploadedFile(file, file.Filename) c.String(http.StatusOK, file.Filename) }) r.Run(\":8020\") } 也可以加一些限制： func main() { r := gin.Default() r.POST(\"/upload\", func(c *gin.Context) { _, headers, err := c.Request.FormFile(\"file\") if err != nil { log.Printf(\"Error when try to get file: %v\", err) } //headers.Size 获取文件大小 if headers.Size > 1024*1024*2 { fmt.Println(\"文件太大了\") return } //headers.Header.Get(\"Content-Type\")获取上传文件的类型 if headers.Header.Get(\"Content-Type\") != \"image/png\" { fmt.Println(\"只允许上传png图片\") return } c.SaveUploadedFile(headers, \"./video/\"+headers.Filename) c.String(http.StatusOK, headers.Filename) }) r.Run(\":8020\") } 上传多个文件 照旧，html先搞上 &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;title>Document&lt;/title> &lt;/head> &lt;body> &lt;form action=\"http://localhost:8020/upload\" method=\"post\" enctype=\"multipart/form-data\"> 上传文件:&lt;input type=\"file\" name=\"files\" multiple> &lt;input type=\"submit\" value=\"提交\"> &lt;/form> &lt;/body> &lt;/html> 后台: package main import ( \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" ) func main() { // 1.创建路由 // 默认使用了2个中间件Logger(), Recovery() r := gin.Default() // 限制表单上传大小 8MB，默认为32MB r.MaxMultipartMemory = 8 &lt;&lt; 20 r.POST(\"/upload\", func(c *gin.Context) { form, err := c.MultipartForm() if err != nil { c.String(http.StatusBadRequest, fmt.Sprintf(\"get err %s\", err.Error())) } // 获取所有图片 files := form.File[\"files\"] // 遍历所有图片 for _, file := range files { // 逐个存 if err := c.SaveUploadedFile(file, file.Filename); err != nil { c.String(http.StatusBadRequest, fmt.Sprintf(\"upload err %s\", err.Error())) return } } c.String(200, fmt.Sprintf(\"upload ok %d files\", len(files))) }) //默认端口号是8080 r.Run(\":8020\") } routes group routes group是为了管理一些相同的URL，例如迭代版本不同 举个例子： r := gin.Default() v1 := r.Group(\"/v1\") v1.GET(\"/login/\",func(c *gin.Context) { c.String(200,\"v1版本\") }) v2 := r.Group(\"/v2\") v2.GET(\"/login/\", func(c *gin.Context) { c.String(200, \"v2版本\") }) url分别可以访问：/v1/login/或者v2/login/ gin的异步操作 goroutine机制可以方便地实现异步处理 另外，在启动新的goroutine时，不应该使用原始上下文，必须使用它的只读副本 r.GET(\"/long_async\", func(c *gin.Context) { // 需要搞一个副本 copyContext := c.Copy() //fmt.Println(copyContext.Request.URL) // 异步处理 go func() { time.Sleep(3 * time.Second) log.Println(\"异步执行：\" + copyContext.Request.URL.Path) }() }) // 2.同步 r.GET(\"/long_sync\", func(c *gin.Context) { time.Sleep(3 * time.Second) log.Println(\"同步执行：\" + c.Request.URL.Path) })","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"}],"author":"天听"},{"title":"sqlalchemy-union","slug":"sqlalchemy-union","date":"2021-06-09T09:27:12.629Z","updated":"2021-06-09T09:42:52.708Z","comments":true,"path":"2021/06/09/sqlalchemy-union/","link":"","permalink":"http://godhearing.cn/2021/06/09/sqlalchemy-union/","excerpt":"","text":"前言 union是mysql中的联合查询，他的作用就是将两条查询语句和为一句。举个例子： 将两个字段相似的表里的数据统一起来展示在一个统计页面。如果是单纯的展示数据那很简单，两个表查出来之后组合一下就完事了，但是有坑的地方就是分页和按照时间搜索，这两个功能决定了不可能单独查询两张表。 想要完成这个功能，使用union的联合查询可以很轻易的做到。 定义数据表 定义 两张表，字段类型相同，但名称不同。 #coding:utf-8 from sqlalchemy import Column,CHAR,INTEGER from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker Base = declarative_base() class User(Base): __tablename__ = \"user\" id = Column(CHAR(20),primary_key = True) name = Column(CHAR(20)) age = Column(INTEGER) class Teacher(Base): __tablename__ = \"teacher\" id = Column(CHAR(20),primary_key = True) tec_name = Column(CHAR(20)) tec_age = Column(INTEGER) engine = create_engine('mysql+mysqldb://root:12345678@localhost:3306/test') def create_table(table_name): table_name.metadata.create_all(engine) print \"创建成功\" def insert_data(): DBSession = sessionmaker(bind=engine) session = DBSession() for x in range(10): temp = {} temp['id'] = x temp['name'] = 'user_' + str(x) temp['age'] = x user = User(**temp) session.add(user) for x in range(15): temp = {} temp['id'] = x temp['tec_name'] = 'tec_' + str(x) temp['tec_age'] = x * 2 tec = Teacher(**temp) session.add(tec) session.commit() session.close() print('success') if __name__ = '__main__': create_table(User) create_table(Teacher) insert_data() User表字段： mysql> desc user; +-------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+----------+------+-----+---------+-------+ | id | char(20) | NO | PRI | NULL | | | name | char(20) | YES | | NULL | | | age | int(11) | YES | | NULL | | +-------+----------+------+-----+---------+-------+ 3 rows in set (0.00 sec) User表数据： mysql> select * from user; +----+--------+------+ | id | name | age | +----+--------+------+ | 0 | user_0 | 0 | | 1 | user_1 | 1 | | 2 | user_2 | 2 | | 3 | user_3 | 3 | | 4 | user_4 | 4 | | 5 | user_5 | 5 | | 6 | user_6 | 6 | | 7 | user_7 | 7 | | 8 | user_8 | 8 | | 9 | user_9 | 9 | +----+--------+------+ 10 rows in set (0.00 sec) Teacher表字段： mysql> desc teacher; +----------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+----------+------+-----+---------+-------+ | id | char(20) | NO | PRI | NULL | | | tec_name | char(20) | YES | | NULL | | | tec_age | int(11) | YES | | NULL | | +----------+----------+------+-----+---------+-------+ 3 rows in set (0.00 sec) mysql> teacher表数据： mysql> select * from teacher; +----+----------+---------+ | id | tec_name | tec_age | +----+----------+---------+ | 0 | tec_0 | 0 | | 1 | tec_1 | 2 | | 10 | tec_10 | 20 | | 11 | tec_11 | 22 | | 12 | tec_12 | 24 | | 13 | tec_13 | 26 | | 14 | tec_14 | 28 | | 2 | tec_2 | 4 | | 3 | tec_3 | 6 | | 4 | tec_4 | 8 | | 5 | tec_5 | 10 | | 6 | tec_6 | 12 | | 7 | tec_7 | 14 | | 8 | tec_8 | 16 | | 9 | tec_9 | 18 | +----+----------+---------+ 15 rows in set (0.00 sec) 查询首先做一个简单的查询，将两个表的数据分别查出来 def select(): DBSession = sessionmaker(bind=engine) session = DBSession() table_data = session.query(User).all() session.close() for x in table_data: print(x.name,'------>',x.age) table_data = session.query(Teacher).all() session.close() for x in table_data: print(x.tec_name,'------>',x.tec_age) union原理，首先将第一张表的数据全部查询出来，然后将第二张表的数据全部查询出来，最后将两个数据使用union联合成一张新表，这张新表可以再次被筛选过滤，分页等。 def select(): DBSession = sessionmaker(bind=engine) session = DBSession() table_data = session.query(User).all() session.close() # for x in table_data: # print(x.name,'------>',x.age) # table_data = session.query(Teacher).all() # session.close() # for x in table_data: # print(x.tec_name,'------>',x.tec_age) user_data = session.query(User.name,User.age) tec_data = session.query(Teacher.tec_name.label('name'), Teacher.tec_age.label('age')) result = user_data.union_all(tec_data) for x in result: print(x.name,'------>',x.age) 在上面的查询中需要有一个注意点就是label，可以看到tec_data的查询语句中使用了label这个属性，该属性的作用是将Teacher这张表查询出来的tec_name 字段名称变成name，已达到和User表字段的统一，只有两张表的字段名称一致，类型一致的情况下才能联合查询。 另外还使用了一个union_all 字段，该字段的意思是如果两张表存在相同的记录也要全部展示出来，想要让相同的记录合并起来使用union即可。 往往查询出来还不是最终目的，还需要对查询出来的数据过滤。查询出来的数据不是一张正真的表，如果使用字段去匹配过滤条件呢？以查询出age 大于 5为例 ，有两种过滤方式： 1.使用User.age 作为筛选条件 2.使用Teacher.age 作为筛选条件 规则就是使用两张表里任意一张表的原始字段过滤即可，该过滤条件会在联合查询出来的结果起上作用。 使用 User 表字段def select(): DBSession = sessionmaker(bind=engine) session = DBSession() table_data = session.query(User).all() session.close() # for x in table_data: # print(x.name,'------>',x.age) # table_data = session.query(Teacher).all() # session.close() # for x in table_data: # print(x.tec_name,'------>',x.tec_age) user_data = session.query(User.name,User.age) tec_data = session.query(Teacher.tec_name.label('name'), Teacher.tec_age.label('age')) # result = user_data.union_all(tec_data) # for x in result: # print(x.name,'------>',x.age) result = user_data.union_all(tec_data).filter(User.age > 5) for x in result: print(x.name,'------>',x.age) 使用 Teacher 表字段 def select(): DBSession = sessionmaker(bind=engine) session = DBSession() table_data = session.query(User).all() session.close() # for x in table_data: # print(x.name,'------>',x.age) # table_data = session.query(Teacher).all() # session.close() # for x in table_data: # print(x.tec_name,'------>',x.tec_age) user_data = session.query(User.name,User.age) tec_data = session.query(Teacher.tec_name.label('name'), Teacher.tec_age.label('age')) result = user_data.union_all(tec_data).filter(Teacher.tec_age>5) for x in result: print(x.name,'------>',x.age) 关于union联合查询有一个说法很形象：join查询就像是横向扩展，将多张表的数据横向组合在一起，而union像是纵向扩展，将多张表数据纵向排列起来。","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://godhearing.cn/tags/SQLAlchemy/"}],"author":"天听"},{"title":"关于最近的迷茫和感受","slug":"关于最近的迷茫和感受","date":"2021-05-28T09:05:33.266Z","updated":"2021-06-22T03:03:49.930Z","comments":true,"path":"2021/05/28/guan-yu-zui-jin-de-mi-mang-he-gan-shou/","link":"","permalink":"http://godhearing.cn/2021/05/28/guan-yu-zui-jin-de-mi-mang-he-gan-shou/","excerpt":"","text":"对个人的思考感觉一直在学东西，从来没有停止过脚步，停止了，就会被干掉，但是呢，越学，越感到迷茫，学得都是应用层面的东西，就像是阮一峰老师分享的文章里面说的实施细节。 软件开发中，技术变化如此之快，花费了大量的时间学习技术和工具，一旦这些技术被取代，你的知识将变得毫无价值。 我最近总是在想这段话，软件开发算不算是真正的知识？ 如果它是一种真正的知识，那么理论上，我们学到的东西大部分应该不会过时，就好像微积分不会过时一样。可是实际上，我们都知道，软件开发技能有时效性，十年前学习的编程知识，十年后几乎肯定不能用于生产。那样的话，软件开发就不能算真正的知识，只是一种实施的细节。 以上内容出自阮一峰老师很早之前的一篇分享，对我影响很大，就在我满腔热血的准备学习一门新语言，学习新的知识。这篇文章像是一盆水，狠狠的让我停下脚步，开始了思考，假如说，10年后，没有人用我学习的这门语言了，我会的这门语言毫无价值，那我应该如何？ 现在去银行存款，去税务局交税，去医院看病，如果你是第一次来，肯定搞不清楚具体流程，必须有人教你，要带哪些证件，要填哪些表，要去哪些窗口排队等等。 当你掌握了流程，某一天，他改变了，你又该怎么办，又得有人教你流程。我目前的状况怕是和这个场景很相似，额，应该不只是我，所有都在搞应用层代码的码农都应该是这种情况，可是，我们赖以生存的，就是应用层的技术，你去搞底层，底层会给你发工资吗？我们这些人，未来出路在哪？多学一门语言，多学一个框架，多学一个思想，解决的只是广面的问题。想要在这个神仙云集的行业中出头，要解决的一定不只是广面，最重要的还是深度，不过，要付出的代价，怕是更多，当然了，也有说多劳多得的说法，话是没错，但是我认为，不全对。在本来就不平等的棋盘上，没有赢这么一说，有些人跟你对弈，你只有象和士，其进攻你的，是兵，车，马，炮，你该怎样赢他，赢不了，你只有拼尽全力，打出一个和。 或许在其他人看来，你已经很厉害了，但是，只有你自己知道，你和其他人的差距。总的来说，这盘棋，或许不存在赢的可能，但是你坚持的时间够久，那就算是逐渐的接近了胜利。这个时代已经变了，单枪匹马打天下，根本就不现实。脚下的路，唯有一条，提高自己的认知，不知道有没有注意过，你接触了一个新的认知，你达到了一个新的高度，永远有跟不上你脚步的人，他们会慢慢的消失在身后，但同时，你也能看到很多在你前面的人，他们和你一样，在提高着自己的认知，在不断的刷新迭代和优化自己。 你永远赚不到超出你认知范围之外的钱，除非你靠运气，但是，靠运气赚到的钱，往往又靠实力亏掉。你所赚的每一分钱，都是你对这个世界认知的变现，你所亏的每一分钱，也是因为你对这个世界认知的缺陷，这个世界最大的公平在于，当一个人的财富大于自己认知的时候，这个世界有一百种方法来收割你，直到你的认知和你的财富相匹配为止。 小矫情一下https://v2ex.com/t/729626，这是一个让我刷新认知的帖子，因为流量的变现，一个不起眼的应用，却能够赚着我们赚不到的钱。还有via浏览器，单一个华为应用市场，就有1200万的下载量，是这个帖子中例子的三四倍，在幕后赚的钱也无法估计，因为，贫穷限制了我的想象力。码农发家致富，成功几率相对来说还是不小的，只是还没有发现路。或者说，没注意到哪里有路。这世界很大，对于初出茅庐的我来说，就好像在风雨中的小船，随时可能会翻，只能眼睁睁着狂风暴雨，听天由命。 有人跟我说过，迷茫证明你在变强，可是…天天尼玛迷茫哪来的时间变强。不过有一点是对的，迷茫了，证明你已经在舒适区里呆不下去了，你要踏出这个舒适区了，或许你没有方向，或许你没有动力，但是只要找到了那个方向，便会不顾一切的朝着那个方向走去。 不过人生在世，活的就要一个自在，总以为骄傲的抬着头，就能看见头顶的月亮，但是有人说了，抬头不累吗，拿面镜子将他映在脚下不行吗？然后又有人说了，咱躺着看不行吗？是躺着不舒服吗？emmm….懒都没有懒到点上。 鸡汤横行的世界，都中了鸡汤的毒，很难有能保持初心的人，不过我始终相信，志向越大，付出的东西就会越多，如果还在迷茫，就适当的向生活低下头吧，低下头还迷茫大不了再抬起来嘛。不过这好像是在向生活磕头了，行了，就到这里吧，写之前我还是挺迷茫的，写完之后，突然就有些看开了，也许是因为快下班了吧。。。 哦对了，还有一碗鸡汤没灌，每当我觉得前路不明朗的时候，就会念一遍这句话，希望这鸡汤真的有用吧。 如果你种下了一颗种子，你不会每隔几分钟把它挖出来看看有没有发芽。所以你为什么要一直怀疑自己，怀疑自己的努力和决定。 耐心一点儿，不要想太多。不断的给自己的种子浇水就好了。","categories":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/categories/Book/"}],"tags":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/tags/Book/"}],"author":"天听"},{"title":"Centos7安装python3","slug":"Centos7安装python3","date":"2021-05-27T03:40:56.309Z","updated":"2021-05-27T04:05:20.642Z","comments":true,"path":"2021/05/27/centos7-an-zhuang-python3/","link":"","permalink":"http://godhearing.cn/2021/05/27/centos7-an-zhuang-python3/","excerpt":"","text":"简略步骤直接把所有步骤简略写下，简略的解释，如果要安装的话，直接复制命令即可。 1、安装依赖包 yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel -y 2、下载python3的安装包 wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz 3、解压安装包 tar -zxvf Python-3.7.4.tgz cd Python3.7.4 4、配置路径，编译安装 ./configure --prefix=/usr/local/Python-3.7.4 make &amp;&amp; make install 5、添加软连接 ln -s /usr/local/Python-3.7.4/bin/python3 /usr/bin/python ln -s /usr/local/Python-3.7.4/bin/pip3 /usr/bin/pip \"\"\"如果出现文件已存在，则将其删除，重新再执行\"\"\" rm -rf /usr/bin/python rm -rf /usr/bin/pip \"\"\"不出现不用执行\"\"\" 6、修复yum，将这两个文件的第一行的python改为python2，注意，是python2 vi /usr/libexec/urlgrabber-ext-down vi /usr/bin/yum","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/tags/Linux/"}],"author":"天听"},{"title":"sqlalchemy序列化","slug":"sqlalchemy序列化","date":"2021-05-19T08:04:58.646Z","updated":"2021-05-19T08:09:48.262Z","comments":true,"path":"2021/05/19/sqlalchemy-xu-lie-hua/","link":"","permalink":"http://godhearing.cn/2021/05/19/sqlalchemy-xu-lie-hua/","excerpt":"","text":"前言 sqlalchemy虽然说是python最好用的orm之一，但是其内部的功能还是有不完善的地方，就比如序列化，如果表字段要破百，假设前后端不分离的情况下还好，但是前后端分离的情况下，前端无法处理，而sqlalchemy又不带序列化，所以我们只能自己写一个序列化了 代码from sqlalchemy import inspect from sqlalchemy.orm import ONETOMANY, MANYTOMANY class Serializer: def __init__(self, instance, many=False, include=[], exclude=[], depth=2): self.instance = instance self.many = many self.include = include self.exclude = exclude self.depth = depth @property def data(self): if self.include and self.exclude: raise ValueError('include and exclude can\\'t work together') if self.many: if isinstance(self.instance, list): return self._serializerlist(self.instance, self.depth) pageinfo = { 'items': True, 'pages': self.instance.pages, 'has_prev': self.instance.has_prev, 'page': self.instance.page, 'has_next': self.instance.has_next, 'iter_pages': list(self.instance.iter_pages(left_edge=1, left_current=2, right_current=3, right_edge=1)) } return {'data': self._serializerlist(self.instance.items, self.depth), 'pageinfo': pageinfo} return self._serializer(self.instance, self.depth) def _serializerlist(self, instances, depth): results = [] for instance in instances: result = self._serializer(instance, depth) if result: results.append(result) return results def _serializer(self, instance, depth): result = {} if depth == 0: return result depth -= 1 model_class = self.get_model_class(instance) inp = self.get_inspect(model_class) model_data = self._serializer_model(inp, instance, depth) relation_data = self._serializer_relation(inp, instance, depth) result.update(model_data) result.update(relation_data) return result def _serializer_model(self, inp, instance, depth): result = {} model_columns = self.get_model_columns(inp) for column in model_columns: result[column] = getattr(instance, column) return result def _serializer_relation(self, inp, instance, depth): result = {} relation_columns = self.get_relation_columns(inp) for relation in relation_columns: column = relation.key if relation.direction in [ONETOMANY, MANYTOMANY]: children = getattr(instance, column) if relation.lazy == 'dynamic': children = children.all() result[column] = Serializer( children, many=True, exclude=[relation.back_populates], depth=depth).data else: child = getattr(instance, column) if relation.lazy == 'dynamic': child = child.first() result[column] = Serializer( child, many=False, exclude=[relation.back_populates], depth=depth).data return result def get_model_class(self, instance): return getattr(instance, '__class__') def get_inspect(self, model_class): return inspect(model_class) def get_model_columns(self, inp): if self.include: model_columns = [ column.name for column in inp.columns if column.name in self.include ] elif self.exclude: model_columns = [ column.name for column in inp.columns if column.name not in self.exclude ] else: model_columns = [column.name for column in inp.columns] return model_columns def get_relation_columns(self, inp): if self.include: relation_columns = [ relation for relation in inp.relationships if relation.key in self.include ] elif self.exclude: relation_columns = [ relation for relation in inp.relationships if relation.key not in self.exclude ] else: relation_columns = [relation for relation in inp.relationships] return relation_columns 具体使用使用上很简单(以flask-sqlalchemy为例),原生sqlalchemy类似 多个实例posts = Post.query.all() serializer = Seralizer(posts,many=True) data = serializer.data 单个实例post = Post.query.first() serializer = Seralizer(post,many=False) data = serializer.data 排除字段serializer = Seralizer(post,exclude=['title']) 仅包括字段serializer = Seralizer(post,include=['title']) 关系查询深度serializer = Seralizer(post,depth=3)","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://godhearing.cn/tags/SQLAlchemy/"}],"author":"天听"},{"title":"Docker-componse搭建ES单机集群","slug":"Docker-Compose搭建ES单机集群","date":"2021-05-18T10:13:42.351Z","updated":"2021-05-31T09:16:13.137Z","comments":true,"path":"2021/05/18/docker-compose-da-jian-es-dan-ji-ji-qun/","link":"","permalink":"http://godhearing.cn/2021/05/18/docker-compose-da-jian-es-dan-ji-ji-qun/","excerpt":"","text":"前言 也不想前言了，作为一名非专业运维，各种bug层出不穷，不过好在功夫不负有心人，终于是成功了搭建了ES单机集群 准备工作 首先就是一个巨坑，我在使用docker-compose内写了挂载，他自动创建，然后，就没有然后了，他自动创建的文件夹和文件是没有读写权限的，所以我们手动创建。 # 创建数据/日志目录 这里我们部署2个节点 # 由于服务器辣鸡，所以只部署两个，如果想增多的话，就手动加着建就行了 mkdir /opt/elasticsearch/data/{node0,node1,node2} -p mkdir /opt/elasticsearch/logs/{node0,node1,node2} -p mkdir /opt/elasticsearch/node cd /opt/elasticsearch/node mkdir es1 mkdir es2 cd /opt/elasticsearch # 给予777最大权限 chmod 0777 data/* -R &amp;&amp; chmod 0777 logs/* - 把该创建完成的都创建完成，之后，又是一个坑，系统参数不足导致的内存一直不够用，这里我们直接修改/etc/sysctl.conf echo vm.max_map_count=655360 &gt;&gt; /etc/sysctl.conf # 在root账户执行这一句 作用是重新载入sysctl系统参数 sysctl -p Docker-compose文件 然后进入到/opt/elasticsearch目录下，建立docker-compose.yml文件即docker-compose使用的主模板文件： version: '3' services: es1: image: elasticsearch:7.2.0 container_name: es1 privileged: true environment: - \"ES_JAVA_OPTS=-Xms128m -Xmx128m\" ulimits: memlock: soft: -1 hard: -1 volumes: - ./data/node0:/usr/share/elasticsearch/data - ./node/es1/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./logs/node0:/usr/share/elasticsearch/logs ports: - 9200:9200 networks: - esnet es3: image: elasticsearch:7.2.0 container_name: es2 privileged: true environment: - \"ES_JAVA_OPTS=-Xms128m -Xmx128m\" ulimits: memlock: soft: -1 hard: -1 volumes: - ./data/node2:/usr/share/elasticsearch/data - ./node/es2/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./logs/node2:/usr/share/elasticsearch/logs ports: - 9202:9200 links: - es1 networks: - esnet # 使用桥接的网络连接方式 networks: esnet: driver: bridge 然后cd node/es1/写elasticsearch.yml配置文件 # 集群名称 EsCluster cluster.name: yawp # 节点名称 node.name: es1 # 是否为主节点 node.master: true # 该节点是否存储数据 node.data: true # 对外开放的http端口 http.port: 9200 # 回环地址 network.host: 0.0.0.0 discovery.zen.ping.unicast.hosts: [\"es1\"] bootstrap.memory_lock: true # 是否运行跨域REST请求 http.cors.enabled: true # 允许跨域请求来自何处 http.cors.allow-origin: \"*\" # 有成为主节点资格的最小节点数 discovery.zen.minimum_master_nodes: 1 xpack.security.enabled: false cluster.initial_master_nodes: [\"es1\"] 从节点的配置文件elasticsearch.yml # 集群名称 EsCluster cluster.name: yawp # 节点名称 node.name: es2 # 是否为主节点 node.master: false # 该节点是否存储数据 node.data: true # 对外开放的http端口 http.port: 9200 # 回环地址 network.host: 0.0.0.0 # 有资格成为主节点的资格列表 discovery.zen.ping.unicast.hosts: [\"es1\"] # 是否运行跨域REST请求 http.cors.enabled: true # 允许跨域请求来自何处 http.cors.allow-origin: \"*\" # 有成为主节点资格的最小节点数 discovery.zen.minimum_master_nodes: 1 xpack.security.enabled: false cluster.initial_master_nodes: [\"es1\"] 然后回到/opt/elasticsearch启动集群 docker-compose up -d 启动成功后通过docker-compose ps查看集群情况，如果你的容器在运行，但是输入这个命令却找不到，请不用担心，停掉容器，重新启动一下就好了。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://godhearing.cn/tags/Redis/"}],"author":"天听"},{"title":"docker内ES配置IK分词","slug":"docker内ES配置IK分词","date":"2021-05-17T09:44:19.584Z","updated":"2021-05-17T09:59:31.443Z","comments":true,"path":"2021/05/17/docker-nei-es-pei-zhi-ik-fen-ci/","link":"","permalink":"http://godhearing.cn/2021/05/17/docker-nei-es-pei-zhi-ik-fen-ci/","excerpt":"","text":"前言 使用ES，安装分词器是必不可少的，因为es默认的分词器对中文太不友好，按照每个汉字划分，缺少灵魂，而ik这个分词器，就是一个中文分词器，非常的好用。下面介绍两种安装IK的方式，第一种比较简单，第二种比较繁琐，如果还没有安装ES的话，请移步：这里 第一种方式 进入容器 docker exec -it 容器id /bin/bash 在线下载并安装，需要注意，一定要安装你对应es版本的插件 ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.5.4/elasticsearch-analysis-ik-6.5.4.zip cd到plugins看到analysis-ik，就代表安装成功，此时，我们只需要exit退出容器，然后执行docker restart 容器id重启即可 第二种方式确认ES版本,去github上下载正确版本的IK分词器 https://github.com/medcl/elasticsearch-analysis-ik 然后通过下载页.下载zip包 https://github.com/medcl/elasticsearch-analysis-ik/releases 通过docker命令将其拷贝进容器内 docker cp /tmp/elasticsearch-analysis-ik-7.8.0.zip {容器名}:/usr/share/elasticsearch/plugins # 进入容器 docker exec -it elasticsearch /bin/bash # 创建目录 mkdir /usr/share/elasticsearch/plugins/ik 将文件压缩包移动到ik中 mv /usr/share/elasticsearch/plugins/elasticsearch-analysis-ik-7.8.0.zip /usr/share/elasticsearch/plugins/ik # 进入目录 cd /usr/share/elasticsearch/plugins/ik # 解压 unzip elasticsearch-analysis-ik-7.8.0.zip # 删除压缩包 rm -rf elasticsearch-analysis-ik-7.8.0.zip 重启docker容器即可。 然后我们来测试一下，打开postman，向localhost:9200/_analyze?pretty地址发送一个post请求，注意要改成json格式 然后查看结果，不再是一个字一个字的，就对了，ok，测试完毕","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}],"author":"天听"},{"title":"canal同步ES","slug":"canal同步ES","date":"2021-05-10T06:08:10.122Z","updated":"2021-05-10T06:57:02.532Z","comments":true,"path":"2021/05/10/canal-tong-bu-es/","link":"","permalink":"http://godhearing.cn/2021/05/10/canal-tong-bu-es/","excerpt":"","text":"前言canal 是阿里巴巴开源的一个项目，主要用途是基于 MySQL 数据库 binlog 日志解析，提供增量数据订阅和消费。 基于日志增量订阅和消费的业务包括： 数据库镜像 数据库实时备份 索引构建和实时维护（拆分异构索引、倒排索引等） 业务 cache 刷新 带业务逻辑的增量数据处理 MySQL 配置修改 MySQL 配置文件/etc/mysql/my.cnf，开启 binlog 写入功能，并配置模式为 ROW。 log-bin=mysql-bin # 开启 binlogbinlog-format=ROW # 选择 ROW 模式 server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复 重启数据库，查看配置是否生效。命令是：sudo service mysql restart mysql> show variables like 'binlog_format'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set (0.19 sec) mysql> mysql> show variables like 'log_bin'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | ON | +---------------+-------+ 1 row in set (0.00 sec) mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 4230 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 然后创建用户，并授权。 mysql> CREATE USER canal IDENTIFIED BY 'canal'; mysql> GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%'; mysql> FLUSH PRIVILEGES; mysql> show grants for 'canal'@'%'; +----------------------------------------------------------------------------+ | Grants for canal@%% | +----------------------------------------------------------------------------+ | GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO `canal`@`%` | +----------------------------------------------------------------------------+ 1 row in set (0.00 sec) canal服务端 canal的原理呢，就是canal服务端来监听mysql的binlog，然后canal的客户端来连接并解析日志，达到自己想要的效果，比如增量数据到ES，我们用docker来启动服务端 第一步，拉取镜像 docker pull canal/canal-server:v1.1.4 第二步，写一个docker-compose，启动起来比较方便，或者直接使用它提供的脚本也可以。 version: '3' services: canal-server: image: canal/canal-server:v1.1.4 container_name: canal-server restart: unless-stopped network_mode: host ports: - 11111:11111 environment: - canal.auto.scan=false - canal.instance.master.address=127.0.0.1:3306 - canal.instance.dbUsername=canal - canal.instance.dbPassword=canal - canal.instance.filter.regex=.*\\\\..* - canal.destinations=test - canal.instance.connectionCharset=UTF-8 - canal.instance.tsdb.enable=true volumes: - /root/canal/test/log/:/home/admin/canal-server/logs/ 如果碰到host network_mode is incompatible with port_bindings这个错误，那我们直接用它默认的网络即可，将network_mode删掉就好。 官方提供的shell脚本 # sh run.sh -e canal.auto.scan=false -e canal.destinations=test -e canal.instance.master.address=127.0.0.1:3306 -e canal.instance.dbUsername=canal -e canal.instance.dbPassword=canal -e canal.instance.connectionCharset=UTF-8 -e canal.instance.tsdb.enable=true -e canal.instance.gtidon=false 启动服务： # docker-compose up Recreating canal-server ... done Attaching to canal-server canal-server | DOCKER_DEPLOY_TYPE=VM canal-server | ==&gt; INIT /alidata/init/02init-sshd.sh canal-server | ==&gt; EXIT CODE: 0 canal-server | ==&gt; INIT /alidata/init/fix-hosts.py canal-server | ==&gt; EXIT CODE: 0 canal-server | ==&gt; INIT DEFAULT canal-server | Generating SSH1 RSA host key: [ OK ] canal-server | Starting sshd: [ OK ] canal-server | Starting crond: [ OK ] canal-server | ==&gt; INIT DONE canal-server | ==&gt; RUN /home/admin/app.sh canal-server | ==&gt; START ... canal-server | start canal ... canal-server | start canal successful canal-server | ==&gt; START SUCCESSFUL ... 看见START SUCCESSFUL即启动成功。 canal客户端 直接copy官方的客户端代码即可，地址，我就不搬砖了 connected to 127.0.0.1:11111 Auth succed Subscribe succed 看到这个，即为成功 我们来验证一下，随便建个库，建个表，然后插入几条数据。 打印出的，db就是我们的库，table就是表，event_type字段1表示新增，2表示更新，3表示删除，data里就是我们的数据，可以根据自己的需要来灵活的变动。 补充canal服务端启动之后，在docker-compose.yml里设置的volumes位置，会生成两个日志文件，分别是meta.log和test.log，可以查看服务是不是正常，有没有报错信息。 后续如果要更改，更好的做法是将消息发送到消息队列，然后再从消息队列消费。在1.1.5版本中，官方支持了RabbitMQ，后续有时间再继续更新","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}],"author":"天听"},{"title":"微信小程序支付后台调用","slug":"微信小程序支付后台调用","date":"2021-04-23T08:17:39.002Z","updated":"2021-04-23T08:34:01.754Z","comments":true,"path":"2021/04/23/wei-xin-xiao-cheng-xu-zhi-fu-hou-tai-diao-yong/","link":"","permalink":"http://godhearing.cn/2021/04/23/wei-xin-xiao-cheng-xu-zhi-fu-hou-tai-diao-yong/","excerpt":"","text":"支付交互流程 这里呢，就不多说了，先上图 后台调用，只需要调用统一下单API，就可以了。 获取openid 这是第一步，openid是微信用户在你的小程序中的唯一标识，只需调用vx.login即可。 import requests from config import APPID, SECRET class OpenidUtils(object): def __init__(self, jscode): self.url = \"https://api.weixin.qq.com/sns/jscode2session\" self.appid = APPID # 小程序id self.secret = SECRET # 不要跟后面支付的key搞混 self.jscode = jscode # 前端传回的动态jscode def get_openid(self): # url一定要拼接，不可用传参方式 url = self.url + \"?appid=\" + self.appid + \"&amp;secret=\" + self.secret + \"&amp;js_code=\" + self.jscode + \"&amp;grant_type=authorization_code\" r = requests.get(url) print(r.json()) openid = r.json()['openid'] return openid 支付请求import requests import json import hashlib import time import random import string import xmltodict class WX_PayToolUtil(): \"\"\" 微信支付工具 \"\"\" def __init__(self, APP_ID, MCH_ID, API_KEY, NOTIFY_URL): self._APP_ID = APP_ID # 小程序ID self._MCH_ID = MCH_ID # 商户号 self._API_KEY = API_KEY self._UFDODER_URL = \"https://api.mch.weixin.qq.com/pay/unifiedorder\" # 接口链接 self._NOTIFY_URL = NOTIFY_URL # 异步通知 def generate_sign(self, param): '''生成签名''' stringA = '' ks = sorted(param.keys()) # 参数排序 for k in ks: stringA += (k + '=' + param[k] + '&amp;') # 拼接商户KEY stringSignTemp = stringA + \"key=\" + self._API_KEY # md5加密,也可以用其他方式 hash_md5 = hashlib.md5(stringSignTemp.encode('utf8')) sign = hash_md5.hexdigest().upper() return sign def getPayUrl(self, orderid, openid, goodsPrice, **kwargs): \"\"\"向微信支付端发出请求，获取url\"\"\" # key = self._API_KEY nonce_str = ''.join(random.sample(string.ascii_letters + string.digits, 30)) # 生成随机字符串，小于32位 params = { 'appid': self._APP_ID, 'mch_id': self._MCH_ID, 'nonce_str': nonce_str, \"body\": '艺术品订单', 'out_trade_no': orderid, 'total_fee': str(goodsPrice), 'spbill_create_ip': \"127.0.0.1\", 'notify_url': self._NOTIFY_URL, 'trade_type': \"JSAPI\", \"openid\": openid, } # 生成签名 params['sign'] = self.generate_sign(params) param = {'root': params} xml = xmltodict.unparse(param) response = requests.post(self._UFDODER_URL, data=xml.encode('utf-8'), headers={'Content-Type': 'text/xml'}) # xml 2 dict msg = response.text xmlmsg = xmltodict.parse(msg) # 4. 获取prepay_id if xmlmsg['xml']['return_code'] == 'SUCCESS': if xmlmsg['xml']['result_code'] == 'SUCCESS': prepay_id = xmlmsg['xml']['prepay_id'] # 时间戳 timeStamp = str(int(time.time())) # 5. 五个参数 data = { \"appId\": self._APP_ID, \"nonceStr\": nonce_str, \"package\": \"prepay_id=\" + prepay_id, \"signType\": 'MD5', \"timeStamp\": timeStamp, } # 6. paySign签名 paySign = self.generate_sign(data) data[\"paySign\"] = paySign # 加入签名 # 7. 传给前端的签名后的参数 return data 当然你可能会遇到的错误有签名错误，一般的情况是你的appSecret和商户号的API密钥两个弄错了，当然如果不是还有可能是其他问题，解决方案链接https://www.cnblogs.com/wanghuijie/p/wxpay_sign_error.html。 很可能遇到的错误比如返回invalid total_fee，这是因为，微信支付和支付宝不同，他的计量单位是分，所以不能出现小数点。所以我们系统如果是以元为单位要处理下金额，即先乘以100，再去小数点","categories":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"author":"天听"},{"title":"CORS跨域问题","slug":"CORS跨域问题","date":"2021-04-18T00:19:19.529Z","updated":"2021-04-18T00:23:56.426Z","comments":true,"path":"2021/04/18/cors-kua-yu-wen-ti/","link":"","permalink":"http://godhearing.cn/2021/04/18/cors-kua-yu-wen-ti/","excerpt":"","text":"问题The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' when the request's credentials mode is 'include'. The credentials mode of requests initiated by t... 原因前端配置了withCredentials=true 解决办法后端配置Access-Control-Allow-Origin不能为*, 必须是相应地址 后端需配置Access-Control-Allow-Credentials 后端需要配置Access-Control-Allow-Headers为对应的请求头集合","categories":[{"name":"cors","slug":"cors","permalink":"http://godhearing.cn/categories/cors/"}],"tags":[{"name":"cors","slug":"cors","permalink":"http://godhearing.cn/tags/cors/"}],"author":"天听"},{"title":"apscheduler设置时区","slug":"apscheduler设置时区","date":"2021-04-18T00:02:30.892Z","updated":"2021-04-18T00:23:56.426Z","comments":true,"path":"2021/04/18/apscheduler-she-zhi-shi-qu/","link":"","permalink":"http://godhearing.cn/2021/04/18/apscheduler-she-zhi-shi-qu/","excerpt":"","text":"前言 在最近的使用中，感觉到这个apscheduler非常的好用，但是吧，一切美好的想象就停留在了部署的那天…. 我本以为一切如我所愿，按照我预期的运行，于是我在开启了一个定时任务后，满怀期待的等待着，只要他执行，我就可以下班走人了，但是，时间到了之后，他还是没有执行，于是我就意识到，事情开始往我意想不到的方向发展了。 我在查看了redis之后，发现，他的任务还在，并没有报错，只是时间是在八小时之后，哎，我这才意识到是时区的问题。那，废话不多说，直接开始上代码。 解决方案 直接在添加任务处指定时区，如下： # 这样的话，就是utc时间了。 schedudler.add_job(worker,'cron',day=\"*/1\", timezone=pytz.utc) # 改为下面这个时区，就是亚洲上海，与我们时间一致，也就能最好的解决这个问题 timezone=pytz.timezone('Asia/Shanghai') 还有一种方法，就是更改你的系统和docker的时区，因为apscheduler的时间是根据你本机的时区来进行的。 1: 以ubuntu为例，更改时区 date -R或者timedatectl status都能查看时区，然后修改时区timedatectl set-timezone \"Asia/Shanghai\" 2: 修改docker的时区 这个呢，可以在启动容器时将本地的时区文件挂载映射，看起来像是这样： docker run -v /etc/timezone:/etc/timezone -v /etc/localtime:/etc/localtime -it ubuntu bash 或者直接在dockerfile中添加上RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo 'Asia/Shanghai' &gt;/etc/timezone 问题完美解决","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"定时任务","slug":"定时任务","permalink":"http://godhearing.cn/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"}],"author":"天听"},{"title":"brew安装nginx","slug":"brew安装nginx","date":"2021-04-09T04:34:32.626Z","updated":"2021-04-09T04:42:06.871Z","comments":true,"path":"2021/04/09/brew-an-zhuang-nginx/","link":"","permalink":"http://godhearing.cn/2021/04/09/brew-an-zhuang-nginx/","excerpt":"","text":"1.安装 brew install nginx 或 sudo brew install nginx 2.启动 brew services start nginx 或者 sudo brew services start nginx 3.重启 brew services restart nginx 或者 sudo brew services restart nginx 4.停止 brew services stop nginx 或者 sudo brew services stop nginx 5.查看 cat usr/local/etc/nginx/nginx.conf 6.编辑 vi usr/local/etc/nginx/nginx.conf","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/tags/Nginx/"}],"author":"天听"},{"title":"pytest进阶","slug":"pytest进阶","date":"2021-04-09T02:42:59.444Z","updated":"2021-04-09T04:08:03.008Z","comments":true,"path":"2021/04/09/pytest-jin-jie/","link":"","permalink":"http://godhearing.cn/2021/04/09/pytest-jin-jie/","excerpt":"","text":"分组执行pytest的分组执行依旧可以用mark进行标记，标记名可以自定义的取，比如冒烟用例可以用smoke。 看起来像是这样： class TestLogin: @pytest.mark.smoke def test_01(self): print('hello salmon') # 同时使用两个装饰器，也是可以的。 @pytest.mark.run(order=2) @pytest.mark.smoke def test_03(): print('bing') 然后，我们在配置文件里添加上markers，这是分组，整体的pytest.ini可能会是这样的 [pytest] addopts = -vs testpaths = ./test_pytest python_files = test*.py python_classes = Test* python_functions = test Markers = smoke:冒烟用例 www:名字可以随意取 执行：我们只需要在执行时加一个-m参数即可，-m后面跟的是你要执行哪些组，比如你要执行smoke pytest -m \"smoke\" 或者你想同时执行smoke和www pytest -m \"smoke or www\" 亦或者再执行同时标记有smoke和www的 class TestLogin: @pytest.mark.smoke @pytest.mark.www def test_01(self): print('hello salmon') pytest -m \"smoke and www\" 跳过用例无条件跳过同样的，我们使用mark进行标记 @pytest.mark.skip() # 还可以说明一下，为什么跳过 @pytest.mark.skip(reason='asdasdasd') 有条件跳过依旧是使用mark进行标记，只要满足了某个判断，就可以跳过 a = 1 @pytest.mark.skipif(a > 10, reason='因为大于10，所以跳过') 生成html报告前提是，安装了pytest-html 在pytest.ini文件下，addopts之后添加--html 路径 [pytest] addopts = -vs --html ./report/report.html testpaths = ./test_pytest python_files = test*.py python_classes = Test* python_functions = test Markers = smoke:冒烟用例","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"测试","slug":"测试","permalink":"http://godhearing.cn/tags/%E6%B5%8B%E8%AF%95/"}],"author":"天听"},{"title":"pytest基础使用","slug":"pytest","date":"2021-04-08T09:29:25.716Z","updated":"2021-04-08T09:29:25.716Z","comments":true,"path":"2021/04/08/pytest/","link":"","permalink":"http://godhearing.cn/2021/04/08/pytest/","excerpt":"","text":"pytest单元测试框架 什么是单元测试框架 单元测试是指在软件开发当中，针对软件的最小单位(函数，方法)进行正确性的检查测试 单元测试框架主要做什么 测试发现：从多个文件里面去找到我们测试用例 测试执行：按照一定的顺序和规则去执行，并生成结果 测试判断：通过断言判断预期结果和实际结果的差异 测试报告：统计测试进度，耗时，通过率，生成测试报告 单元测试框架和自动化测试框架的关系自动化测试框架： 提高测试效率，降低维护成本 减少人工干预，提高测试的准确性，增加代码的重用性 核心思想是让不懂代码的人也能够通过这个框架去实现自动化测试 单元测试框架只是自动化测试框架的组成部分之一 需要安装的插件pytest pytest-html # 生成html格式的自动化测试报告 pytest-xdist # 测试用例分布式执行，多CPU分发 pytest-ordering # 用于改变测试用例多执行顺序 pytest-rerunfailures # 用例失败后重跑 allure-pytest # 用于生成美观的测试报告 默认的测试用例规则 模块名必须以test_开头或者以_test结尾 测试类必须以Test开头，并且不能有init方法 测试方法必须以test开头 基础应用运行方式 主函数模式 通过程序入口:if __name__ == '__main__':下来执行 (1) 运行所有：pytest.main()(2) 指定模块运行：pytest.main(['文件名'])，如果不指定文件名，它会自动(3) 寻找当前文件夹下所有符合条件的文件并执行。 指定文件夹执行：pytest.main(['路径/文件夹名']) (4) 通过nodeid去执行： ​ 解释一下nodeid，nodeid由模块名，分隔符，类名，方法名，函数名组成， 通过::来分割类、方法、函数 pytest.main(['-vs', './文件夹名/文件名::类名::方法名'])，如果是函数测试的话，那直接文件名后用::分割，写上函数即可。 命令行模式 在当前目录下，执行pytest或者python -m pytest 至于，为什么可能会执行python -m pytest呢，是因为有可能你执行pytest会找不到，这是因为通过pip安装pytest不会使其成为系统命令，而是会将其安装到python。-m命令将pytest作为其自己的命令运行，然后任何后续脚本都将作为参数。 通过读取pytest.ini配置文件运行 参数详解-s: 表示输出调试信息，包括print打印出的信息，如果使用主函数模式，他的传参方式是一个数组，看起来像是这样 pytest.main(['-s']) 如果是命令行模式，就直接pytest -s即可 -v：输出用例的类名和方法名，可以和-s一起用 -n：支持多线程或者分布式运行测试用例，前提是安装了pytest-xdist，用法，在命令行-n 2代表启用两个线程，这个线程数可以根据需要来自己定义，在入口函数中使用的话，就直接-n=线程数即可 --reruns：代表失败后重跑，前提是安装了pytest-rerunfailures，用法，在命令行--reruns 2即可，后面的数字代表，如果失败了再跑几次，如果超过这个数它还是报错，那就说明这个东西它已经没救了。可以放弃治疗，在入口中，使用方法和-n一致。 -x：只要有一个用例错误，测试停止 --maxfail 2：失败两个用例，才会停止 -k 'XXX'：根据测试用例的部分字符串指定测试用例，比如你的函数名带QQ的，那么，-k 'QQ'，只有带QQ的才会被执行，不带的则会直接跳过 执行顺序 pytest默认是从上到下执行，这个和unittest的执行方式不同，unittest的顺序是由ascll码来决定顺序 标记更改执行顺序标记更改，手动的去标记某些方法或者函数的执行顺序，这个是需要使用到装饰器，@pytest.mark.run(order=1)的 order，代表的是第几个执行，例如：order=1，则是第一个执行 前提是，你安装了pytest-ordering 配置文件执行 无论是命令行模式还是入口函数模式，真正使用起来并不多，而最多的是使用配置文件，就像我们使用docker，是可以一个个的使用dockerfile来写，但是没人会这么做吧，还是一个docker-componse解决。pytest的配置文件名，是pytest.ini，一般来说，放在文件的根目录，这个没有什么可说的，但是需要 注意 的是，他的编码，必须是ANSI，可以使用notpad++来更改，或者更直接点，我们通过python代码来手动更改 import os import codecs # oldfile:UTF8文件的路径 # newfile:要保存的ANSI文件的路径 def convertUTF8ToANSI(oldfile, newfile): #打开UTF8文本文件 f = codecs.open(oldfile, 'r', 'utf8') utfstr = f.read() f.close() # 把UTF8字符串转码成ANSI字符串 outansestr = utfstr.encode('mbcs') # 使用二进制格式保存转码后的文本 f = open(newfile, 'wb') f.write(outansestr) f.close() # 头部格式，不可更改 [pytest] # 命令行参数，用空格分割 addopts = -vs # 测试用例文件夹路径，可自己配置 testpaths = ./test_profile # 配置测试搜索的模块文件名称，我们之前说过，文件名、类名、方法名都必须是固定格式，但是，这里我们变了，我们可以通过这里来设置自己定义的 python_files = test*.py # 配置测试搜索的测试类名 python_classes = Test* # 配置测试搜索的测试方法名 python_functions = test","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"测试","slug":"测试","permalink":"http://godhearing.cn/tags/%E6%B5%8B%E8%AF%95/"}],"author":"天听"},{"title":"apscheduler定时任务","slug":"apscheduler定时任务","date":"2021-04-06T07:51:55.993Z","updated":"2021-04-18T00:07:02.557Z","comments":true,"path":"2021/04/06/apscheduler-ding-shi-ren-wu/","link":"","permalink":"http://godhearing.cn/2021/04/06/apscheduler-ding-shi-ren-wu/","excerpt":"","text":"前言 在web开发中，定时任务在很多场景中是经常用到的，比较常见的定时任务就是linux自带的crontab和celery还有今天要说的这个apscheduler了。 APScheduler基于Quartz的一个Python定时任务框架，实现了Quartz的所有功能，使用起来十分方便。提供了基于日期、固定时间间隔以及crontab类型的任务，并且可以持久化任务。基于这些功能，我们可以很方便的实现一个python定时任务系统。 安装直接使用pip安装即可 pip install apscheduler 简介 APScheduler由四部分组成，分别是：触发器，作业存储，执行器，调度器。 触发器(trigger) 包含调度逻辑，每一个作业有它自己的触发器，用于决定接下来哪一个作业会运行。除了他们自己初始配置意外，触发器完全是无状态的。 作业存储(job store) 存储被调度的作业，默认的作业存储是简单地把作业保存在内存中，其他的作业存储是将作业保存在数据库中。一个作业的数据讲在保存在持久化作业存储时被序列化，并在加载时被反序列化。调度器不能分享同一个作业存储。 执行器(executor) 处理作业的运行，他们通常通过在作业中提交制定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 调度器(scheduler) 是其他的组成部分。你通常在应用只有一个调度器，应用的开发者通常不会直接处理作业存储、调度器和触发器，相反，调度器提供了处理这些的合适的接口。配置作业存储和执行器可以在调度器中完成，例如添加、修改和移除作业。 简单应用添加任务： import time from apscheduler.schedulers.blocking import BlockingScheduler def my_job(): print time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) sched = BlockingScheduler() sched.add_job(my_job, 'interval', seconds=5) sched.start() 上面的例子表示每隔5s执行一次my_job函数，输出当前时间信息 上面是通过add_job()来添加任务，另外还有一种方式是通过scheduled_job()修饰器来修饰函数 import time from apscheduler.schedulers.blocking import BlockingScheduler sched = BlockingScheduler() @sched.scheduled_job('interval', seconds=5) def my_job(): print time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) sched.start() 移除任务： job = scheduler.add_job(myfunc, 'interval', minutes=2) job.remove() #如果有多个任务序列的话可以给每个任务设置ID号，可以根据ID号选择清除对象，且remove放到start前才有效 sched.add_job(myfunc, 'interval', minutes=2, id='4') sched.remove_job('my_job_id') 暂停任务： apsched.job.Job.pause() apsched.schedulers.base.BaseScheduler.pause_job() 恢复任务： apsched.job.Job.resume() apsched.schedulers.base.BaseScheduler.resume_job() 查询任务： # 根据任务设置的id查看某个定时任务详情 print(sched.get_job(job_id='4')) # 查看实例下所有定时任务 print(sched.get_jobs()) 关闭调度器： 默认情况下调度器会等待所有正在运行的作业完成后，关闭所有的调度器和作业存储。 如果你不想等待，可以将wait选项设置为False。 sched.shutdown() sched.shutdown(wait=False) 参数解析add_job的第二个参数是trigger，它管理着作业的调度方式。它可以为date, interval或者cron。对于不同的trigger，对应的参数也相同。 date: 定时调度（只会执行一次） # 表示2009年11月6号执行一次 sched.add_job(my_job, 'date', run_date=date(2009, 11, 6), args=['text']) interval: 间隔调度（每隔多久执行） #表示每隔3天17时19分07秒执行一次任务 sched.add_job(my_job, 'interval',days = 03,hours = 17,minutes = 19,seconds = 07) cron：定时调度（某一定时时刻执行） # 例子 #表示2017年3月22日17时19分07秒执行该程序 sched.add_job(my_job, 'cron', year=2017,month = 03,day = 22,hour = 17,minute = 19,second = 07) #表示任务在6,7,8,11,12月份的第三个星期五的00:00,01:00,02:00,03:00 执行该程序 sched.add_job(my_job, 'cron', month='6-8,11-12', day='3rd fri', hour='0-3') #表示从星期一到星期五5:30（AM）直到2014-05-30 00:00:00 sched.add_job(my_job(), 'cron', day_of_week='mon-fri', hour=5, minute=30,end_date='2014-05-30') #表示每5秒执行该程序一次，相当于interval 间隔调度中seconds = 5 sched.add_job(my_job, 'cron',second = '*/5') 需要注意的问题 在add_job中，如果调用的是一个有形参的函数，不能够直接在该函数后实例传参，而是应该在add_job里的args或者kwargs里传，所以，假如你有个函数，需要传参db和q 如果这样写： ched.add_job(my_job(db,'q'), 'date', run_date='2021-04-06 14:58:40') 你会看到一个这样的错误： ValueError: The following arguments have not been supplied: db, q 正确写法是： schedule.add_job(hello, 'date', args=[db, 'q'], run_date='2021-04-06 14:59:00') 至于集成到web框架中，尽情期待，有时间了会补上，最后，官方文档，各位有兴趣可以看一看 补充一则很难搞的错误 我在使用apscheduler定时任务的时候，将他挂载到了fastapi上，使用起来倒是一点毛病没有，但是，我将他储存的时候，他却给我报了一个这样的错误。 _pickle.PicklingError: Can't pickle &lt;class 'sqlalchemy.orm.session.Session'&gt;: it's not the same object as sqlalchemy.orm.session.Session 我迟迟没有头绪，而且根据源码，我也确定，我并没有写错什么，但是这个错误就是会出现。 在我经过了不懈的寻找之后发现….是我自己写的调度函数问题，我在函数中传了一个sqlalchemy.orm.session，然而，它的储存也正好如此，所以，在传参中，他们重复了。 这个问题，应该很难遇到，因为毕竟没人跟我一样这么搞嘛，不过呢，既然遇到了我就要记录一下，以免再卡这么久。 只要我们不传session对象就好了。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"定时任务","slug":"定时任务","permalink":"http://godhearing.cn/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"}],"author":"天听"},{"title":"nginx负载均衡配置","slug":"nginx负载均衡配置","date":"2021-03-30T07:59:22.378Z","updated":"2021-03-30T08:15:30.652Z","comments":true,"path":"2021/03/30/nginx-fu-zai-jun-heng-pei-zhi/","link":"","permalink":"http://godhearing.cn/2021/03/30/nginx-fu-zai-jun-heng-pei-zhi/","excerpt":"","text":"前言 nginx的作用呢，相信大家都是特别熟悉的了，这里也就不多说了，但是以防还有处于懵懂状态的同学，还是简单介绍一下负载均衡。 我们平时在一台服务器搭建起应用后，万一承受量超过了他的极限，这样的话，服务就会垮掉，所以，我们为了不让这样的事情发生，通常情况下是使用nginx的一个负载均衡的功能，他能够将发来的请求，均匀的，或者是特定的去分配给多台服务器，来提高承受并发的能力。 配置在/etc/nginx下，(这是默认目录，如果你的nginx不在这里，那去对应的地方即可) 打开nginx.conf，在http模块下任意位置写上如下配置，这个服务器集群名可以随意起 upstream 服务器集群名{ server 127.0.0.1:8000; server 39.98.35.129:8000; } 然后我们去server配置，然后注释掉我们本来的地址proxy_pass，改为变量绑定，要注意的是，nginx是一个很神奇的东西，所以，多一个斜杠少一个斜杠都会定成败，一定要注意。 location /api/ { proxy_set_header X-Real-IP $remote_addr; # proxy_pass http://127.0.0.1:8000/; proxy_pass http://服务器集群名/; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } nginx负载均衡的默认策略是轮询，也就是轮着来，如果你的服务器配置是差不多的情况下，是不用改的。还有一种常见的策略，就是权重，在配置服务器集群的那里，来给每个节点配置不同的权重，他们就会接到对应的几率来获得访问，像这样： upstream 服务器集群名 { server 127.0.0.1:8000 weight=3; server 39.98.35.129:8000 weight=7; } ip_hash（ IP绑定）上述方式存在一个问题就是说，在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。 我们可以采用ip_hash指令解决这个问题，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到该服务器。 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 upstream backserver { ip_hash; server 127.0.0.1:8000; server 39.98.35.129:8000; }","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/tags/Nginx/"}],"author":"天听"},{"title":"ubuntu下pip安装问题","slug":"ubuntu下pip安装问题","date":"2021-03-30T05:53:11.213Z","updated":"2021-03-30T06:00:57.247Z","comments":true,"path":"2021/03/30/ubuntu-xia-pip-an-zhuang-wen-ti/","link":"","permalink":"http://godhearing.cn/2021/03/30/ubuntu-xia-pip-an-zhuang-wen-ti/","excerpt":"","text":"问题Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-buil 解决办法 解决方法是更新 setuptools 和 pip： pip install --upgrade setuptools python -m pip install --upgrade pip","categories":[{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/tags/Linux/"}],"author":"天听"},{"title":"Docker打包vue多处/static/css路径解决办法","slug":"Docker打包vue多出staticcss路径解决办法","date":"2021-03-25T10:44:01.132Z","updated":"2021-03-25T10:48:20.566Z","comments":true,"path":"2021/03/25/docker-da-bao-vue-duo-chu-staticcss-lu-jing-jie-jue-ban-fa/","link":"","permalink":"http://godhearing.cn/2021/03/25/docker-da-bao-vue-duo-chu-staticcss-lu-jing-jie-jue-ban-fa/","excerpt":"","text":"解决办法在build/utils.js文件中添加publicPath:’…/…/’,重新打包即可。 if (options.extract) { return ExtractTextPlugin.extract({ use: loaders, fallback: \"vue-style-loader\", publicPath: \"../../\" }); } else { return [\"vue-style-loader\"].concat(loaders); }","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"mac下安装go语言开发环境","slug":"mac下安装go语言开发环境","date":"2021-03-24T02:33:24.709Z","updated":"2021-03-24T04:20:11.923Z","comments":true,"path":"2021/03/24/mac-xia-an-zhuang-go-yu-yan-kai-fa-huan-jing/","link":"","permalink":"http://godhearing.cn/2021/03/24/mac-xia-an-zhuang-go-yu-yan-kai-fa-huan-jing/","excerpt":"","text":"前言 在mac下，homebrew这个包管理工具是每个开发环境必不可少的，然后就是go语言编译器，这是必不可少的，最后就是编辑器，可以用Goland，也可以简单一点的用vscode和sublime 安装homebrewHomebrew有点类似于Linux操作系统中的apt-get（Ubuntu）、yum（yum），Mac的操作系统中使用它解决包依赖问题，套用官方的话来说： Homebrew 能干什么?使用 Homebrew 安装 Apple 没有预装但 你需要的东西。 输入命令，安装: fabric:~ fabric$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 出现这样内容，就是安装成功了 ==&gt; Installation successful! ==&gt; Homebrew has enabled anonymous aggregate user behaviour analytics. Read the analytics documentation (and how to opt-out) here: https://docs.brew.sh/Analytics.html ==&gt; Next steps: - Run `brew help` to get started - Further documentation: https://docs.brew.sh 安装go 我们可以选择使用brew安装go，也可以直接去官网，选择需要的版本来进行安装。 官网安装比较简单，就是无脑下一步就可以了，安装包会默认安装在/usr/local目录下。 配置环境变量第一种方法首先需要查看你的go目录，默认是/usr/local，然后进入这个目录，再cd到go中，查看是否在此，确认之后，就可以直接在~/.bash_profile中，添加以下： export PATH=$PATH:/usr/local/go/bin 然后重启source .bash_profile 第二种方法理论上，你现在使用终端输入go应该是不好使的，因为没有配置环境变量，如果你之前有bash_profile文件，那就打开就好了，执行cd ~和.bash_profile添加上这两行： export GOROOT=/usr/local/go export PATH=$GOROOT/bin 然后保存，退出，运行此命令，使环境变量生效 source ~/.bash_profile 小插曲 使用第二种方法的人，可能有的人关闭终端后，会突然性的发现，什么命令都用不了了，emmmm…，不过不用担心，有办法，首先终端输入: export PATH=\"/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/usr/X11/bin\" 然后暂时性的命令可以用了，然后打开vim ~/.bash_profile，在最后一行，把上面的命令添加上去，然后保存，执行source ~/.bash_profile ， 之后就可以了。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"}],"author":"天听"},{"title":"sqlalchemy的事务","slug":"sqlalchemy的事务","date":"2021-03-23T07:46:01.103Z","updated":"2021-03-23T10:36:06.804Z","comments":true,"path":"2021/03/23/sqlalchemy-de-shi-wu/","link":"","permalink":"http://godhearing.cn/2021/03/23/sqlalchemy-de-shi-wu/","excerpt":"","text":"前言 说起事务，后端工程师都不会陌生，这是存在于关系型数据库中的一个特性，它具有四个特性： 原子性事务包含的所有操作要么都执行成功，要么都失败，不可分割 一致性中间过程不管怎么执行，结果一定是一致的 隔离性事务在执行过程中，不受其他事务的影响 持久性执行成功之后，结果时永久修改的，不能撤回 MySQL执行的SQL是autocommit的，SALAlchemy 查询语句也是 autocommit的，就是说如果没有明确声明事务的begin，每个单独的SQL都是一个独立的事务。但是在做交易系统时，比如银行给用户A转账给用户B时，有两个操作，从A里面减100，然后给B加100。这两个操作必须放在一个事务里面才行，否是就会出现钱扣了，对方又没到账的情况。 所以，事务，是sqlalchemy乃至所有orm不可或缺的一环 开启事务 开启事务有两种方法，这里只说最简单的一种方法，就是利用with，为了更加真实的模拟业务场景，我们创建两张表和sqlalchemy的连接 from sqlalchemy import create_engine, Column, Integer, String from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker connect = create_engine('mysql+pymysql://root:root@localhost:3306/test01') ModelBase = declarative_base() class User(ModelBase): # 定义表名 __tablename__ = \"user\" # 声明字段 id = Column(Integer, primary_key=True) name = Column(String(length=255)) class Role(ModelBase): __tablename__ = 'role' id = Column(Integer, primary_key=True) role = Column(String(length=255)) DBSession = sessionmaker(bind=connect,autoflush=False, autocommit=False, expire_on_commit=True) 然后我们利用一个fastapi框架来模拟一下 from fastapi import FastAPI, HTTPException, status app = FastAPI() @app.get('/') async def create_sqlalchemy(): # 使用上下文管理器开启事务 with DBSession.begin() as session: data = { \"id\": 0, 'name': 'Salmon' } # 添加user数据 new_user = User(**data) session.add(new_user) data2 = { 'id': 0, # 'id':'添加错误数据' 'role':'SuperUser1' } # 添加role数据 new_role = Role(**data2) session.add(new_role) # 模拟错误 if data2['role'] == 'SuperUser1': raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST) return 'ok' 需要知道的是，在事务开启后，使用with不需要自己来执行commit操作,执行完sql会自动提交，如果报异常会自动rollback 。 我们这里，主动抛出异常，两个表中的数据都没有改变。然后我们发送一个错误数据，不主动抛出，再来试试，解开那个id注释。然后把if和raise这两行删掉，然后，引发他原生错误。数据也不会发生改变。","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://godhearing.cn/tags/SQLAlchemy/"}],"author":"天听"},{"title":"mac下破解pycharm","slug":"mac下破解pycharm","date":"2021-03-18T03:48:26.242Z","updated":"2021-03-18T04:11:20.075Z","comments":true,"path":"2021/03/18/mac-xia-po-jie-pycharm/","link":"","permalink":"http://godhearing.cn/2021/03/18/mac-xia-po-jie-pycharm/","excerpt":"","text":"前言 提起写python最好的编辑器，你能想起来什么，vscode？sublime？No，我认为最好的还是pycharm，虽然他过于重量级，但是，论代码提示和python的功能，最好用的python编辑器还是非pycharm莫属，只是一个缺点，这个货，他要钱。。 对于有资金的大佬，还是建议直接购买官方正品。但是像我这种穷人，只能够忍着眼泪(😂)，开始破解了。 下载插件 下载pycharm，可以直接下载官方正版的，我这边使用的是2020.3版本，下载完之后，打开，试用，这都不说了。 关注公众号：程序员软件库，回复758344，获取下载链接和提取码。 ps：这不是我的广告啊，只是我懒得把它放到自己的网盘了，不过多一步的事情，不必在意这么多啦。😁 然后下载出来之后，将 ide-eval-resetter.zip 插件拖动至 PyCharm 主界面中，对，你没看错，直接拖。 提示重启PyCharm ，如下图 ps：有的不提示重启，可直接跳过此步骤。插件直接在界面底部显示 重启完成并进入PyCharm主界面，点击菜单【Help】-&gt;【Eval Reset】 然后钩上这个框之后，重启pycharm 然后点击【Help】-&gt;【Register】，查看一下试用期，续命成功！ 结语 那什么，有力量的还是支持正版哈。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"pycharm","slug":"pycharm","permalink":"http://godhearing.cn/tags/pycharm/"}],"author":"天听"},{"title":"协同过滤算法","slug":"协同过滤算法","date":"2021-03-16T08:16:41.948Z","updated":"2021-03-16T12:31:20.943Z","comments":true,"path":"2021/03/16/xie-tong-guo-lu-suan-fa/","link":"","permalink":"http://godhearing.cn/2021/03/16/xie-tong-guo-lu-suan-fa/","excerpt":"","text":"前言 协同过滤算法，这是一个比较著名的推荐算法，其主要的功能就是预测和推荐，算法通过对用户历史行为数据的挖掘发现用户的偏好，基于不同的偏好对用户进行群组划分并推荐品味相似的商品，协同过滤算法分为两类，分别是基于用户的协同过滤算法和基于物品的协同过滤算法，简单的说，就是物以类聚，人以群分。 基于用户的协同过滤 基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系。在有相同喜好的用户间进行商品推荐。 说的再简单点，就是，你去买了一个XX手办，对它评价挺高，然后有一个路人甲也买了一个XX手办，评价和你很类似，然后根据算法算出你们的喜好可能是相近的，所以，下次路人甲买了其他的商品，也会把这件商品推荐给你，因为你们的相似度很高。当然了，这就是一个例子，商品的数量足够多，那么，推荐的也就越准确。 实现这个协同过滤算法的第一个重要步骤就是计算用户之间的相似度。而 计算相似简历相关系数矩阵目前主要分为： 皮尔逊相关系数 基于欧几里德距离的相似度 余弦相似度 我们这里就说基于欧几里德距离(又称欧式距离)这一种。 欧式距离计算相似度是所有相似度计算里面最简单、最易理解的方法。它以经过人们一致评价的物品为坐标轴，然后将参与评价的人绘制到坐标系上，并计算他们彼此之间的直线距离。计算出来的欧几里德距离是一个大0的数，为了使其更能体现用户之间的相似度，可以把它规约到(0.1]之间，最终得到如下计算公式： 只要有一个共同评分的项，就能用欧式距离计算相似度，如果没有共同评分项，那也就意味着两个用户或者物品并不相似。 python实现协同过滤 我们使用代码来实现一下协同过滤，我们首先创造模型，比如说，我们都喜欢看动漫，我们就可以根据个人的喜好来进行推荐，首先，新建txt文件，名字随意。 1,海贼王,2.0 1,火影忍者,5.0 1,全职猎人,2.6 2,刀剑神域,1.0 2,海贼王,5.0 2,犬夜叉,4.6 3,画江湖,2.0 3,全职高手,5.0 3,喜羊羊与灰太狼,2.6 解释一下：1喜欢看海贼王，给出了2.0的评分。以此类推，可以看到，1、2、3三个人中，只有1和2喜欢看海贼王，这也是他们之间唯一的交集。 content = [] with open('./comic.txt') as fp: content = fp.readlines() print(content) 首先，读取，这里就不用多说了。 # 将用户、评分、和动漫写入字典data data = {} for line in content: # 处理掉多余的空格并分割 line = line.strip().split(',') # 如果字典中没有某位用户，则使用用户ID来创建这位用户 if not line[0] in data.keys(): data[line[0]] = {line[1]: line[2]} # 否则直接添加以该用户ID为key字典中 else: data[line[0]][line[1]] = line[2] print(data) def Euclid(user1, user2): # 取出两位用户看过的动漫和评分 user1_data = data[user1] user2_data = data[user2] distance = 0 # 找到两位用户都看过的动漫，并计算欧式距离 for key in user1_data.keys(): if key in user2_data.keys(): # 注意，distance越大表示两者越相似 distance += pow(float(user1_data[key]) - float(user2_data[key]), 2) return 1 / (1 + sqrt(distance)) # 这里的返回值越小，相似度越大 计算某个用户和其他用户的相似度 def top_simliar(userID): res = [] for userid in data.keys(): # 排除与自己计算相似度 if not userid == userID: simliar = Euclid(userID, userid) res.append((userid, simliar)) # 排序 res.sort(key=lambda val: val[1]) return res 可以看到，与其他两位用户的相似度已经被推算出来了。 接下来，我们可以根据相似度来进行推送，按照我们这个例子，1和2之间是唯一有交集的，如果我们要给1推送，也只能推送2看过的，也就是犬夜叉和刀剑神域。 def recommend(user): # 相似度最高的用户 top_sim_user = top_simliar(user)[0][0] # 相似度最高的用户看过的动漫 items = data[top_sim_user] recommendations = [] # 筛选出该用户未看过的动漫 for item in items.keys(): if item not in data[user].keys(): recommendations.append((item, items[item])) recommendations.sort(key=lambda val: val[1], reverse=True) # 按照评分排序 return recommendations ok，结果还算不错，按照我们所想。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"}],"author":"天听"},{"title":"Casbin","slug":"Casbin","date":"2021-03-11T16:00:00.000Z","updated":"2021-03-11T16:00:00.000Z","comments":true,"path":"2021/03/12/casbin/","link":"","permalink":"http://godhearing.cn/2021/03/12/casbin/","excerpt":"","text":"前言 什么是casbin，Casbin是一个强大的、高效的开源访问控制框架，其权限管理机制支持多种访问控制模型。 Casbin可以做到： 支持自定义请求的格式，默认的请求格式为{subject, object, action}。 具有访问控制模型model和策略policy两个核心概念。 支持RBAC中的多层角色继承，不止主体可以有角色，资源也可以具有角色。 支持超级用户，如root或Administrator，超级用户可以不受授权策略的约束访问任意资源。 支持多种内置的操作符，如keyMatch，方便对路径式的资源进行管理，如/foo/bar可以映射到/foo* Casbin不能做到： 身份认证 authentication（即验证用户的用户名、密码），casbin只负责访问控制。应该有其他专门的组件负责身份认证，然后由casbin进行访问控制，二者是相互配合的关系。 管理用户列表或角色列表。 Casbin 认为由项目自身来管理用户、角色列表更为合适， 用户通常有他们的密码，但是 Casbin 的设计思想并不是把它作为一个存储密码的容器。 而是存储RBAC方案中用户和角色之间的映射关系。 安装python使用casbin需要安装，命令是 pip install casbin 教程 在 Casbin 中, 访问控制模型被抽象为基于PERM(Policy(策略), Effect(效果), Request(请求), Matcher(匹配器))的一个文件。 因此，切换或升级项目的授权机制与修改配置一样简单。 您可以通过组合可用的模型来定制您自己的访问控制模型。 例如，您可以在一个model中获得RBAC角色和ABAC属性，并共享一组policy规则。 比如说，我们定义一个model.conf来实现ACL模型 # 请求定义 [request_definition] r = sub, obj, act # 策略定义 [policy_definition] p = sub, obj, act # 策略效果 [policy_effect] e = some(where (p.eft == allow)) # 匹配器 [matchers] m = r.sub == p.sub &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act ⚠！这里注重说一下匹配器，这代表了你的匹配规则，比如现在这个例子，只有请求(r)和定义的策略(p)完全一致，才可以被认可。 ACL模型的示例，我们创建一个policy.csv： p, alice, data1, read p, bob, data2, write 这表示，alice可以读取data1，bob可以编写data2 然后我们在同级目录下测试，新建一个py文件 import casbin e = casbin.Enforcer(\"./model.conf\", \"./policy.csv\") # 你的请求 sub = \"alice\" obj = \"data1\" act = \"read\" if e.enforce(sub, obj, act): print('通过') else: print(\"拒绝\") 解释一下，我们通过casbin的Enforcer引入，然后定义了三个变量，去通过enforce验证他们，这也就代表了，他们会去policy.csv去查找策略，然后通过匹配器来进行匹配，如果通过了，他将会为你返回一个casbin，这是个布尔值。 RBAC我们再来演示一下RBAC，首先需要在model.conf中更改一下策略，新增加一个role_definition，然后更改一下匹配器的策略。所以，你的model.conf看起来像是这样 [request_definition] r = sub, obj, act [policy_definition] p = sub, obj, act [role_definition] g = _,_ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub,p.sub) &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act Casbin 支持 RBAC 系统的多个实例, 例如, 用户可以具有角色及其继承关系, 资源也可以具有角色及其继承关系。 这两个 RBAC 系统不会互相干扰。 此部分是可选的。 如果在模型中不使用 RBAC 角色, 则省略此部分。 _, _表示角色继承关系的前项和后项，即前项继承后项角色的权限。 然后，我们在policy.csv中重新定义我们的策略。 p, superuser, data1, all g, alice, superuser 首先，就是superuser有对data1资源的所有操作，而，alice的角色正好是superuser ok，那让我们回到py文件 首先，实例化这块没有什么改变，我们首先验证一下，你输入的用户是否有角色了。我们使用has_role_for_user import casbin dd = casbin.Enforcer(\"./model.conf\", \"./policy.csv\") sub = \"alice\" obj = \"data1\" act = \"all\" a = dd.has_role_for_user(sub,'superuser') print(a) 返回的是一个布尔值，这是我们在policy.csv定w的alice是superuser。 如果说，你想要为这个用户添加角色，则可以使用add_role_for_user，如果用户已经有该角色了，则返回false，这一块还没有太了解是会更改policy.csv还是说别的什么，所以，这一块暂时只能是先搁置了，后续待补。。。","categories":[{"name":"Casbin","slug":"Casbin","permalink":"http://godhearing.cn/categories/Casbin/"}],"tags":[{"name":"Casbin","slug":"Casbin","permalink":"http://godhearing.cn/tags/Casbin/"},{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"}],"author":"天听"},{"title":"Docker详解(巨详)","slug":"Docker详解(巨详)","date":"2021-03-08T16:00:00.000Z","updated":"2021-03-08T16:00:00.000Z","comments":true,"path":"2021/03/09/docker-xiang-jie-ju-xiang/","link":"","permalink":"http://godhearing.cn/2021/03/09/docker-xiang-jie-ju-xiang/","excerpt":"","text":"Docker核心概念镜像 类似于虚拟机镜像、或者是面对对象里的”类”。如果一个镜像可以包含一个基本的操作系统环境，里面仅安装Apache，就可以把它称为Apache镜像，镜像是创建 docker 容器的基础。 注意：镜像自身是只读的，容器从镜像启动的时候，会在镜像的最上层创建一个可写层。 容器 类似于一个轻量级的沙箱，或者是面对对象里的 “对象”。 docker 利用容器来运行和隔离应用，容器是从镜像创建的应用运行实例，可以将其启动、开始、停止、删除，而且这些容器都是彼此相互隔离的，互不可见的。 可以把容器看作是一个简易版的Linux系统环境，以及运行在其中的应用程序打包而成的盒子。 仓库 类似于代码仓库，它是 docker 集中存放镜像文件的场所，一个仓库集中存放某一类镜像，往往包括多个镜像文件，通过不同的标签（tag）来进行区分。 例如：存放 ubuntu 镜像的仓库称为 ubuntu 仓库，其中可能包含14.04、12.04等不同版本的镜像。 docker 仓库分为公开仓库和私有仓库，比较出名的公开仓库有： docker Hub、时速云、aliyun 等。 安装及配置 卸载旧版本 ：yum remove docker * ； 安装 docker ：yum -y install docker ，如果提示您接受 GPG 密钥，请选“是” ； [可选] 配置用户组： docker 安装后自动创建 docker 用户组，我们可以将用户添加到 docker 用户组，可以避免每次都使用 sudo 运行：usermod -aG docker YOUR_NAME ； [可选] 让 docker 服务可以通过本地 2375 端口接受来自外部的请求：docker _OPTS=”$ docker _OPTS -H tcp://0.0.0.0:2375 -H unix:/// /var/run/ docker .sock” ； [可选] 然后重启 docker 服务：systemctl restart docker ； 启动 docker ：systemctl start docker ； 运行 hello-world 镜像来验证是否正确安装 ：docker run hello-world ； 基础命令 info：显示 docker 系统信息，包括镜像和容器数； version：显示 docker 版本信息； 仓库管理命令 login ：登陆到一个 docker 镜像仓库，未指定镜像仓库地址，默认为官方仓库 docker Hub ； docker login [Options] [SERVER] [Options]: -u: 登陆的用户名 -p: 登陆的密码 logout ：登出镜像仓库，未指定镜像仓库地址，默认为官方仓库 docker Hub ； docker logout [SERVER] pull ：从镜像仓库中拉取或者更新指定镜像 ； docker pull [Options] NAME[:TAG] [Options]: -a: 拉取所有 tagged 镜像 --disable-content-trust: 忽略镜像的校验,默认开启 push ：将本地的镜像上传到镜像仓库 ； docker push [Options] NAME[:TAG] [Options]: --disable-content-trust: 忽略镜像的校验,默认开启 search ：从 docker Hub 查找镜像 ； docker search [Options] Name [Options]: --automated: 只列出 automated build 类型的镜像 --no-trunc: 显示完整的镜像描述 -s: 列出收藏数不小于指定值的镜像 -f：过滤条件？ 镜像管理命令 images：列出所有本地镜像 ； docker images [Options] [REPOSITORY[:TAG]] [Options]: -a: 列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层） --digests: 显示镜像的摘要信息 -f: 显示满足条件的镜像 --format: 指定返回值的模板文件 --no-trunc: 显示完整的镜像信息 -q: 只显示镜像ID rmi : 删除本地一个或多少镜像 ； docker rmi [OPTIONS] IMAGE [IMAGE...] -f: 强制删除 --no-prune: 不移除该镜像的过程镜像，默认移除 tag : 为镜像添加一个标签 ； docker tag IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] history：查看指定镜像的创建历史 ； docker history [Options] IMAGE [Options]: -H: 以可读的格式打印镜像大小和日期，默认为true --no-trunc: 显示完整的提交记录 -q: 仅列出提交记录ID save：将指定镜像保存成 tar 归档文件 ； docker save [Options] IMAGE [IMAGE...] [Options]: -o: 输出到的文件 # 示例： $ docker save centos_test_v3.tar centos:v3 load：导入使用 save 命令导出的镜像 ； docker load [Options] [Options]: --input,-i :指定导入的文件，代替 STDIN --quiet,-q :精简输出信息 # 示例1： $ docker load < busybox.tar.gz # Loaded image: busybox:latest # 示例2: $ docker load --input fedora.tar # Loaded image: fedora:latest import：导入镜像 ； docker import [Options] file|URL(压缩文件或url) [容器名:标签] [Options]: -c: 应用 docker 指令创建镜像 -m: 提交时的说明文字 # 示例： $ docker import centos_test.tar os/centos_test:v2 容器管理命令生命周期管理 run：根据镜像创建并启动一个容器 ； docker run [Options] image-name [Options]: -i: 以交互模式运行容器，通常与 -t 同时使用 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用 -a: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项 -d: 后台运行容器，并返回容器ID -p: 指定端口映射，格式为：8000:8000 -m: 设置容器使用内存最大值 -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -e username=\"ritchie\": 设置容器的环境变量 -h \"mars\": 指定容器的 hostname --restart='always': 设置容器自动重启，[no:不重启/on-failure:非0退出时重启] --name=\"nginx-lb\":为容器指定一个名称 --network='bridge'：指定容器的网络 --privileged=True：让容器获取宿主机 root 权限 --link=[]: 添加链接到另一个容器 --volume,-v: 绑定一个卷 --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致 --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致 --env-file=[]: 从指定文件读入环境变量 --cpuset=\"0,1,2\": 绑定容器到指定CPU运行 --net=\"bridge\": 指定容器的网络连接类型，支持 bridge/host/none/container --expose=[]: 开放一个端口或一组端口 --rm：退出容器时删除容器 create：创建一个新的容器但不启动它 ； docker create [Options] IMAGE [COMMAND] [ARG...] [Options]: 命令同 run start/stop/restart：启动/停止/重启 一个或多个容器 ； docker stop id/name[容器2, ...] docker start id/name[容器2, ...] docker restart id/name[容器2, ...] pause/unpause ：暂停/恢复 一个或多个容器进程 ； docker pause id/name[容器2, ...] docker unpause id/name[容器2, ...] kill：杀掉一个运行中的容器 ； docker kill [Options] id/name [Options]: -s :向容器发送一个信号 # 示例： $ docker kill -s KILL mynginx rm：删除容器 ； docker rm [Options] id/name[容器2, ...] [Options]: -f: 强制删除一个运行中的容器 -l: 移除容器间的网络连接，而非容器本身 -v: 删除与容器关联的卷 # 示例1：删除所有未运行的容器 $ docker rm $(docker ps -aq) # 示例2：删除所有终止状态的容器 $ docker container prune 容器操作 ps：查看容器 ； docker ps [Options] [Options]: -a: 显示所有的容器，包括未运行的 -q: 只显示容器编号 -l: 显示最近创建的容器 -n: 列出最近创建的n个容器 -s: 显示总的文件大小 -f: 根据条件过滤显示的内容 --format : 指定返回值的模板文件 --no-trunc: 不截断输出 exec：进入容器或在运行的容器中执行命令 ； docker exec [Options] id/name COMMAND [ARG...] [Options]: -d: 后台运行 -i: 即使没有附加也保持 STDIN 打开 -t: 分配一个伪终端 COMMAND：要在容器中执行的命令 [ARG...]: 命令的参数 # 示例： $ docker exec -it c3c7732da20e /bin/bash top：查看容器中运行的进程信息 ； docker top id logs：查看容器日志 ； docker logs [Options] id/name [Options]: -t: 显示时间戳 -f: 跟踪日志输出 --tail: 仅列出最新N条容器日志 --since: 显示某个开始时间的所有日志 inspect： 获取容器/镜像的元数据（配置信息）。 docker inspect [Options] id/name [Options]: -s: 显示总的文件大小 -f: 指定返回值的模板文件 --type: 指定返回类型 JSON attach：进入容器 ； docker attach [Options] id/name [Options]: --sig-proxy=false: 退出容器时不停止容器 注意：使用 attach 进入容器，ctrl + c/d 退出容器时会停止容器，所以需要使用 --sig-proxy=false 参数 events: 从服务器获取实时事件 ； docker events [Options] [Options]: -f：根据条件过滤事件 --since：从指定的时间戳后显示所有事件 --until：流水时间显示到指定的时间为止 wait: 阻塞运行直到容器停止，然后打印出它的退出代码 ； docker wait id/name export：导出容器 ； docker export [Options] id/name [Options]: -o: 将输入内容写到文件 # 示例1：将 id 为 abcdefg 的容器按日期保存为tar文件 $ docker export -o centos-'date +%Y%m%d'.tar abcdefg # 示例2：将容器 test_centos 导出为 centos_test.tar $ docker export test_centos > ./centos_test.tar port：列出指定的容器的端口映射 docker port id/name rename：重命名一个容器 ； update：更新一个或多个容器的配置 ； 容器rootfs commit：根据容器创建一个新的镜像 ； docker commit [Options] id/name 镜像名:标签 [Options]: -a: 提交的镜像作者 -c: 使用 docker file指令来创建镜像 -m: 提交时的说明文字 -p: 在commit时，将容器暂停 cp：用于容器与主机之间的数据拷贝 ； # 将容器内数据拷贝到外部 docker cp [Options] id/name:from_path to_host_path # 将外部数据拷贝到容器内 docker cp [Options] from_host_path id/name:to_path [Options]: -L: 保持源目标中的链接 diff：检查容器里文件结构的更改 ； docker diff id/name 网络管理命令docker network create：创建一个 docker 网络 ； docker network create [OPTIONS] net_name [OPTIONS]: -d：指定 docker 网络类型，有 bridge、overlay 等类型 ls：查看已有的 docker 网络 ； docker network ls -q：仅显示网络id -f：过滤条件 # 示例：根据网络类型过滤 $ docker network ls -f 'driver=bridge' connect：将容器连接到网络 ； docker network connect net_name container_id/name disconnect：断开容器与网络的连接 ； docker network disconnect [OPTIONS] net_name container_id/name [OPTIONS]: -f：强制断开 inspect：显示一个或多个网络上的详细信息 ； docker network inspect [Options] net_name [Options]: -f：用给定的模版格式化输出信息 rm：删除1个或多个网络 ； docker network rm net_name[net_2, ...] prune：删除所有未使用的网络 ； docker network prune [OPTIONS] [OPTIONS]: -f：不提示确认 Dockerfile dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 构建命令 build：dockerfile 利用 build 命令来构建一个镜像 ； docker build [Options] image_name:tag dockerfile_path [Options]: -c：cpu核心数 -m：最大内存 -t：镜像标记 # 注意：存放 dockerfile 的文件夹中不要放无关的文件，因为 docker 会收集 dockerfile工作路径的所有文件用来构建镜像! 语法规则 dockerfile 分为四个部分：基础镜像信息、维护者信息、镜像操作执行、容器启动时执行的指令 ； FROM：指定要引用的基础镜像，先从本地找，找不到再去 docker hub 找 ； 注意事项： 基础镜像有 tag 一定要带上，否则不识别 ； 任何 dockerfile 的第一条指令必须为 FROM ； 如果在同一个 dockerfile 中创建多个镜像，可以使用多个 FROM 指令； FROM centos:7.5.1804 RUN：在镜像 build 时执行命令 ； 注意事项： RUN 指令每执行一次都会在 docker 上新建一层镜像，造成镜像膨胀，所以无必要的话应该用 &amp;&amp; 连接多个指令； # 使用 RUN 指令有两种方式： # 1. 默认使用 shell 执行命令， RUN # 示例1： RUN -c # 等价于 /bin/sh -c # 2. 推荐用法，采用 exec 的格式执行命令 RUN [\"executable\", \"param1\", \"param2\"] \"executable\"：可执行文件 \"param1\"，\"param2\"：执行时的参数 # 示例2： RUN [\"/bin/bash\", \"-c\", \"echo hello\"] # RUN /bin/bash -c echo hello RUN [\"./test.php\", \"dev\", \"offline\"] # RUN ./test.php dev offline # 示例3：使用 && 连接多个命令 FROM centos RUN yum install wget \\ && wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ && tar -xvf redis.tar.gz CMD：类似于 RUN，为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束 ； 注意事项： 类似于 RUN 指令，但二者运行的时间点不同，CMD 在 docker run 时运行。 每个 dockerfile 只能有 1 个 CMD 指令，如有多个只会执行最后一条，且如用户启动容器时指定了运行的命令，则会覆盖 CMD 的命令； # 有三种格式： CMD CMD [\"executable\", \"param1\", \"param2\"] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 CMD [\"param1\", \"param2\"] ENTRYPOINT：类似于 CMD 指令，但不会被 docker run 的指定的参数覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序 ； 注意事项： CMD 指令指定参数将会成为 ENTRYPOINT 指定命令的参数 ； 每个 dockerfile 只能有1个 ENTRYPOINT 指令，当有多个时只有最后一个生效 ； 会被 docker run 时指定的 –entrypoint 参数覆盖 ； FROM nginx ENTRYPOINT [\"nginx\", \"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] # 变参 # 当不传参启动容器时 $ docker run nginx:test # 容器内执行 $ nginx -c /etc/nginx/nginx.conf # 当传参启动容器时 $ docker run nginx:test -c /etc/nginx/new.conf # 容器内执行 $ nginx -c /etc/nginx/new.conf ENV：指定环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量，在此镜像启动的容器中也会存在。 注意事项： 可以有多个 ENV 该指令指定的环境变量会被启动容器时的 –env 参数覆盖 ENV ENV = = ... ARG：构建参数，与 ENV 作用一至，不过作用域不一样，ARG 设置的环境变量仅在 docker build 的过程中有效 ； ARG ARG = WORKDIR：为后续的 RUN、CMD 和 ENTRYPOINT 指令配置工作目录，可以使用多个指令，后续的指令如果是相对路径，则会基于之前命令指定的路径； WORKDIR /path/to/workdir COPY：从宿主机复制数据到容器中 ； COPY src: 宿主机源路径 dest: 容器内目标路径，可以是绝对路径，也可以是工作目录(WORKDIR)的相对路径，如无路径会自动创建 ADD：类似 COPY ，不过在源文件为 tar 压缩文件的话，压缩格式为 gzip, bzip2 , xz 的情况下，会自动复制并解压到目标路径 ；如无自动解压的需求，请优先使用 COPY ； ADD src: 宿主机源路径 dest: 容器内目标路径 # 如源文件是 gzip bzip2 xz 格式的压缩文件，ADD 会自动解压 用法基本与 ADD相同，但是&lt;src&gt;只能是本地主机的路径 VOLUME：定义匿名数据卷，在启动容器时忘记挂载数据卷，会自动挂载到匿名卷，避免重要的数据，因容器重启而丢失 ； VOLUME [\"\", \"\", ...] VOLUME # 在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点 EXPOSE：仅仅只是声明容器内服务所监听的端口 ； EXPOSE USER：用于指定后续执行命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）； USER [:] # 当服务不需要管理员权限时，可以通过该命令指定运行用户 RUN groupadd -r postgres && useradd -r -g postgres postgres ONBUILD：配置当所创建的镜像作为其他镜像的基础镜像时，所执行的创建操作指令 ，可以有多个 ONBUILD 指令 ； ONBUILD [instruction] # 示例： ONBUILD ADD . /app/src STOPSIGNAL：指定镜像构建的容器接收的退出信号的值 ； STOPSIGNAL signal HEALTHCHECK：用于指定某个程序或者指令来监控 docker 容器服务的运行状态 ； HEALTHCHECK [Options] CMD command：设置检查容器健康状况的命令，根据所执行命令返回值来判断 ; [Options]: --interval=DURARION (默认：30s)：多久检查一次 --timeout=DURARION (默认：30s)：每次检查等待结果的超时时间 --retries=N (默认：3)：如果失败了，重试几次才最终确定失败 # 禁止基础镜像中的健康检查 HEALTHCHECK NONE SHELL：指定其他命令使用 shell 时默认的 shell 类型，默认值为：[“/bin/sh”, “-c”] ； SHELL [\"executable\", \"parameters\"] MAINTAINER：指定维护者信息 ； LABEL：用来生成镜像的元数据标签信息，可以有多个 LABEL ； LABEL LABEL = # 示例： LABEL version=\"1.0\" 示例FROM python:3.8-buster MAINTAINER Chen ENV LC_ALL C.UTF-8 ENV LANG C.UTF-8 ENV TZ Asia/Shanghai WORKDIR /app COPY ./the_first/ /app/the_first/ RUN pip install gunicorn && \\ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ -r /app/the_first/requirements.txt CMD cd ./the_first && \\ python3 manage.py collectstatic --noinput && \\ gunicorn --workers 2 --log-level debug --bind '0.0.0.0:8000' 'the_first.wsgi' Docker-compose语法规则version: '3' # 创建容器数据卷 volumes: # 数据卷名 static: # 创建网络 networks: # 网络名称 django_network: # 网络模式 driver: bridge # 创建加密文件 secrets: my_secret: file: ./my_secret.txt services: # 定义一个服务名 the_first_django: # 指定构建容器的 Dockerfile 所在的路径 build: # 指定上下文环境，是基于 compose.yml 的相对路径 context: ./ # 指定 Dockerfile的路径，是 基于 context 的相对路径 dockerfile: compose/production/django/Dockerfile # 指定构建过程中的环境变量，构建后删除 args: buildno: 1 password: secret # 指定容器运行的镜像，不使用 build image: django:latest # 指定容器名称 container_name: the_first # 网络模式：bridge｜host｜none # network_mode: \"service:[service name]\" # network_mode: \"container:[container name/id]\" network_mode: \"bridge\" # 加入指定的网络 networks: - django_network # 映射端口(宿主机:容器) ports: - \"8000:8000\" # 暴露端口，但不映射到宿主机，只被连接的服务访问 expose: - '8080' # 设置容器自动重启策略 # no：默认，在任何情况下都不会重启容器。 # always：总是重新启动 # on-failure：在容器非正常退出时（退出状态非0），才会重启容器。 # unless-stopped：总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 restart: on-failure:3 # 是否拥有宿主机 root 权限 privileged: true # 添加环境变量 environment: - RACK_ENV: development # 布尔值需要用引号引起来，以确保 YML 解析器不会将其转换为 True 或 False - SHOW: 'true' # 从文件添加环境变量 env_file: - ./common.env # 挂载一个目录或者已经存在的数据卷容器 volumes: - static:/app/the_first/static/ # 将该容器的静态文件共享到其他的容器中 - /opt/data:/var/lib/mysql # 使用绝对路径挂载数据卷 # :ro 表示为只读 - ./configs:/etc/configs/:ro # 使用以 Compose 配置文件为基本路径的相对路径 # 容器启动后执行的命令 # command: /start.sh # ENTRYPOINT，覆盖 RUN 命令 和 Dockerfile 中的同名指令 # entrypoint: /code/entrypoint.sh # 容器依赖 depends_on: - mysql-django # 另外的服务名 - redis-django # 指定设备映射列表 devices: - \"/dev/ttyUSB0:/dev/ttyUSB0\" # 自定义 DNS 服务器 dns: - 8.8.8.8 - 9.9.9.9 # 自定义 DNS 搜索域 dns_search: - dc1.example.com - dc2.example.com # 添加主机名映射，类似 docker client --add-host # 会在此服务的内部容器的 /etc/hosts 中创建 IP 和主机名的映射关系 extra_hosts: - \"somehost:162.242.195.82\" - \"otherhost:50.31.209.229\" # 存储敏感数据，例如密码 secrets: - my_secret # 在容器内安装一个临时文件系统 tmpfs: - /tmp # 用于检测 docker 服务是否健康运行 healthcheck: test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost\" ] # 设置检测程序 interval: 1m30s # 设置检测间隔 timeout: 10s # 设置检测超时时间 retries: 3 # 设置重试次数 start_period: 40s # 启动后，多少秒开始启动检测程序 # 链接到其它服务器中的容器 links: - db - db:mysql - redis # 让 compose 项目里面的容器链接到项目配置外部的容器（外部容器中必须至少有一个容器链接到项目内服务的同一个网络里） external_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql # 服务的日志记录配置 logging: # 指定服务容器的日志记录驱动程序: json-file/syslog/none driver: json-file options: # 当 driver: syslog 时 # syslog-address: \"tcp://ip\" # 指定日志接收地址 # 当 driver: json-file 时 max-size: \"200k\" # 单个文件大小为 200 k max-file: \"10\" # 最多10个文件 # 被依赖的服务 redis-django: image: redis mysql-django: image: mysql 常用命令# -f 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 docker-compose -f docker-compose.yml up -d # 启动所有容器，-d 将会在后台启动并运行所有的容器 docker-compose up -d # 停用移除所有容器以及网络相关 docker-compose down # 查看服务容器的输出 docker-compose logs # 列出项目中目前的所有容器 docker-compose ps # 构建（重新构建）项目中的服务容器。服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。可以随时在项目目录下运行 docker-compose build 来重新构建服务 docker-compose build # 拉取服务依赖的镜像 docker-compose pull # 重启项目中的服务 docker-compose restart # 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 docker-compose rm # 在指定服务上执行一个命令。 docker-compose run ubuntu ping docker.com # 设置指定服务运行的容器个数。通过 service=num 的参数来设置数量 docker-compose scale web=3 db=2 # 启动已经存在的服务容器。 docker-compose start # 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 docker-compose stop 示例version: '3' volumes: static: media: networks: django_network: driver: bridge services: # mysql 服务 mysql-django: image: mysql:latest container_name: mysql-django networks: - django_network ports: - 13306:3306 restart: always privileged: true environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: mysql13306123456 MYSQL_DATABASE: the_first volumes: - /usr/local/workspace/mysql/data/:/var/lib/mysql/ command: # MySQL8.0 之后，默认的加密规则使用的是 caching_sha2_password # 客户端工具不支持，所以不要修改其加密规则 --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci # redis 服务 redis-django: image: redis:latest container_name: redis-django networks: - django_network ports: - 16379:6379 restart: always privileged: true environment: TZ: Asia/Shanghai # 容器启动后，启动根据配置文件启动 redis-server command: redis-server /etc/redis/redis.conf --appendonly yes --requirepass 'redis16379123456' volumes: - /usr/local/workspace/redis/data:/data/ - /usr/local/workspace/redis/redis.conf:/etc/redis/redis.conf/ # django 服务 the_first: image: the_first:latest container_name: the_first networks: - django_network expose: - '8000' restart: always privileged: true environment: TZ: Asia/Shanghai volumes: - static:/app/the_first/static/ # 将该容器的静态文件共享到其他的容器中 - media:/app/the_first/media/ depends_on: - mysql-django - redis-django # nginx 服务 nginx-django: image: nginx:latest container_name: nginx-django networks: - django_network ports: - 80:80 - 443:443 restart: always privileged: true environment: TZ: Asia/Shanghai volumes: # 与 django 服务共享 static media 目录 - static:/app/the_first/static/ - media:/app/the_first/media/ - /usr/local/workspace/nginx/conf/nginx.conf:/etc/nginx/nginx.conf - /usr/local/workspace/nginx/conf.d:/etc/nginx/conf.d - /usr/local/workspace/nginx/html:/usr/share/nginx/html - /usr/local/workspace/nginx/log:/var/log/nginx depends_on: - the_first Docker 命令快捷栏 设置 docker 开机自动启动(服务器重启后自动重启) $ systemctl enable docker 进去容器内部 $ docker exec -it 容器id/name /bin/bash 启动一个 redis 容器 $ docker run -itd -p 16379:6379 --name='redis-django' --network='bridge' --restart='always' \\ -v /usr/local/workspace/redis/data:/data \\ -v /usr/local/workspace/redis/redis.conf:/etc/redis/redis.conf \\ redis redis-server /etc/redis/redis.conf --appendonly yes --requirepass 'redis16379123456' # 参数解释： # 挂载数据卷和配置文件 -v /usr/local/workspace/redis/data:/data -v /usr/local/workspace/redis/redis.conf:/etc/redis/redis.conf # 使用配置文件启动 /etc/redis/redis.conf --appendonly yes: 开启持久化 --requirepass 'redis16379123456': 设置密码 启动一个 mysql 容器 $ docker run -itd -p 13306:3306 --name='mysql-django' --network='bridge' --restart='always' -e MYSQL_ROOT_PASSWORD=mysql13306123456 \\ -v /usr/local/workspace/mysql/data:/var/lib/mysql \\ mysql:latest # 参数解释： # 挂载数据卷和配置文件 -v /usr/local/workspace/mysql/data:/var/lib/mysql -v /usr/local/workspace/mysql/etc:/etc/mysql -e: 为容器设置环境变量，为 mysql 初始密码 启动一个 rabbitmq 容器 $ docker run -itd -p 5672:5672 --name='rabbitmq-django' --network='bridge' --restart='always' rabbitmq:latest # 端口参数解释： 5672、5671: AMQP 0-9-1 without and with TLSclient 端通信口 15671: 管理监听端口 15672: 管理界面ui使用的端口 25672: (Erlang distribution）server 间内部通信口 4369: （epmd)epmd代表 Erlang 端口映射守护进程，erlang 发现口 启动一个 nginx 容器 ```shell $ docker run -itd -p 80:80 –name=’nginx-django’ –network=’bridge’ –restart=’always’ -v /usr/local/workspace/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \\ v /usr/local/workspace/nginx/conf.d:/etc/nginx/conf.d -v /usr/local/workspace/nginx/html:/usr/share/nginx/html -v /usr/local/workspace/nginx/log:/var/log/nginx nginx 参数解释：将宿主机 workspace/nginx/ 下的配置文件、日志、html 文件挂载到 Nginx 容器中 -v /usr/local/workspace/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /usr/local/workspace/nginx/conf.d:/etc/nginx/conf.d -v /usr/local/workspace/nginx/html:/usr/share/nginx/html -v /usr/local/workspace/nginx/log:/var/log/nginx 启动一个 Django 项目 $ docker run -itd -p 8000:8000 --name='django' --network='bridge' --restart='always' django","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"nginx跨域配置","slug":"nginx跨域配置","date":"2021-03-04T16:00:00.000Z","updated":"2021-03-04T16:00:00.000Z","comments":true,"path":"2021/03/05/nginx-kua-yu-pei-zhi/","link":"","permalink":"http://godhearing.cn/2021/03/05/nginx-kua-yu-pei-zhi/","excerpt":"","text":"当出现403跨域错误的时候 No 'Access-Control-Allow-Origin' header is present on the requested resource，需要给Nginx服务器配置响应的header参数 解决方案只需要在Nginx的配置文件中配置以下参数： location / { add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; if ($request_method = 'OPTIONS') { return 204; } } 上面配置代码即可解决问题了，不想深入研究的，看到这里就可以啦=-= 解释 Access-Control-Allow-Origin 服务器默认是不被允许跨域的。给Nginx服务器配置`Access-Control-Allow-Origin *`后，表示服务器可以接受所有的请求源（Origin）,即接受所有跨域的请求。 Access-Control-Allow-Headers 是为了防止出现以下错误： Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response. 这个错误表示当前请求Content-Type的值不被支持。其实是我们发起了”application/json”的类型请求导致的。这里涉及到一个概念：预检请求（preflight request）,请看下面”预检请求”的介绍。 Access-Control-Allow-Methods 是为了防止出现以下错误： Content-Type is not allowed by Access-Control-Allow-Headers in preflight response. 给OPTIONS 添加 204的返回，是为了处理在发送POST请求时Nginx依然拒绝访问的错误 发送”预检请求”时，需要用到方法 OPTIONS ,所以服务器需要允许该方法。 预检请求（preflight request）其实上面的配置涉及到了一个W3C标准：CROS,全称是跨域资源共享 (Cross-origin resource sharing)，它的提出就是为了解决跨域请求的。 跨域资源共享(CORS)标准新增了一组 HTTP 首部字段，允许服务器声明哪些源站有权限访问哪些资源。另外，规范要求， 对那些可能对服务器数据产生副作用的HTTP 请求方法（特别是 GET 以外的 HTTP 请求，或者搭配某些 MIME 类型的 POST 请求），浏览器必须首先使用 OPTIONS 方法发起一个预检请求（preflight request），从而获知服务端是否允许该跨域请求。服务器确认允许之后，才发起实际的 HTTP 请求。在预检请求的返回中，服务器端也可以通知客户端，是否需要携带身份凭证（包括 Cookies 和 HTTP 认证相关数据）。 其实Content-Type字段的类型为application/json的请求就是上面所说的搭配某些 MIME 类型的 POST 请求,CORS规定，Content-Type不属于以下MIME类型的，都属于预检请求： application/x-www-form-urlencoded multipart/form-data text/plain 所以 application/json的请求 会在正式通信之前，增加一次”预检”请求，这次”预检”请求会带上头部信息 Access-Control-Request-Headers: Content-Type： OPTIONS /api/test HTTP/1.1 Origin: http://foo.example Access-Control-Request-Method: POST Access-Control-Request-Headers: Content-Type ... 省略了一些 服务器回应时，返回的头部信息如果不包含Access-Control-Allow-Headers: Content-Type则表示不接受非默认的的Content-Type。即出现以下错误： Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response.","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/tags/Nginx/"}],"author":"天听"},{"title":"FastAPI学习之路-4  依赖注入","slug":"FastAPI学习之路-4 依赖注入","date":"2021-03-02T16:00:00.000Z","updated":"2021-03-02T16:00:00.000Z","comments":true,"path":"2021/03/03/fastapi-xue-xi-zhi-lu-4-yi-lai-zhu-ru/","link":"","permalink":"http://godhearing.cn/2021/03/03/fastapi-xue-xi-zhi-lu-4-yi-lai-zhu-ru/","excerpt":"","text":"前言 依赖注入系统，可以说是fastapi框架中很重要也是一个核心的系统。因为fastapi主要用的都是函数式编程实现API的方式，不像Django里面，有FBV和CBV。为了实现和Django一样的视图编程方式，于是就引入了这个依赖注入的概念。 依赖注入的优势： 提高代码的复用率，一些公共的资源，比如，函数，提取出来复用，一些pydantic验证也是如此。 共享数据库连接，避免操作数据库CURD时创造多条连接。 增强安全、认证和角色管理，就比如，超级管理员拥有什么权限。管理员拥有超级管理员之中的某些权限，而用户又只拥有管理员之下的某些权限。 从这个fastapi框架的角度来看，他的兼容性也是非常的强大的。通过依赖注入系统，可以支持SQL数据库，和NoSQL数据库。 声明依赖项 我们不以复杂的理论和代码来论证这个依赖注入，我们声明一个函数，用来当作依赖项来进行讲解。 async def common_parameters(q: Optional[str] = None, skip: int = 0, limit: int = 100): return {\"q\": q, \"skip\": skip, \"limit\": limit} 这里，我们声明了一个函数，有三个参数，q、skip、limit，我没有对他进行任何处理，只是原封不动的返回了，如果在真实的开发场景中，我们如果把所有的业务处理全部放在一个函数中，那就成了祖传代码，除了你自己，几乎别人没人看得懂。(PS：你自己也不一定看得懂了~~(●’◡’●)) 好了，回到我们这个场景，假如，我们要多次传入这三个参数，我们按照正常来说，也可以这么写： async def test1(q: Optional[str] = None, skip: int = 0, limit: int = 100) async def test2(q: Optional[str] = None, skip: int = 0, limit: int = 100) 然后，你会发现你的代码重复了，如果要进行相同的校验或者业务逻辑处理，需要更加繁琐的代码，也是需要一一重复。 这时候，我们把这个当作依赖，直接注入到函数中，就会减少重复。 @app.get('/') async def test(common_parameters:dict = Depends(common_parameters)): return common_parameters 这样，我们同样传入这三个参数，但是，在这个函数中，相应的减少了一些业务代码的处理。 类作为依赖项 fastapi不只是可以将函数作为依赖项，一切可以被称为资源的，都可以作为依赖项。比如，类。用法，几乎是一致的。 class common_parameters: def __init__(self,q:Optional[str] = None,skip: int = 0, limit: int = 100): self.q = q self.skip = skip self.limit = limit @app.get('/') async def test(common_parameters:dict = Depends(common_parameters)): return common_parameters 子依赖 可以理解为类的继承， 声明了一个依赖项，经过一些处理后，再被另一个依赖给处理，有点像二次加工 # 声明一个子依赖项 def query_extractor(q: Optional[str] = None): return q # 声明第二个依赖项，将子依赖作为依赖注入 def query_or_cookie_extractor( q: str = Depends(query_extractor), last_query: Optional[str] = Cookie(None) ): if not q: return last_query return q # 最后传到的 @app.get(\"/items/\") async def read_query(query_or_default: str = Depends(query_or_cookie_extractor)): return {\"q_or_cookie\": query_or_default} 路径中操作依赖项 在路径中，无论是fastapi项目的实例还是APIrouter的实例，所导的路径装饰器中，都可以进行依赖的注入。在路由中，使用dependencies来进行依赖的注入。他可以是一个Depends，也可以是一个列表，来进行多个依赖项的注入 # 声明两个依赖 async def verify_token(x_token: str = Header(...)): if x_token != \"fake-super-secret-token\": raise HTTPException(status_code=400, detail=\"X-Token header invalid\") async def verify_key(x_key: str = Header(...)): if x_key != \"fake-super-secret-key\": raise HTTPException(status_code=400, detail=\"X-Key header invalid\") return x_key # 在路由装饰器中，列表形式注入多个依赖项 @app.get(\"/items/\", dependencies=[Depends(verify_token), Depends(verify_key)]) async def read_items(): return [{\"item\": \"Foo\"}, {\"item\": \"Bar\"}] 全局依赖 在全局中，无论是APIRouter还是FastAPI,他们所实例的变量都可以通过dependencies来进行依赖的注入，这样，所有使用了他们的路由装饰器，都会进行其依赖的操作。 # 在FastAPI中使用依赖注入 app = FastAPI(dependencies=[Depends(verify_token), Depends(verify_key)]) # 在APIRouter中使用 apirouter = APIRouter(dependencies=[Depends(verify_token), Depends(verify_key)]) 关于fastapi 对于fastapi，我个人把他分成了几个部分，如果有感觉模糊不清的道友，可以参照这个思路，fastapi就非常的好理解了。 验证部分这里分为了几个验证部分，分别为：Body、Header、Path、Query、Cookie，其中，Body包括了pydantic，这里需要注意的一点就是，pydantic和fastapi不是一体的。不要搞混了。 请求部分关于对请求的处理，这里也包括了对Path的验证 依赖注入系统这个就不用多说了，😄 响应处理包括json、表单、文件等请求或者响应，这个不应该单独分出来，应该是和请求部分一致学习的 认证授权数据库等 只要是好好的给他分割开来，逐一击破，对于新的框架学习来说，学习成本实际上并没有这么的高。","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"}],"author":"天听"},{"title":"Elasticsearch详解","slug":"ElasticSearch详解","date":"2021-03-01T16:00:00.000Z","updated":"2021-03-01T16:00:00.000Z","comments":true,"path":"2021/03/02/elasticsearch-xiang-jie/","link":"","permalink":"http://godhearing.cn/2021/03/02/elasticsearch-xiang-jie/","excerpt":"","text":"前言Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。是一种流行的企业级搜索引擎。Elasticsearch用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。官方客户端在Java、.NET（C#）、PHP、Python、Apache Groovy、Ruby和许多其他语言中都是可用的。 详解 之前出过一篇攻略，关于安装，我们就不再重复了，可以查看这篇攻略以及索引。Docker安装ES 之前可能说的不是很详细，所以，我们还是需要再把这个原理和应用再细细划分一下。 我们在接触这个ES之前，一定是接触过数据库的，我们以管理型数据库mysql为例，对照一下，mysql和es之间的区别。 关系型数据库(Eg. MySQL) 非关系型数据库（Eg. ElasticSearch） 数据库Database 索引Index 表Table 类型Type 数据行Row 文档Dpcument 数据列Column 字段Field Node(节点)与Cluster(集群)Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。 Index(索引)Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。 所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。 下面的命令可以查看当前节点的所有 Index。 $ curl -X GET 'http://localhost:9200/_cat/indices?v' Document(文档)Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示，例如这样： { \"user\": \"天听\", \"title\": \"攻城狮\", \"desc\": \"CV大法传承人\" } 同一个 Index 里面的 Document，不要求有相同的结构，但是最好保持相同，这样有利于提高搜索效率。 Type(类型)Document 可以分组，比如有一个weather的Index，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。 不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。 类比一下MySQL，索引就好比是数据库，type就是数据表，文档，就是表中的数据。 下面的命令可以列出每个 Index 所包含的 Type。 $ curl 'localhost:9200/_mapping?pretty=true' 完结至于为什么要出这篇文章呢….emmmm….是时间太久，太久没用过这个东西，我自己也已忘的一干二净了，所以借此机会，再写一些关于ES的搜索知识用来巩固，自己的技术还得是总结下来才是自己的，否则时间一久，就跟没有似的。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}],"author":"天听"},{"title":"python发送阿里云短信","slug":"python发送阿里云短信","date":"2021-02-26T00:00:00.000Z","updated":"2021-02-26T00:00:00.000Z","comments":true,"path":"2021/02/26/python-fa-song-a-li-yun-duan-xin/","link":"","permalink":"http://godhearing.cn/2021/02/26/python-fa-song-a-li-yun-duan-xin/","excerpt":"","text":"前言 在当今这个网络如此发达的时代，手机已经成为了必不可少的随身物品，而，普通的单因子登录已经不是百分百安全的了，很容易被暴力破解，所以，双因子、三因子这些认证方式，已经从“你有什么”，变成了“你是谁”，从而让账户变得更加的安全，所以，短信验证码已经成为了必不可少的东西，今天，带来阿里云+python来发送短信。 1.注册阿里云 注册步骤很简单，地址：在这里，需要注意的是，发送短信只有实名认证的用户才能够使用，参考这里 2.创建AccessKey和Access Key Secret 3.获取签名名称和模板code 4.安装依赖1.安装pythonsdk # 安装阿里云的相关依赖 # python2执行此命令 pip install aliyun-python-sdk-core # python3执行此命令 pip install aliyun-python-sdk-core-v3 2.安装SDK 下载地址在这里，解压之后，进入根目录执行命令： python setup.py install #如果为python3，请执行：python3 setup.py install 3.安装api依赖 pip install aliyun-python-sdk-kms pip install aliyun-python-sdk-dysmsapi 5.写一个工具函数from aliyunsdkcore.client import AcsClient from aliyunsdkcore.request import CommonRequest # 用户AccessKey ACCESS_KEY_ID = \"\" # Access Key Secret ACCESS_KEY_SECRET = \"\" class SMS: def __init__(self,signName,templateCode): self.signName = signName self.templateCode = templateCode self.client = client = AcsClient(ACCESS_KEY_ID,ACCESS_KEY_SECRET,'cn-hangzhou') def send(self, phone_numbers, template_param): request = CommonRequest() request.set_accept_format('json') request.set_domain('dysmsapi.aliyuncs.com') request.set_method('POST') request.set_protocol_type('https') # https | http request.set_version('2017-05-25') request.set_action_name('SendSms') request.add_query_param('RegionId', \"cn-hangzhou\") request.add_query_param('PhoneNumbers', phone_numbers) request.add_query_param('SignName', self.signName) request.add_query_param('TemplateCode', self.templateCode) request.add_query_param('TemplateParam', template_param) response = self.client.do_action_with_exception(request) return response 6.调用此函数from random import randint class Aliyun_Sms(APIView): def get(self,request): # 生成验证码 params = \"{'code':%d}\"%(randint(1000,100000)) para = request.GET.get('phone_numbers') sms = SMS(\"模板名称\",\" 模版CODE\") res = sms.send(phone,params) return HttpResponse(res)","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"Linux下安装mysqlclient","slug":"Linux下安装mysqlclient","date":"2021-02-24T16:00:00.000Z","updated":"2021-02-24T16:00:00.000Z","comments":true,"path":"2021/02/25/linux-xia-an-zhuang-mysqlclient/","link":"","permalink":"http://godhearing.cn/2021/02/25/linux-xia-an-zhuang-mysqlclient/","excerpt":"","text":"在 Linux 下安装 mysqlclient，如果是选择直接安装的话，一般会报如下错误 (djenv) handsome@fzq:~$ pip install mysqlclient -i https://pypi.doubanio.com/simple/ Looking in indexes: https://pypi.doubanio.com/simple/ Collecting mysqlclient Downloading https://pypi.doubanio.com/packages/ec/fd/83329b9d3e14f7344d1cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.gz (90kB) 100% |████████████████████████████████| 92kB 2.1MB/s Complete output from command python setup.py egg_info: /bin/sh: 1: mysql_config: not found Traceback (most recent call last): File \"&lt;string>\", line 1, in &lt;module> File \"/tmp/pip-install-y16_5477/mysqlclient/setup.py\", line 18, in &lt;module> metadata, options = get_config() File \"/tmp/pip-install-y16_5477/mysqlclient/setup_posix.py\", line 53, in get_config libs = mysql_config(\"libs_r\") File \"/tmp/pip-install-y16_5477/mysqlclient/setup_posix.py\", line 28, in mysql_config raise EnvironmentError(\"%s not found\" % (mysql_config.path,)) OSError: mysql_config not found ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-y16_5477/mysqlclient/ 解答方案， 一般来说安装一个依赖即可 (djenv) handsome@fzq:~$ sudo apt install libmysqlclient-dev 安装完依赖之后，再次执行pip install mysqlclient -i https://pypi.doubanio.com/simple/ 即可成功安装","categories":[{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"}],"author":"天听"},{"title":"扫描普通链接进入微信小程序","slug":"扫描普通链接进入微信小程序","date":"2021-02-19T16:00:00.000Z","updated":"2021-02-19T16:00:00.000Z","comments":true,"path":"2021/02/20/sao-miao-pu-tong-lian-jie-jin-ru-wei-xin-xiao-cheng-xu/","link":"","permalink":"http://godhearing.cn/2021/02/20/sao-miao-pu-tong-lian-jie-jin-ru-wei-xin-xiao-cheng-xu/","excerpt":"","text":"前言 为了更加方便的将小程序推广，微信现在推出了扫描普通二维码进入某个微信小程序的指定页面，而小程序是没有链接性的地址的，所以，我们还是要先阅读一下文档 实现步骤step 1：开启扫描二维码打开小程序登录小程序后台，进入“开发-开发设置-扫普通链接二维码打开小程序”，开启功能后即可配置二维码规则。 step2：配置二维码二维码规则： 二维码规则的域名须通过ICP备案的验证。 支持http、https、ftp开头的链接（如：http://wx.qq.com、https://wx.qq.com/mp/、https://wx.qq.com/mp?id=123） 一个小程序帐号可配置不多于10个二维码前缀规则。 校验文件： 下载随机校验文件，并将文件上传至服务器指定位置的目录下，方可通过所属权校验。 这里要确定的是，能够访问到这个文件，我这里直接使用的nginx，在nginx下添加配置 location /tra/校验文件名.txt { alias /校验文件名.txt; } 使用微信扫码，只要是你配置的前缀名，他就会去服务器找这个校验文件，如果校验成功，就会去小程序你指定的页面。 小程序功能页面： 小程序功能页面可打开指定页面，扫描二维码可打开对应页面。 step3：扫描二维码打开小程序配置二维码时可配置测试链接，利用第三方二维码生成工具，使用测试链接生成二维码，利用微信“扫一扫”或微信内长按识别二维码跳转小程序。 链接?后为参数部分，可在onLoad事件中提取q参数并自行decodeURIComponent一次，即可获取原二维码的完整内容。 onLoad: function (options) { if (options.q) { let queryAll = decodeURIComponent(options.q); let id = gup('id', queryAll); //console.log(queryAll); //console.log(id); } }, /** * 获取URL中某个字符串字段 * gup('id', 'https://www.lubanso.com/wx/home/?id=bHViYW5zb7W7DJI=&amp;jhkfdhkjfda') * //===> bHViYW5zb7W7DJI= */ function gup(name, url) { if (!url) url = location.href; name = name.replace(/[\\[]/, \"\\\\\\[\").replace(/[\\]]/, \"\\\\\\]\"); var regexS = \"[\\\\?&amp;]\" + name + \"=([^&amp;#]*)\"; var regex = new RegExp(regexS); var results = regex.exec(url); return results == null ? null : results[1]; }","categories":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"tags":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"author":"天听"},{"title":"Vue中使用setTimeout遇到的问题","slug":"Vue中使用js--setTimeout","date":"2021-02-18T16:00:00.000Z","updated":"2021-02-18T16:00:00.000Z","comments":true,"path":"2021/02/19/vue-zhong-shi-yong-js-settimeout/","link":"","permalink":"http://godhearing.cn/2021/02/19/vue-zhong-shi-yong-js-settimeout/","excerpt":"","text":"遇到的问题1我在执行setTiemout之后，效果并没有触发，然后发现语法错误 // 错误写法 for (var i = 0; i &lt; 6; i++) { setTimeout( console.log(`这是第 ${i} 次`); ,1000); } // 正确写法 for (var i = 0; i &lt; 6; i++) { (function (t, data) { // 注意这里是形参 setTimeout(function () { console.log(`这是第 ${t} 次，这是其他参数：${data}`); }, 1000 * t); // 还是每秒执行一次，不是累加的 })(i, '其他参数') // 注意这里是实参，这里把要用的参数传进去 } 这是个小插曲，之后，我遇到了Uncaught TypeError: this.XXXXX is not a function 原因是，没在一个域内，所以，直接使用this会报错 解决办法1在函数内新生成一个that变量，然后用that代替this var that = this a(){ that.B } 遇到的问题2我直接调用了this变量，可能变量会undefined 解决办法2不要要this，直接let 一个新变量，使得在作用域是同一片","categories":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"python3获取微信小程序码","slug":"python3获取微信小程序码","date":"2021-02-08T16:00:00.000Z","updated":"2021-02-08T16:00:00.000Z","comments":true,"path":"2021/02/09/python3-huo-qu-wei-xin-xiao-cheng-xu-ma/","link":"","permalink":"http://godhearing.cn/2021/02/09/python3-huo-qu-wei-xin-xiao-cheng-xu-ma/","excerpt":"","text":"前言 微信小程序是现在开发者经常接触到的了，对微信小程序的一些功能也在逐渐的了解当中，这篇文章记录一下获取微信小程序码遇到的问题 获取access_token 话不多说，直接上代码 url = \"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential\" res = requests.get( url=url, params={ 'appid': 你的APPID, 'secret': 你的SECRET, } ).json().get('access_token') 获取小程序码首先就是要注意，它发送的是json，而不是data数据，所以我们在使用requests的时候，一定要先注意，发送json # 获取小程序码 url2 = \"https://api.weixin.qq.com/wxa/getwxacodeunlimit?access_token=TOKEN\" req = { 'scene': scene, 'page': page, 'width': width, } res = requests.post(url=url2, json=req).content buf = io.BytesIO() buf.write(res) response = HttpResponse(content=buf.getvalue(), content_type='image/png') response['sss'] = 'sss' return response 此时要注意，我们在url里传access_token，而不是在json里。他的文档写的并不是很清楚，如果在json中加上access_token，那么就有可能会报错47001。 还有，如果没注意到，用了data传参，也有可能会报错47129，改成json即可。 如果成功，返回的是一个二进制文件，需要怎么处理就看自己的了。我这里是以文件流形式返回回去并且添加一个响应头参数。 还有最重要的一点，41030状态码，并不是说你错了，而是你的小程序还没有上线，这个page路径找不到。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"阿里云域名解析","slug":"阿里云域名解析","date":"2021-02-08T16:00:00.000Z","updated":"2021-02-08T16:00:00.000Z","comments":true,"path":"2021/02/09/a-li-yun-yu-ming-jie-xi/","link":"","permalink":"http://godhearing.cn/2021/02/09/a-li-yun-yu-ming-jie-xi/","excerpt":"","text":"前言对于刚开始接触网站搭建的新手来说，好多东西都需要去了解学习，搭建网站首先需要购买服务器，然后购买域名，然后是域名解析，最后是域名备案等这些大的流程步骤。本节就来将域名的购买备案和解析的步骤，服务器是以阿里云服务器来讲。 申请域名 首先就是去万网购买域名，地址在这里，选择你喜欢的域名和后缀，之后就可以加入清单并且购买。 然后，在国内的域名是需要去备案的 备案首先就是要有了自己的服务器实例，然后在控制台去申请备案服务号。 申请步骤按照它的提示来即可，非常简单。 有了备案服务号之后，就可以去申请备案服务号的地方点开ICP代备案系统，提供它所需要的信息，比如你网站的用途啊，还有你的网站负责人的信息，如果是你本人那就是填写你自己的信息啦！这里需要注意的是，你的网站名称，不要用什么什么技术博客之类的，可以改为使用你自己的名言或者一个其他名字就可以了，反正只有你自己看的见。 提交完成后，等待他的流程结束，你的域名也就处于可用状态了。 然后就可以解析到你的服务器上。 解析域名具体步骤 打开控制台，进入域名 找到想要解析的域名 添加记录。服务器公网ip 主机记录，就填写www就可以 解析线路。选择系统默认即可。 记录值。指的是服务器/虚拟主机的外网IP地址。查看外网IP地址，就直接去购买的服务器详情里面查看。 TTL。一般以系统默认的情况就行，默认的10分钟就行。 填写完添加记录之后，最后点击“确定”按钮即可解析完成。 在nginx中配置域名 解析完成，你的服务器和你的域名都处于可用状态，这时候，你就可以将自己的域名配置到nginx中。 server { listen 8000 default_server; listen [::]:8000 default_server; server_name # 改为自己的域名 root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { root /home/hexo; # 你的博客地址 index index.html index.htm; } error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 然后执行这两条命令，重启nginx，就可以通过自己的域名来访问啦。 systemctl stop nginx systemctl start nginx https解析 在阿里云上，是会赠送免费的ssl证书的，首先需要打开控制台，搜索ssl并进入，然后选择云盾ssl证书 为了解决免费证书近期存在的吊销、统计等问题，自2021年起，免费证书申请将切换到证书资源包下了，地址在这里 然后支付0元之后，他就在你的云盾ssl证书中了，直接申请即可，点击下载，选择nginx，你会下载一个压缩包，把其中的两个文件放在你的服务器上，你自己知道的位置即可。 然后再在nginx配置中，重新配置一个server，使用443端口。并加上如下配置. server { listen 443 ssl http2 default_server; listen [::]:443 ssl http2 default_server; server_name www/godhearing.cn; root /usr/share/nginx/html; ssl_certificate \"你下载的位置.pem\"; ssl_certificate_key \"你下载的位置.key\"; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. # include /etc/nginx/default.d/*.conf; # location / { root /home/hexo; index index.html index.htm; } error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } } 然后，重启nginx即可。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/tags/Hexo/"}],"author":"天听"},{"title":"阿里云服务器搭建hexo博客教程","slug":"阿里云服务器搭建hexo博客教程","date":"2021-02-08T16:00:00.000Z","updated":"2021-02-08T16:00:00.000Z","comments":true,"path":"2021/02/09/a-li-yun-fu-wu-qi-da-jian-hexo-bo-ke-jiao-cheng/","link":"","permalink":"http://godhearing.cn/2021/02/09/a-li-yun-fu-wu-qi-da-jian-hexo-bo-ke-jiao-cheng/","excerpt":"","text":"前言 作为一名合格的程序员，搭建个人博客对于程序员来讲必不可少。为什么？一是因为互联网的知识需要我们不断学习，如果我们我们经常能够进行总结记录，时不时回头复习一下，效果会好很多； 二是因为作为程序员经常会遇到各种各样的 bug，如果我们将犯的错误记录下来，下次再次遇到同样的问题时，就不用再反复再网上寻找答案了，能为我们节省出很多时间。 因此，搭建个人博客有以下几点好处： 提升知识掌握的深度，印象深刻； 锻炼自己能够把问题“讲清楚”的能力； 培养开源意识，与他人分享知识。 这里呢，Hexo怎么搭建，我就不说了，相信大家都有搭建hexo的能力。 服务端准备工作域名注册登陆 https://wanwang.aliyun.com/ 挑选一个自己喜欢的域名，具体操作如下： 1、进入网站主页后，点击“域名注册”: 2、然后在万网进行域名搜索和购买。 3、初始化自己的服务器 4、备案，在阿里云的控制台界面，具体备案步骤呢，也就不多说了，往后有机会出一篇备案的攻略。 5、阿里云的服务器默认不开放端口号，这样使得我们在网站部署完成之后仍然无法访问。因此我们需要新建安全组并添加 80 端口，再将安全组添加到 ECS 实例中。具体操作如下。在控制台的 ECS 实例中点击安全组，然后新建安全组。 在访问规则为 入方向 的标签下添加 80 端口。 最后回到 ECS 服务器实例，将刚刚配置的安全组加入到实例中。 安装 nginx我们使用 nginx 作为 web 服务器，这里可以将 nginx 理解为安装在服务器上的一个软件。 还有可以直接使用yum等来安装，不是非得必要用我这个办法。 安装 nginx 依赖环境，安装期间有提示一律选 yes。推荐直接复制每行命令，避免出错。 yum install gcc-c++ yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 下载 nginx 安装包。 wget -c https://nginx.org/download/nginx-1.10.1.tar.gz 将安装包解压到 /usr/local 目录下。 tar -xvf nginx-1.10.1.tar.gz -C /usr/local 进入 /usr/local 目录，确认 nginx 解压到该目录下。 cd /usr/local 进入 nginx-1.10.1 目录，会发现该目录下有一个 configure 文件，执行该配置文件。 cd nginx-1.10.1/ ls ./configure 编译并安装 nginx make make install 查找nginx安装目录 whereis nginx 进入安装目录 cd /usr/local/nginx ls 由于 nginx 默认通过 80 端口访问，而 Linux 默认情况下不会开发该端口号，因此需要开放 linux 的 80 端口供外部访问。 /sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEPT 进入 /usr/local/nginx/sbin 目录，启动 nginx。 cd sbin ./nginx 没有任何消息，代表启动成功。此时输入公网 IP 即可进入 nginx 的欢迎页面了。 进入 /usr/local/nginx/conf 目录，并对 nginx.conf 配置文件进行相关配置。 cd /usr/local/nginx/conf ls vim nginx.conf 打开后按 i 键由命令模式切换到编辑模式，修改三处地方： 首先将最顶端的用户改为 root； 其次将 server_name 改为自己的域名，如果没有备案，可以先填写自己的公网 IP（在控制台 ECS 实例中查看），访问时暂时用公网 IP 进行访问。最后将 root 项中的值改为你自己博客的根目录 修改结束之后，先按 Esc 由编辑模式切换到命令模式，再输入 :wq 命令保存并退出编辑器。 配置git为 hexo 创建一个部署目录 /home/hexomkdir -p /home/hexo 进入 /usr/local/nginx/conf 目录，并对 nginx.conf 配置文件进行相关配置。 cd /usr/local/nginx/conf ls vim nginx.conf 打开后按 i 键由命令模式切换到编辑模式，修改三处地方： 首先将最顶端的用户改为 root； 其次将 server_name 改为自己的域名，如果没有备案，可以先填写自己的公网 IP（在控制台 ECS 实例中查看），访问时暂时用公网 IP 进行访问。 最后将 root 项中的值改为 /home/hexo;，如果 server 中的端口号不是 80，则改为 80。 修改结束之后，先按 Esc 由编辑模式切换到命令模式，再输入 :wq 命令保存并退出编辑器。 安装 node.js返回用户根目录，进行安装 node.js。 cd ~ curl -sL https://rpm.nodesource.com/setup_10.x | bash - yum install -y nodejs 通过查看版本号验证是否安装成功。 node -v npm -v 创建 Git 用户为了使我们能够在本地向服务器实现自动部署，需要在服务器端另外新建一个 Git 用户，然后使用公钥连接成功之后，就可以方便地随时进行自动部署了。 复制粘贴以下命令安装 Git，有提示选择 yes 即可。 yum install git 安装结束之后，查看版本号判断是否安装成功。 git --version 创建 Git 用户 adduser git 修改 Git 用户权限为 740 chmod 740 /etc/sudoers 在配置文件中增加 Git 用户。首先打开文件： vim /etc/sudoers 找到root ALL=(ALL) ALL，在他下面，添加上git ALL=(ALL) ALL 将 Git 用户的权限改回去。 chmod 400 /etc/sudoers 设置 Git 用户密码 sudo passwd git 以上我们就完成了 Git 用户的创建，接下来我们向 Git 用户添加公钥，就像配置 Github 那样。 向 Git 用户配置 ssh 公钥在服务器端 切换到 git 用户，在根目录下创建 .ssh文件夹。 su git cd ~ mkdir .ssh 这时，命令行信息中的 # 变成了 $，且 root 变成了 git，表示我们切换成功。 注意哦，然后在本地计算机桌面右键打开 GitBash Here，在本地生成公钥/私钥对。 cd ~ cd .ssh ssh-keygen 如果有询问直接回车即可。结束之后，会在 C:\\Users\\你的本地用户名\\.ssh 里生成两个文件：id_rsa 和 id_rsa.pub。.ssh 为隐藏文件夹，你可能需要显示隐藏文件夹之后才可以看到。 在 本地 终端输入以下命令，为私钥设置权限。PS：终端用不了的话，用git bash chmod 700 ~/.ssh chmod 600 ~/.ssh/id_rsa 将这两个文件，复制进服务器里，然后将id_rsa.pub复制一份到/home/git/.ssh cp id_rsa.pub authorized_keys 修改文件权限PS：应该git用户无法修改，输入su返回超级用户 chmod 600 ~/.ssh/authorized_keys chmod 700 ~/.ssh 确保设置了正确的SELinux上下文。 restorecon -Rv ~/.ssh 现在我们来测试一下是否设置成功。在本地任意位置右键打开 GitBash Here，输入公网 IP： ssh -v git@xxx.xxx.xxx.xxx（你的公网 IP） 如果最后提示Welcome to Alibaba Cloud Elastic Compute Service !，我们就是成功了 在服务端配置 Git 仓库 cd ~ git init --bare hexo.git vi ~/hexo.git/hooks/post-receive 进入后按 i 键由命令模式切换到编辑模式。输入以下命令后保存： git --work-tree=/home/hexo --git-dir=/home/git/hexo.git checkout -f 授予钩子文件可执行权限。 chmod +x ~/hexo.git/hooks/post-receive cd ~ sudo chmod -R 777 /home/hexo 重启 ECS 实例。reboot 至此，我们就完成了服务端的配置 本地Hexo配置安装两个插件 修改 _config.yml 文件 把 deploy 参数改成如下方式，注意填写自己的公网 IP 哦。 除此之外，URL 项改为自己的域名，没有备案的化可以先填写公网 IP。 然后我们就可以进行发布啦 hexo clean hexo generate hexo deploy 或 hexo cl hexo g hexo d 或 hexo cl && hexo g && hexo d 发布三件套命令，哪个都行 这时，在浏览器中输入自己的公网 IP，你就可以看到自己的博客了。 还有一件事，就是域名是需要解析的，具体步骤请参考这篇攻略 参考文章","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/tags/Hexo/"}],"author":"天听"},{"title":"python PIL中文变方框的解决办法","slug":"python PIL中文变方框的解决办法","date":"2021-02-05T16:00:00.000Z","updated":"2021-02-05T16:00:00.000Z","comments":true,"path":"2021/02/06/python-pil-zhong-wen-bian-fang-kuang-de-jie-jue-ban-fa/","link":"","permalink":"http://godhearing.cn/2021/02/06/python-pil-zhong-wen-bian-fang-kuang-de-jie-jue-ban-fa/","excerpt":"","text":"前言 最近有个小功能，我看来十分容易，无非就是往图片上添加个文字，但是，在服务器上，瞬间爆炸，所有的中文，都变成一个个的框框，我走了很多弯路，踩了很多坑，在这里记录一下，希望能帮助到同样遇到这个问题的人。 问题描述 代码from PIL import Image, ImageDraw, ImageFont image= Image.new('RGB', (559, 320),(255,255,255)) draw = ImageDraw.Draw(image) # draw.text() font = ImageFont.truetype(\"arial\", 40, encoding=\"unic\") # 设置字体 draw.text((100, 50), \"哈哈哈\", 'black', font) # del draw image.show() 解决办法 我踩了很多的坑，发现有一种及其简易的方式来解决这个问题 修改字体为simsun.ttc即可，听着简单，但是，假如身处服务器，根本没有这个字体，你需要从windows下copy过去 打开”C:\\Windows\\WinSxS\\amd64_microsoft-windows-font-truetype-simsun_31bf3856ad364e35_10.0.18362.1_none_cd668f05ece74044 名字可能会不同，可以搜索simsun.ttc 通过连接服务器，传到服务器的 /usr/share/fonts文件夹下，如果没有就自己建一个 在此目录下，执行#fc-cache -fv，扫描字体目录并生成字体信息的缓存，然后应用程序就可以”立即”使用这些新安装的字体 然后重启你的服务器 按照相对路径或者绝对路径来进行使用即可。 效果","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"Docker部署Django应用","slug":"Docker部署Django应用","date":"2021-01-28T16:00:00.000Z","updated":"2021-01-28T16:00:00.000Z","comments":true,"path":"2021/01/29/docker-bu-shu-django-ying-yong/","link":"","permalink":"http://godhearing.cn/2021/01/29/docker-bu-shu-django-ying-yong/","excerpt":"","text":"前言 在开发完成之后，我们所有的应用，都是要部署到生产环境中的，而docker的出现，让我们看到了容器式部署的好处，就是便捷。 打包 安装docker和开发web应用我就不多说了，既然能了解到这里，那这些步骤相信对大家来说都是小菜一碟。 我们从打包开始，首先就是我们使用的Gunicorn的配置，在项目根目录下新建一个gunicorn.conf.py import multiprocessing workers=3 # 并行工作进程数 threads = 2 # 指定每个工作者的线程数 bind=['0.0.0.0:8000'] # 监听内网端口8000 proc_name='yawp' # 进程名称 pidfile='/tmp/yawp.pid' # 设置进程文件目录 worker_class='gevent' # 工作模式协程 timeout=30 # 超时 errorlog = '/home/gunicorn.error.log' # 发生错误时log的路径 accesslog = '/home/gunicorn.access.log' # 正常时的log路径 然后我们改完项目的一些配置之后，在根目录下新建一个Dockerfile，注意，名字一定要正确，没有任何的后缀，不要自行脑补 FROM python:3.7 WORKDIR /Project/你的项目名 COPY requirements.txt ./ RUN pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple COPY . . ENV LANG C.UTF-8 CMD [\"gunicorn\", \"Blue_Lake.wsgi:application\",\"-c\",\"./gunicorn.conf.py\"] 给不懂Dockerfile命令的同志讲解一下，Dockerfile一般有以下几个基础命令 基础镜像，以哪个镜像作为基础进行制作，用法是FROM基础镜像名称 维护者信息，需要写下该Dockerfile编写人的姓名和邮箱，用法是MIANTAINER 名字/邮箱 镜像操作命令，对基础的镜像要进行的改造命令，比如安装新的软件，进行哪些特殊配置等，常见的是RUN命令 容器启动命令，当基于该镜像的容器启动时需要执行哪些命令，常见的是CMD命令或ENTRYPOINT命令 然后准备工作完成后，我们就可以进行打包了 docker build -t 项目名 . 第一次打包可能会比较慢，如果遇到网络问题，重复执行打包命令即可 还有一件事，重复执行打包命令，会产生空悬镜象，这个解决办法请自行百度，不过不占用我们的空间，不管他也没有事。 然后键入命令docker images 就可以在看到你的项目镜像了，如果想把这个打包好的项目导出，只需要执行 docker save -o 你要导出的文件名.tar 镜像名 这样你就可以把他装进U盘，随时带走，也可以进行docker push操作将其上传到dockerhub上。 然后启动项目 docker run --name 容器名 -d -p 8000:8000 镜像名 以前也写过一篇类似的攻略，不过当时有点懵懂，所以，我又再次写了一篇攻略，另外，还有一些细小的命令，比如挂载和复制出来。 docker cp 4570:/Project/你的项目/settings.py 宿主机地址/settings.py 这种场景主要是体现在我们需要修改某些配置的时候，我们需要知道的是，不同的项目打包完之后，项目目录并不是你所熟知的项目目录，就比如我这个django项目，打包完成之后，他的容器内部项目结构是这样的。 我们所熟知的项目，就在Project里。 然后修改，可以使用vim，也可以使用别的，这都无所谓了。修改完之后，怎么将这个给挂载回去呢，这时候，需要用到我们的-v命令。 docker run --name 你的镜像名 -d -p 8001:8000 -v /宿主机文件地址/settings.py:/Project/你的项目/settings.py 镜像名 docker的部署，就是这么简单，比一条条的输入命令要强的多了吧。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"Vue解决html2canvas截图不全的问题","slug":"Vue解决html2canvas截图不全的问题","date":"2021-01-26T16:00:00.000Z","updated":"2021-01-26T16:00:00.000Z","comments":true,"path":"2021/01/27/vue-jie-jue-html2canvas-jie-tu-bu-quan-de-wen-ti/","link":"","permalink":"http://godhearing.cn/2021/01/27/vue-jie-jue-html2canvas-jie-tu-bu-quan-de-wen-ti/","excerpt":"","text":"问题截图 原因分析我所知道的原因有两点， 第一点：在点击保存图片时，此时要保存的资源较多，造成模块并没有完全加载完毕，就已经生成了截图； 解决方案：(加上一个延时操作) // 利用 html2canvas 下载 canvas setTimeout(() => { html2canvas(img, { canvas: canvas }).then(function(canvas) { _this.photoUrl = canvas.toDataURL(); }); }, 500); 第二点：滚轮滑动造成的，主要是html2canvas是根据body进行截图，若内容高度高于body时，就会出现这样的问题(大概意思就是有滚动条时造成的)解决方案：(在生成截图前，先把滚动条置顶) window.pageYOffset = 0; document.documentElement.scrollTop = 0 document.body.scrollTop = 0","categories":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"Docker编译时遇到的bug","slug":"Docker编译时遇到的bug","date":"2021-01-24T16:00:00.000Z","updated":"2021-01-24T16:00:00.000Z","comments":true,"path":"2021/01/25/docker-bian-yi-shi-yu-dao-de-bug/","link":"","permalink":"http://godhearing.cn/2021/01/25/docker-bian-yi-shi-yu-dao-de-bug/","excerpt":"","text":"遇到的问题 解决办法 在这里注册一个账号，然后终端输入docker login，输入账号密码之后，再次执行打包命令即可 第二种方法 第二种方法就比较简单加粗暴了，换源，推荐阿里的镜像源， 其他的也可以 按照这里，去获得分配的加速器地址。 打开docker桌面版的设置，也就是settings 将这里的地址换成阿里云给分配的地址 然后终端输入docker info 查看换源成功，即可","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"axios获取文件流并且展示为图片","slug":"axios获取文件流形式如何展示","date":"2021-01-22T16:00:00.000Z","updated":"2021-01-22T16:00:00.000Z","comments":true,"path":"2021/01/23/axios-huo-qu-wen-jian-liu-xing-shi-ru-he-zhan-shi/","link":"","permalink":"http://godhearing.cn/2021/01/23/axios-huo-qu-wen-jian-liu-xing-shi-ru-he-zhan-shi/","excerpt":"","text":"前言 axios请求返回的是一张图片，这种情况下，我们可以直接将src定义为请求的地址，但是，如果有请求头等其他信息，这个方法就非常的鸡肋了，所以，我们还是需要搞一下 URL.createObjectURL() 话不多说，我们直接上代码 this.axios({ url:'http://127.0.0.1:8000/sss/', method:'get', responseType:'arraybuffer' }).then(res => { return 'data:image/png;base64,' + btoa( new Uint8Array(res) .reduce((data, byte) => data + String.fromCharCode(byte), '') ); }).then(data => { this.image = data //图片地址 }) 要注意，respnseType是必须的，如果不加，会出不来 我在这里自己定义了一个image，src属性绑定的就是他，这样，我们就获取到了文件流的同时，把图片预览出来。","categories":[{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/categories/axios/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"},{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/tags/axios/"}],"author":"天听"},{"title":"Go语言基础","slug":"go","date":"2021-01-21T16:00:00.000Z","updated":"2021-01-21T16:00:00.000Z","comments":true,"path":"2021/01/22/go/","link":"","permalink":"http://godhearing.cn/2021/01/22/go/","excerpt":"","text":"前言 在一次偶然中，接触到了Go语言，这门语言是一个非常强大的编译型语言，从我接触起，便一直在记录，但愿这些能够帮助到想学习Go语言的人。 博客 文档 Go语言圣经 1.编译使用go build ​ 1.在项目目录下执行go build ​ 2.在其他路径下执行go build，需要在后面加上项目的路径(项目路径从GOPATH/src后开始写起，编译之后的可执行文件就保存在当前目录下) ​ 3.go build -o ***.exe生成编译文件时执行名字 go run像执行脚本文件一样执行go代码 go installgo install分为两步： ​ 1.先编译得到一个可执行文件 ​ 2.将可执行文件拷贝到GOPATH/bin 交叉编译Go支持跨平台编译 例如：在windows平添编译一个能在linux平台执行的可执行文件 SET CGO_ENABLED=0 //禁用CGO SET GOOS=linux //目标平台是linux SET GOARCH=amd64 //目标处理器架构是amd64 如果要编译可执行文件，必须要有main包和main函数(入口函数，无参数无返回值) 2.变量和常量package main //导入语句 import \"fmt\" //函数外只能放标识符(变量，常量，函数，类型)的声明 //程序的入口函数 func main() { fmt.Println(\"hello world\") } 标识符与关键字在编程语言中标识符就是程序员定义的具有特殊意义的词，比如变量名、常量名、函数名等等。 Go语言中标识符由字母数字和_(下划线）组成，并且只能以字母和_开头。 举几个例子：abc, _, _123, a123。 如果标识符的首字母是大写的，就表示对外部可见,可以通过包导入 关键字 break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var nil类似python的null 变量Go语言中的变量必须先声明再使用 注意，函数外的每个语句都必须以关键字开始(var，const，func等) 声明的变量必须使用，不使用就无法编译 同一个作用域中，不能重复声明同一个变量 var s1 string ：声明了一个s1变量为字符串类型 批量声明: var( a string b int c bool ) 输入fmt.Scan()代表用户输入,输入需要传指针来修改内存地址上的数据 var s string fmt.Scan(&amp;s) fmt.Println(\"用户输入的内容：\", s) fmt.Sanf格式化输入 var s string var d string fmt.Scanf(&amp;s &amp;d) fmt.Println(\"用户输入的内容：\", s d) 输出Printf(“name:%v”, “天听”)无论是什么类型，都能用%v来打印数值 func main() { fmt.Print(\"AAA\") //输出打印内容 fmt.Println(\"BBB\") //打印完内容之后会加一个换行符 fmt.Printf(\"name:%s\", \"天听\") //%s:占位符，等于格式化输出 } 变量类型占位符%T可以查看类型 fmt.Printf(\"%T\\n\", 111) fmt.Printf(\"%d\\n\", i1) fmt.Printf(\"%b\\n\", i1) //转换为二进制 fmt.Printf(\"%o\\n\", i1) // 转换为八进制 fmt.Printf(\"%x\\n\", i1) //转换为十六进制 // %d 十进制 // %c 字符 // %s 字符串 // %p 指针 // %v 值 // %f 浮点数 // %t 布尔值 %%代表的才是%，不能用\\转义 类型推导有时候我们会将变量的类型省略，这个时候编译器会根据等号右边的值来推导变量的类型完成初始化 var name = \"Q1mi\" var age = 18 //根据值判断该变量是什么类型 短变量声明在函数内部，可以使用更简略的 := 方式声明并初始化变量。 package main import ( \"fmt\" ) // 全局变量m var m = 100 func main() { n := 10 m := 200 // 此处声明局部变量m fmt.Println(m, n) } 匿名变量在使用多重赋值时，如果想要忽略某个值，可以使用匿名变量（anonymous variable）。 匿名变量用一个下划线_表示，(_多用于占位，表示忽略值。)例如： func foo() (int, string) { return 10, \"Q1mi\" } func main() { x, _ := foo() _, y := foo() fmt.Println(\"x=\", x) fmt.Println(\"y=\", y) } 常量 const声明常量的关键字const const PI = 3.1415926 也可以批量声明常量，和批量声明变量一致，只不过关键字从var变为const 批量声明常量时，如果某一行声明后没有赋值，默认就和上一行一样 const( n1 = 100 n2 n3 ) iotaiota是go语言的常量计数器，只能在常量的表达式中使用。 iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)。 使用iota能简化定义，在定义枚举时很有用 const ( n1 = iota //0 n2 //1 n3 //2 n4 //3 ) 使用_跳过某些值 const ( n1 = iota //0 n2 //1 _ n4 //3 ) iota声明中间插队 const ( n1 = iota //0 n2 = 100 //100 n3 = iota //2 n4 //3 ) const n5 = iota //0 定义数量级 （这里的&lt;&lt;表示左移操作，1&lt;&lt;10表示将1的二进制表示向左移10位，也就是由1变成了10000000000，也就是十进制的1024。同理2&lt;&lt;2表示将2的二进制表示向左移2位，也就是由10变成了1000，也就是十进制的8。） const ( _ = iota KB = 1 &lt;&lt; (10 * iota) MB = 1 &lt;&lt; (10 * iota) GB = 1 &lt;&lt; (10 * iota) TB = 1 &lt;&lt; (10 * iota) PB = 1 &lt;&lt; (10 * iota) ) 多个iota定义在一行 const ( a, b = iota + 1, iota + 2 //1,2 c, d //2,3 e, f //3,4 ) 3.数据类型整型整型分为以下两个大类： 按长度分为：int8、int16、int32、int64 对应的无符号整型：uint8、uint16、uint32、uint64 其中，uint8就是我们熟知的byte型 类型 描述 uint8 无符号 8位整型 (0 到 255) uint16 无符号 16位整型 (0 到 65535) uint32 无符号 32位整型 (0 到 4294967295) uint64 无符号 64位整型 (0 到 18446744073709551615) int8 有符号 8位整型 (-128 到 127) int16 有符号 16位整型 (-32768 到 32767) int32 有符号 32位整型 (-2147483648 到 2147483647) int64 有符号 64位整型 (-9223372036854775808 到 9223372036854775807) 特殊整型 类型 描述 uint 32位操作系统上就是uint32，64位操作系统上就是uint64 int 32位操作系统上就是int32，64位操作系统上就是int64 uintptr 无符号整型，用于存放一个指针 注意： 在使用int和 uint类型时，不能假定它是32位或64位的整型，而是考虑int和uint可能在不同平台上的差异。 注意事项 获取对象的长度的内建len()函数返回的长度可以根据不同平台的字节长度进行变化。实际使用中，切片或 map 的元素数量等都可以用int来表示。在涉及到二进制传输、读写文件的结构描述时，为了保持文件的结构不会受到不同编译目标平台字节长度的影响，不要使用int和 uint。 浮点型Go语言支持两种浮点型数：float32和float64 打印浮点数时，可以使用fmt包配合动词%f，代码如下： package main import ( \"fmt\" \"math\" ) func main() { fmt.Printf(\"%f\\n\", math.Pi) fmt.Printf(\"%.2f\\n\", math.Pi) } 布尔Go语言中以bool类型进行声明布尔型数据，布尔型数据只有true（真）和false（假）两个值。 注意： 布尔类型变量的默认值为false。 Go 语言中不允许将整型强制转换为布尔型. 布尔型无法参与数值运算，也无法与其他类型进行转换。 字符串Go语言中字符串是用双引号包裹的 单引号包裹的是字符，字符是单独的字母、汉字、符号 字符串操作转义： \\代表转义，如果单纯的想打印一个\\，需要在前面再加一个\\ 转义符 含义 \\r 回车符（返回行首） \\n 换行符（直接跳到下一行的同列位置） \\t 制表符 \\' 单引号 \\\" 双引号 \\\\ 反斜杠 多行字符串: `，使用此符号时，无需用\\转义 s1 := ` aaa bbb ccc ` 方法 介绍 len(str) 求长度 +或fmt.Sprintf 拼接字符串 strings.Split 分割 strings.contains 判断是否包含 strings.HasPrefix,strings.HasSuffix 前缀/后缀判断 strings.Index(),strings.LastIndex() 子串出现的位置(下标) strings.Join(a[]string, sep string) join操作 byte和rune类型 组成每个字符串的元素叫做“字符”，可以通过遍历或者单个获取字符串元素获得字符。 字符用单引号（’）包裹起来，如： var a := '中' var b := 'x' 当需要处理中文、日文或者其他复合字符时，则需要用到rune类型。rune类型实际是一个int32 修改字符串： 要修改字符串，需要先将其转换成[]rune或[]byte，完成后再转换为string。无论哪种转换，都会重新分配内存，并复制字节数组。 func changeString() { s1 := \"big\" // 强制类型转换 byteS1 := []byte(s1) byteS1[0] = 'p' fmt.Println(string(byteS1)) s2 := \"白萝卜\" runeS2 := []rune(s2) runeS2[0] = '红' fmt.Println(string(runeS2)) } 类型转换： Go语言中只有强制类型转换，没有隐式类型转换。该语法只能在两个类型之间支持相互转换的时候使用。 强制类型转换的基本语法如下： T(表达式) 其中，T表示要转换的类型。表达式包括变量、复杂算子和函数返回值等 数组数组的长度是数组类型的一部分，例如var al [3]bool，代表了al是一个长度为3元素类型为布尔的数组 数组的初始化var a1 [3]bool //初始化 a1 = [3]bool{true, true, false} //根据初始值自动推断数组的长度是多少 a2 := [...]int{1, 2, 3, 4, 5, 6, 7} fmt.Println(a2) //根据索引初始化 a3 := [5]int{0:2,4:3} fmt.Println(a3) 数组的遍历citys := [...]string{\"北京\", \"上海\", \"深圳\"} for i := 0; i &lt; len(citys); i++ { fmt.Println(citys[i]) } //range遍历 for i, v := range citys { fmt.Println(i, v) } 多维数组 多维数组就是数组的嵌套，如[[1,2],[2,3],[3,4]] var a4 [3][2]int //[3]是外层数组，[2]int是该数组的数据类型，元素为两个int a4 = [3][2]int{ [2]int{1, 2}, [2]int{3, 4}, [2]int{5, 6}, } fmt.Println(a4) 多维数组的遍历var a4 [3][2]int a4 = [3][2]int{ [2]int{1, 2}, [2]int{3, 4}, [2]int{5, 6}, } for _, v := range a4 { for _, i := range v { fmt.Println(i) } } 4.if与forif判断age := 16 if age >= 18 { fmt.Println(\"成年\") } else { fmt.Println(\"未成年\") } 也可以将变量定义到if的作用域中,这样，该变量只会在if作用域中 if age := 16; age >= 18 { fmt.Println(\"成年\") } else { fmt.Println(\"未成年\") } else if if age := 16; age >= 18 { fmt.Println(\"成年\") } else if age >= 16 { fmt.Println(\"小成年\") } else { fmt.Println(\"未成年\") } for循环基本格式 for 初始语句;条件;结束语句{} for i := 0; i &lt; 10; i++ { fmt.Println(i) } 省略初始语句的写法 **;**不能少 var i=5 for ;i&lt;10;i++{ fmt.Println(i) } 省略结束语句的写法,如果不加条件，则是无限循环，由于go语言运行速度极快，不要轻易尝试 var i=5 for ;i&lt;10;{ fmt.Println(i) i++ } for range循环，有两个变量接收，第一个是索引，第二个是值 s:= \"hello天听\" for i,v := range s{ fmt.Printf(\"%d,%c\\n\",i,v) } break：跳出for循环 contiune：跳过本次循环，继续下一次 switch简化大量的判断 var a = 3 switch a { case 1: fmt.Println(\"1\") case 2: fmt.Println(\"2\") case 3: fmt.Println(\"3\") case 4: fmt.Println(\"4\") default: fmt.Println(\"无效\") } 5.运算符Go语言是强类型,相同类型的变量才能比较 算数运算符 运算符 描述 + 相加 - 相减 * 相乘 / 相除 % 求余 关系运算符 运算符 描述 == 检查两个值是否相等，如果相等返回 True 否则返回 False。 != 检查两个值是否不相等，如果不相等返回 True 否则返回 False。 &gt; 检查左边值是否大于右边值，如果是返回 True 否则返回 False。 &gt;= 检查左边值是否大于等于右边值，如果是返回 True 否则返回 False。 &lt; 检查左边值是否小于右边值，如果是返回 True 否则返回 False。 &lt;= 检查左边值是否小于等于右边值，如果是返回 True 否则返回 False。 逻辑运算符 运算符 描述 || 逻辑 AND 运算符。 如果两边的操作数都是 True，则为 True，否则为 False。 &amp;&amp; 逻辑 OR 运算符。 如果两边的操作数有一个 True，则为 True，否则为 False。 ! 逻辑 NOT 运算符。 如果条件为 True，则为 False，否则为 True。 位运算符位运算符对整数在内存中的二进制位进行操作。 运算符 描述 &amp; 参与运算的两数各对应的二进位相与。 （两位均为1才为1） | 参与运算的两数各对应的二进位相或。 （两位有一个为1就为1） ^ 参与运算的两数各对应的二进位相异或，当两对应的二进位相异时，结果为1。 （两位不一样则为1） &lt;&lt; 左移n位就是乘以2的n次方。 “a&lt;&lt;b”是把a的各二进位全部左移b位，高位丢弃，低位补0。 &gt;&gt; 右移n位就是除以2的n次方。 “a&gt;&gt;b”是把a的各二进位全部右移b位。 赋值运算符 = 简单的赋值运算符，将一个表达式的值赋给一个左值 += 相加后再赋值 -= 相减后再赋值 *= 相乘后再赋值 /= 相除后再赋值 %= 求余后再赋值 &lt;&lt;= 左移后赋值 &gt;&gt;= 右移后赋值 &amp;= 按位与后赋值 |= 按位或后赋值 ^= 按位异或后赋值 6.切片len代表长度 cap代表容量 切片必须初始化分配内存 切片不存值，指向同一个底层数组，所以，怎样赋值切片之后改变，都会使同样的底层数组发生改变，类似浅拷贝 自定义切片var s1 []int //定义一个存放int类型元素的切片 //初始化 s1 = []int{1,2,3} 由数组得到切片a1 :=[...]int{1,2,3,4,5} s3 := a1[0:4] make函数创建切片 make([]int,元素数量,容量) s1 := make([]int, 5,10) fmt.Printf(\"s1=%v len(s1)=%d cap(s1)=%d\\n\", s1, len(s1), cap(s1)) 切片的本质就是一个框，框住了一块连续的内存，属于引用类型，真正的数据都保存在底层数组里 切片不能直接比较，只能和nil比较 要检查切片是否为空，请始终使用len(s) == 0来判断，而不应该使用s == nil来判断。 append函数添加 调用append函数必须使用原来的切片变量接收返回值 s1 := []string{\"龙潭\", \"四海门\", \"空境\"} s1 = append(s1, \"洞天\") fmt.Println(s1) //多个元素添加，用...进行拆开 s1 := []string{\"龙潭\", \"四海门\", \"空境\"} s2 := []string{\"望苍城\", \"天下城\", \"万象城\"} s1 = append(s1, s2...) fmt.Println(s1) 切片的扩容策略首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap）。 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍，即（newcap=doublecap）， 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的1/4，即（newcap=old.cap,for {newcap += newcap/4}）直到最终容量（newcap）大于等于新申请的容量(cap)，即（newcap >= cap） 如果最终容量（cap）计算值溢出，则最终容量（cap）就是新申请容量（cap）。 copy //此处，将s1的值拷贝给了s3，深拷贝，此时s1如何修改都与s3无关 s1 := []string{\"龙潭\", \"四海门\", \"空境\"} s2 := s1 var s3 = make([]string, 3, 3) fmt.Println(s1, s2, s3) copy(s3, s1) fmt.Println(s1, s2, s3) 从切片中删除元素// 从切片中删除元素 a := []int{30, 31, 32, 33, 34, 35, 36, 37} // 要删除索引为2的元素 a = append(a[:2], a[3:]...) fmt.Println(a) //[30 31 33 34 35 36 37] 7.指针在Go语言中对于引用类型的变量，在使用的时候不仅要声明它，还要为它分配内存空间，否则值就没办法存储。而对于值类型的声明不需要分配内存空间，是因为它们在声明的时候已经默认分配好了内存空间。Go语言中new和make是内建的两个函数，主要用来分配内存。 go语言中不存在指针操作，只需要记住两个符号 &amp;：取地址 *：根据地址取值 对变量进行取地址（&amp;）操作，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值（*）操作，可以获得指针变量指向的原变量的值。 //&amp; a := 20 fmt.Println(&amp;a) //* p := &amp;a m := *p fmt.Println(m) new函数new函数是一个内置的函数，可以申请一个内存地址 new很少用，一般用来给基本数据类型申请内存，int/string等 var a *int //声明一个变量 fmt.Println(a) var a2 = new(int) // new函数申请内存地址 fmt.Println(*a2) *a2 = 100 //重新赋值 fmt.Println(*a2) 返回的是指针 make函数 make也是用于内存分配的，区别于new，它只用于slice、map以及chan的内存创建而且它返回的类型就是这三个类型本身，而不是他们的指针类型，因为这三种类型就是引用类型，所以就没有必要返回他们的指针。 8.map Go语言中提供的映射关系容器为map，其内部使用散列表（hash）实现。 map是一种无序的基于key-value的数据结构，Go语言中的map是引用类型，必须初始化才能使用。 map[键类型]值类型 var a map[string]int // 声明一个map a = make(map[string]int, 10) //初始化 a[\"天听\"] = 20//添加数据 a[\"并轩\"] = 30 a[\"冰冰\"] = 16 fmt.Println(a) v, ok := a[\"冰冰\"] //返回两个值，一个是数据，一个是布尔值，约定成俗是ok if !ok { //如果ok为false，则查找不到 fmt.Println(\"查无此人\") } else { fmt.Println(v) } map的遍历for k, v := range a { fmt.Println(k, v) } 如果只用一个参数接收，默认接收k for k := range a { fmt.Println(k) } 如果只是单纯的想拿到值，可以用_接收k for _, v := range a { fmt.Println(v) } 删除 用delete删除 delete(a,键) 9.函数函数是一段代码的封装 在go语言中，函数格式为func 函数名(参数)(返回值){} 并且要指定参数类型和返回值类型 func sum(x int, y int) (ret int) { return x * y } func main() { a := sum(2, 55) fmt.Println(a) } 无参数有返回值 func a1() string { return \"jajajaj\" } 返回值可以命名也可以不命名，但是，需要声明返回值 func a2(x int,y int)(ret int){ //声明ret ret = x+y //直接可以使用 } //也可以不命名返回值 func a2(x int,y int) int{ ret := x+y //声明 return ret //使用 } 返回值可以有多个，需要用()括起来 func a2(x int,y int)(int,string){ return 1,\"天听\" } 参数类型简写 当参数中，连续两个参数类型一致，可以将前边的参数类型省略 func a2(x,y int)(ret int){ ret = x+y } 可变长参数 func a2(x string,y ...int){ //...int，可变长，但是，都得是int类型 fmt.Println(x) fmt.Println(y) } 注意，go语言中函数传参永远是深拷贝 defer语句 defer语句会将其后边跟随的语句进行延迟处理，在defer归属的函数即将返回时，将延迟处理的语句按defer定义的逆序进行执行 也就是说，有多个defer语句时，最先写的defer最后被执行，最后写的defer最先被执行 defer多用于函数结束之前释放资源 func deferDemo() { fmt.Println(\"start\") defer fmt.Println(\"AAA\") defer fmt.Println(\"BBB\") defer fmt.Println(\"CCC\") fmt.Println(\"end\") } /* 执行结果为 start end CCC BBB AAA */ Go语言中的return不是原子操作，在底层是分为两步来执行 第一步：返回值赋值 第二步：真正的RET返回 函数中如果存在defer，那么defer执行的时机是在第一步和第二步之间 func f1() (x int) { x = 5 //返回值赋值 defer func() { x++ //执行defer }() return x //RET返回 } func f1() int { //此时的返回值没有命名 x := 5 defer func() { //defer修改的是x而不是返回值 x++ }() return x } 函数也可以作为参数和返回值 只要是满足参数和返回值的类型要求 func f1(){ //类型为func() } func f2() int{ // 类型为func()int return 100 } func f3(x func()){ //此处可以将类型为func()的函数传进来 } func f4(x func()int){//此处可以将类型为func()int的函数传进来 } 匿名函数 因为函数内部无法声明有名字的函数，所以，在匿名函数多用于在函数内部 匿名函数定义：func(){} func main(){ func(x,y int){ fmt.Println(x+y) }(1,2)//如果只是调用一次的函数，可以简写成立即执行函数，加个括号 } 闭包 闭包的本质就是一个函数，函数可以作为返回值，因为函数内部查找变量的顺序是由内而外，所以，在自己内部找不到变量就得去外层找，包含了一个外部作用域变量的特殊函数 func f1(a int) (func(int) int, func(int) int) { //声明一个函数，有参数a，两个有参有返的函数返回值 add := func(i int) int { a += i return a } //内部声明匿名函数 sub := func(i int) int { a -= i return a } //内部声明匿名函数 return add, sub //满足了条件 } func main() { q1, q2 := f1(10) // q1,q2的类型为func(int) int // 参数a是10 fmt.Println(q1(1), q2(2)) //11 9 } 上面的例子中为什么不是8而是9，因为传的参数a并不是内部匿名函数的参数，所以，按照返回值的顺序，先执行了add，再执行sub 10.作用域 和python相同点，全局作用域和局部作用域 不同点为，Go语言的if…else，switch..case等都会产生语句块作用域 11.内置函数 内置函数 介绍 close 主要用来关闭channel len 用来求长度，比如string、array、slice、map、channel new 用来分配内存，主要用来分配值类型，比如int、struct。返回的是指针 make 用来分配内存，主要用来分配引用类型，比如chan、map、slice append 用来追加元素到数组、slice中 panic和recover 用来做错误处理 panic/recover Go语言中目前是没有异常机制，但是使用panic/recover模式来处理错误。 panic可以在任何地方引发，但recover只有在defer调用的函数中有效 func f2() { defer func() { err := recover() fmt.Println(err) fmt.Println(\"关闭程序\") }() fmt.Println(\"B\") panic(\"错误\") } 注意，recover一定要搭配defer使用 12.自定义类型和类型别名自定义类型type 自定义类型 内置类型 type ssint int 类型别名type 别名 = 内置类型 type yourInt = int 13.结构体 结构体的内存地址是连续的，并不是每个字段一个内存 Go语言中没有“类”的概念，也不支持“类”的继承等面向对象的概念。Go语言中通过结构体的内嵌再配合接口比面向对象具有更高的扩展性和灵活性。 Go语言中通过struct来实现面向对象 使用type和struct关键字来定义结构体 type 类型名 struct{ 字段名 字段类型 字段名 字段类型 } type person struct{ name string age int gender string } //实例化\"对象\" var a person a.name = \"天听\" a.age = 18 a.gender = \"男\" 匿名结构体var s struct{ name string age int } 和定义结构体不同的是，匿名结构体是在声明变量，而不是声明结构体类型 结构体指针 结构体可以当做值传进函数内，通过指针，也可以将某个值的内存地址传过去 func f1(x person) { //此时传的是结构体 x.gender = \"女\" } func f2(x *person) { // 此时，传的是结构体的指针 (*x).gender = \"女\" //x.gender = \"女\" //两种写法都可以 } func main() { var a person a.name = \"天听\" a.gender = \"男\" f1(a) fmt.Println(a.gender) //由于传过去的是深拷贝，所以，修改不会成功 f2(&amp;a) fmt.Println(a.gender)//通过指针修改了内存地址上的值，所以，修改成功了 } 结构体初始化var b = persson{ name: \"并轩\", gender: \"女\", } fmt.Println(b) var c = persson{ \"狮子\", \"男\", } fmt.Println(c) 注意，以上两种方法不能混用 当结构体比较大的时候尽量使用结构体指针，减少程序的内存开销 构造函数 返回一个结构体变量的函数 func newDog(name string) dog { return dog{ name:name, } } 方法和接收者 方法是作用域特定类型的函数 接收者表示的是调用该方法的具体类型变量，多用类型名首字母小写表示，写在函数名前面 type dog struct{ name string } func (d dog)wang(){ fmt.Printf(\"%s:汪汪汪~\",d.name) } func main(){ d1 := newDog(\"哈士奇\") d1.wang() } 接收者func (接收者变量 接收者类型) 方法名(参数列表) (返回参数) { 函数体 } 什么时候需要用指针接收者： 需要修改接收者的值，需要用指针接收者 保证一致性，如果有某个方法使用了指针接收者，那么其他的方法也应该使用指针接收者 接收者是拷贝代价比较大的大对象 匿名字段 结构体中只有类型没有名字的字段，并且类型不能重复 当字段比较少也比较简单时使用 type person struct{ string int //string 此时再写string就会报错，因为类型不能重复 } 嵌套结构体 结构体嵌套结构体 type address struct{ //类型1 name string age int } type ren struct{ gender string addr address // 嵌套了类型1 } func main(){ f1 := ren{ gender:\"男\", addr:address{ //嵌套结构体构造数据 name:\"天听\", age:20, } } } 匿名嵌套结构体type address struct{ name string age int } type ren struct{ gender string address // 匿名嵌套 } func main(){ f1 := ren{ gender:\"男\", addr:address{ name:\"天听\", age:20, } } // fmt.Println(f1.addr.name) fmt.Println(f1.name) } 匿名嵌套结构体的好处是可以直接调用被嵌套结构体中的字段 但是，如果嵌套了两个结构体，而这两个结构体中有一样的字段，这种写法会有字段冲突，解决办法就是写全 结构体模拟“继承”//动物类 type animal struct { name string } //动物的方法 func (a animal) move() { fmt.Printf(\"%s会动\\n\", a.name) } //狗类，继承动物类 type dog struct { id uint8 animal } //狗类的方法 func (d dog) wang() { fmt.Printf(\"%s在叫\", d.name) } func main() { d1 := dog{ id: 1, animal: animal{name: \"哈士奇\"}, } d1.wang() d1.move() //动物类的方法，狗类同样也能使用 } 结构体与JSON 序列化：把go语言中的结构体变量转换为json格式的字符串 反序列化：json格式的字符串转换为go语言中能够识别的结构体变量 import \"encoding/json\" func main(){ f1 := person{ name:\"天听\", age:18, } b,err := json.Marshal(f1) } 14.接口关键字：interface 注意：接口是一种类型,他规定了数据有哪些方法 //造接口，只要是有speak方法的变量，他就是speaker类型的接口 type speaker interface { speak() } //造两个结构体 type dog struct{} type cat struct{} //各有一个speak方法 func (d dog) speak() { fmt.Println(\"汪汪汪\") } func (c cat) speak() { fmt.Println(\"喵喵喵\") } //只需要将接口传进来即可 func da(x speaker) { x.speak() } func main() { var c cat var d dog da(c) da(d) } 一个接口中可以有多个方法，但是需要全部实现才算是这个接口类型的变量，只实现其中的某个方法不算这个接口变量. 同一个结构体可以实现多个接口 使用值接收者和指针接收者实现接口区别： 1. 使用**值接收者**实现接口，结构体类型和结构体指针类型的变量都能存 2. **指针接收者**实现接口，只能存结构体指针类型的变量 空接口interface{} // 空接口 所有的类型都实现了空接口，也就是任意类型的变量都能保存在空接口中 func main(){ var m1 map[string]interface{}//注意加括号，否则只是关键字 m1 = make(map[string]interface{}) m1[\"name\"] = \"天听\" m1[\"age\"] = 18 } 在上面的例子中，既可以保存字符串，又能保存整型 类型断言x.(T) x:表示类型为interface{}的变量 T:表示断言，x可能是的类型 func ss(a interface{}){ str,ok := a.(string) //类型断言 if !ok{ fmt.Println(\"猜错了\") }else{ fmt.Println(\"字符串\") } } func ss(a interface{}){ switch t := a.(type){ case string: fmt.Println(\"字符串\") case int: fmt.Println(\"整型\") case bool: fmt.Println(\"布尔\") } } 15.包包中的标识符首字母大写才可以对外部可见 根据自己的需要创建自己的包。一个包可以简单理解为一个存放.go文件的文件夹。 该文件夹下面的所有go文件都要在代码的第一行添加如下代码，声明该文件归属的包。 package 包名 //包的定义 一个文件夹下面直接包含的文件只能归属一个package，同样一个package的文件不能在多个文件夹下。 包名可以不和文件夹的名字一样，包名不能包含 - 符号。 包名为main的包为应用程序的入口包，这种包编译后会得到一个可执行文件，而编译不包含main包的源代码则不会得到可执行文件。 要在代码中引用其他包的内容，需要使用import关键字导入使用的包 import \"包的路径\" // 包的导入 注意事项： import导入语句通常放在文件开头包声明语句的下面。 导入的包名需要使用双引号包裹起来。 包名是从$GOPATH/src/后开始计算的，使用/进行路径分隔。 Go语言中禁止循环导入包。 匿名导入包 如果只希望导入包，而不使用包内部的数据时，可以使用匿名导入包 import _ \"包的路径\" 匿名导入的包与其他方式导入的包一样都会被编译到可执行文件中 init初始化函数 在Go语言程序执行时导入包语句会自动触发包内部init()函数的调用。需要注意的是： init()函数没有参数也没有返回值。 init()函数在程序运行时自动被调用执行，不能在代码中主动调用它 16.文件操作 os.Open()函数能够打开一个文件，返回一个*File和一个err。对得到的文件实例调用，close()方法能够关闭文件。 为了防止文件忘记关闭，通常使用defer注册文件关闭语句 读取文件//打开文件，返回两个参数 fileobj, err := os.Open(\"./main.go\") if err != nil { fmt.Println(\"错误为:\", err) return } defer fileobj.Close() var tmp = make([]byte, 128) //var tmp = [128]byte for { n, err := fileobj.Read(tmp[:]) if err != nil { fmt.Println(\"读取错误为：\", err) return } fmt.Println(\"读取的字节数为：\", n) fmt.Println(string(tmp[:n])) if n &lt; 128 { return } } file.Read() 它接收一个字节切片，返回读取的字节数和可能的具体错误，读到文件末尾时会返回0和io.EOF func (f *File) Read(b []byte) (n int, err error) //Read方法定义 bufio bufio是在file的基础上封装了一层API，支持更多的功能 fileobj, err := os.Open(\"./main.go\") if err != nil { fmt.Println(\"错误为：\", err) return } defer fileobj.Close() //创建一个用来从文件中读取内容的对象 reader := bufio.NewReader(fileobj) for { line, err := reader.ReadString('\\n') if err == io.EOF { return } if err != nil { fmt.Println(\"！！！！\", err) return } fmt.Print(line) } ioutilio/ioutil包的ReadFile方法能够读取完整的文件，只需要将文件名作为参数传入 content, err := ioutil.ReadFile(\"./main.go\") if err != nil { fmt.Println(\"read file failed, err:\", err) return } fmt.Println(string(content)) 文件写入os.OpenFile()函数能够以指定模式打开文件，从而实现文件写入相关功能 name：要打开的文件名 flag：打开文件的模式： 模式 含义 os.O_WRONLY 只写 os.O_CREATE 创建文件 os.O_RDONLY 只读 os.O_RDWR 读写 os.O_TRUNC 清空 os.O_APPEND 追加 Write 写入字节切片数据 WriteString 直接写入字符串数据 fileobj, err := os.OpenFile(\"./ss.txt\", os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0644) if err != nil { fmt.Println(err) return } fileobj.Write([]byte(\"执子之魂，与子共生\\n\")) fileobj.WriteString(\"那些逃离死亡的人，其生命，早已停滞不前\") defer fileobj.Close() bufio.NewWriterfileobj, err := os.OpenFile(\"./ss.txt\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0644) if err != nil { fmt.Println(err) return } wr := bufio.NewWriter(fileobj) //将数据先写入缓存 wr.WriteString(\"明天，只是一个希望，不是一个承诺\") //将缓存中的内容写入文件 wr.Flush() defer fileobj.Close() ioutil.WriteFilestr := \"hello 龙潭\" err := ioutil.WriteFile(\"./xx.txt\", []byte(str), 0666) if err != nil { fmt.Println(\"write file failed, err:\", err) return } 17.时间模块time包 time包是时间模块 当前时间 func main(){ now := time.Now() fmt.Println(now) fmt.Println(now.Year()) } 时间戳 Unix() func main(){ now := time.Now() fmt.Println(now.Unix()) //时间戳 fmt.Println(now.UnixNano()) //纳秒时间戳 } 时间间隔time.Duration是time包定义的一个类型，它代表两个时间点之间经过的时间，以纳秒为单位。 time包中定义的时间间隔类型的常量如下： const ( Nanosecond Duration = 1 Microsecond = 1000 * Nanosecond Millisecond = 1000 * Microsecond Second = 1000 * Millisecond Minute = 60 * Second Hour = 60 * Minute ) time.Duration表示1纳秒，time.Second表示1秒。 时间操作 在日常编码可能会遇到要求时间+时间间隔需求，这时，需要用到Add fmt.Println(time.Second) //当前时间+24小时 fmt.Println(now.Add(24*time.Hour)) 时间格式化 通俗的讲，就是把语言中的时间对象转换成字符串类型的时间 格式化时间模板不是常见的Y-m-d H:M:S而是使用Go的诞生时间2006年1月2号15点04分（记忆口诀为2006 1 2 3 4） now.Format // 注意，格式化是从2006-01-02开始，这个值不可变 fmt.Println(now.Format(\"2006-01-02\")) fmt.Println(now.Format(\"2006-01-02 15:04:05\")) Parse 按照对应的格式解析字符串类型的时间 time.Parse(\"2006-01-02\",\"2020-8-20\") 定时器 使用time.Tick(时间间隔)来设置定时器，定时器的本质上是一个通道（channel） func tickDemo() { ticker := time.Tick(time.Second) //定义一个1秒间隔的定时器 for i := range ticker { fmt.Println(i)//每秒都会执行的任务 } } 18.log日志服务 Go语言内置的log包实现了简单的日志服务。 log.Println(\"打印的日志\") //输入结果 //2020/08/20 16:28:50 打印的日志 log.SetOutput()设置输出位置 19.反射 反射是指在程序运行期对程序本身进行访问和修改的能力。程序在编译时，变量被转换为内存地址，变量名不会被编译器写入到可执行部分。在运行程序时，程序无法获取自身的信息。 反射类似于ORM reflect 在Go语言的反射机制中，任何接口值都由是一个具体类型和具体类型的值两部分组成 在Go语言中反射的相关功能由内置的reflect包提供，任意接口值在反射中都可以理解为由reflect.Type和reflect.Value两部分组成，并且reflect包提供了reflect.TypeOf和reflect.ValueOf两个函数来获取任意对象的Value和Type。 注意，无论是TypeOf还是ValueOf，都有一个kind方法，要注意。 type name 和 type kind在反射中类型具体划分为两种，一种是类型(Type)， 一种是种类(Kind) type Cat struct{ Name string } 一个结构体，此时，我们如果要打印他的类型的话，他是一个Cat类型，但是，同时他也属于struct，struct就属于他的种类(Kind)，Cat就是他的类型(Type)， 20.高并发 都知道，golang天生支持高并发，语言特性使得他是一个并发的杀器。 GoroutineGo语言的并发通过goroutine实现。goroutine类似于线程，属于用户态的线程，我们可以根据需要创建成千上万个goroutine并发工作。goroutine是由Go语言的运行时（runtime）调度完成，而线程是由操作系统调度完成。 Go语言还提供channel在多个goroutine间进行通信。goroutine和channel是 Go 语言秉承的 CSP（Communicating Sequential Process）并发模式的重要实现基础。 在Go语言编程中你不需要去自己写进程、线程、协程，你的技能包里只有一个技能–goroutine，当你需要让某个任务并发执行的时候，你只需要把这个任务包装成一个函数，开启一个goroutine去执行这个函数就可以了，就是这么简单粗暴。 Go语言中使用goroutine非常简单，只需要在调用函数的时候在前面加上go关键字，就可以为一个函数创建一个goroutine。 一个goroutine必定对应一个函数，可以创建多个goroutine去执行相同的函数。 举个例子： package main import ( \"fmt\" \"time\" ) func newTask() { i := 0 for { i++ fmt.Println(\"newTask\",i) time.Sleep(1 * time.Second) } } func main() { go newTask() i := 0 for { i++ fmt.Println(\"main\", i) time.Sleep(1 * time.Second) } } 在程序启动时，Go程序就会为main()函数创建一个默认的goroutine。 当main()函数返回的时候该goroutine就结束了，所有在main()函数中启动的goroutine会一同结束，main函数所在的goroutine就像是权利的游戏中的夜王，其他的goroutine都是异鬼，夜王一死它转化的那些异鬼也就全部GG了。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"}],"author":"天听"},{"title":"微信小程序用户认证逻辑","slug":"微信小程序用户认证逻辑","date":"2021-01-20T16:00:00.000Z","updated":"2021-01-20T16:00:00.000Z","comments":true,"path":"2021/01/21/wei-xin-xiao-cheng-xu-yong-hu-ren-zheng-luo-ji/","link":"","permalink":"http://godhearing.cn/2021/01/21/wei-xin-xiao-cheng-xu-yong-hu-ren-zheng-luo-ji/","excerpt":"","text":"调用 wx.login()获取临时登录凭证code，并回传到开发者服务器。 调用code2Session接口，换取用户唯一标识 OpenID和会话密钥 session_key。 之后开发者服务器可以根据用户标识来生成自定义登录态，用于后续业务逻辑中前后端交互时识别用户身份。 注意： 会话密钥session_key是对用户数据进行加密签名的密钥。为了应用自身的数据安全，开发者服务器不应该把会话密钥下发到小程序，也不应该对外提供这个密钥。 临时登录凭证 code 只能使用一次 总结:小程序端执行wx.login后在回调函数中就能拿到上图的code,然后把这个code传给我们后端程序，后端拿到这个这个code后，可以请求code2Session接口拿到用的openid和session_key,openid是用户在微信中唯一标识，我们就可以把这个两个值(val)存起来，然后返回一个键（key）给小程序端，下次小程序请求我们后端的时候，带上这个key，我们就能找到这个val,就可以，这样就把登入做好了。 wx.login 调用接口获取登录凭证（code）。通过凭证进而换取用户登录态信息，包括用户的唯一标识（openid）及本次登录的会话密钥（session_key）等。用户数据的加解密通讯需要依赖会话密钥完成。[/code] 参数 属性 类型 默认值 必填 说明 最低版本 timeout number 否 超时时间，单位ms 1.9.90 success function 否 接口调用成功的回调函数 fail function 否 接口调用失败的回调函数 complete function 否 接口调用结束的回调函数（调用成功、失败都会执行） object.success 回调函数 参数 属性 类型 说明 code string 用户登录凭证（有效期五分钟）。开发者需要在开发者服务器后台调用 code2Session，使用 code 换取 openid 和 session_key 等信息 code2Session 本接口应在服务器端调用，详细说明参见服务端API。 登录凭证校验。通过wx.login()接口获得临时登录凭证 code 后传到开发者服务器调用此接口完成登录流程。更多使用方法详见小程序登录。 请求地址 GET https://api.weixin.qq.com/sns/jscode2sessionappid=APPID&amp;secret=SECRET&amp;js_code=JSCODE&amp;grant_type=authorization_code 请求参数 属性 类型 默认值 必填 说明 appid string 是 小程序 appId secret string 是 小程序 appSecret js_code string 是 登录时获取的 code grant_type string 是 授权类型，此处只需填写 authorization_code 返回值 Object 返回的 JSON 数据包 属性 类型 说明 openid string 用户唯一标识 session_key string 会话密钥 unionid string 用户在开放平台的唯一标识符，在满足 UnionID 下发条件的情况下会返回，详见 UnionID 机制说明。 errcode number 错误码 errmsg string 错误信息 errcode 的合法值 值 说明 -1 系统繁忙，此时请开发者稍候再试 0 请求成功 40029 code 无效 45011 频率限制，每个用户每分钟100次 二.信息授权wx.getUserInfo 获取用户信息。 参数 属性 类型 默认值 必填 说明 withCredentials boolean 否 是否带上登录态信息。当 withCredentials 为 true 时，要求此前有调用过 wx.login 且登录态尚未过期，此时返回的数据会包含 encryptedData, iv 等敏感信息；当 withCredentials 为 false 时，不要求有登录态，返回的数据不包含 encryptedData, iv 等敏感信息。 lang string en 否 显示用户信息的语言 success function 否 接口调用成功的回调函数 fail function 否 接口调用失败的回调函数 complete function 否 接口调用结束的回调函数（调用成功、失败都会执行） object.lang 的合法值 值 说明 en 英文 zh_CN 简体中文 zh_TW 繁体中文 object.success 回调函数 参数 属性 类型 说明 userInfo UserInfo 用户信息对象，不包含 openid 等敏感信息 rawData string 不包括敏感信息的原始数据字符串，用于计算签名 signature string 使用 sha1( rawData + sessionkey ) 得到字符串，用于校验用户信息，详见 用户数据的签名验证和加解密 encryptedData string 包括敏感数据在内的完整用户信息的加密数据，详见 用户数据的签名验证和加解密 iv string 加密算法的初始向量，详见 用户数据的签名验证和加解密 注意: 1.小程序端获取授权信息要用button按钮触发 2.小程序端需要将 encryptedData, iv, login_key 传到后端用于解密 案例: 登录: 当小程序第一次执行的时候就调用wx.login 小程序端:apps.js App({ onLaunch: function () { var _this=this // 登录 wx.login({ success: res => { // 发送 res.code 到后台换取 openId, sessionKey, unionId wx.request({ url: _this.globalData.Url+'/login/', // 后端路径 data:{\"code\":res.code}, // code header:{\"content-type\":\"application/json\"}, method:\"POST\", success:function(res){ console.log(res) // 小程序端存储login_key wx.setStorageSync(\"login_key\",res.data.data.login_key) } }) } }) }, globalData: { Url:\"http://127.0.0.1:8000\", userInfo: null } }) 后端 django wx ├── settings.py # 小程序id,code2Session等配置 ├── wx_login.py # 用于调用code2Session拿到openid等 └── WXBizDataCrypt.py # 获取用户授权信息的解密算法,官方下载 微信官方解密算法代码 项目/settings.py # 配置数据库 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'wx', 'USER':'root', 'PASSWORD':'root', 'HOST':'127.0.0.1', 'PORT': 3306, 'OPTIONS': {'charset': 'utf8mb4'}, # 微信用户名可能有标签,所以用utf8mb4 } } # 配置 django-redis CACHES = { 'default': { 'BACKEND': 'django_redis.cache.RedisCache', 'LOCATION': 'redis://127.0.0.1:6379', \"OPTIONS\": { \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\", \"PASSWORD\": \"\", }, }, } wx/settings.py # 小程序开发者id AppId=\"...\" # 小程序的AppSecret AppSecret=\"...\" code2Session=\"https://api.weixin.qq.com/sns/jscode2session?appid={}&amp;secret={}&amp;js_code={}&amp;grant_type=authorization_code\" pay_mchid ='...' pay_apikey = '...' wx/wx_login.py from app01.wx import settings import requests # 调用微信code2Session接口,换取用户唯一标识 OpenID 和 会话密钥 session_key def login(code): response = requests.get(settings.code2Session.format(settings.AppId,settings.AppSecret,code)) data = response.json() if data.get(\"openid\"): return data else: return False 项目/views.py from rest_framework.views import APIView from rest_framework.response import Response from app01.wx import wx_login from django.core.cache import cache from app01 import models import time, hashlib class Login(APIView): def post(self, request): param = request.data # 拿到小程序端提交的code if param.get('code'): # 调用微信code2Session接口,换取用户唯一标识 OpenID 和 会话密钥 session_key data = wx_login.login(param.get('code')) if data: # 将openid 和 session_key拼接 val = data['openid'] + \"&amp;\" + data[\"session_key\"] key = data[\"openid\"] + str(int(time.time())) # 将 openid 加密 md5 = hashlib.md5() md5.update(key.encode(\"utf-8\")) key = md5.hexdigest() # 保存到redis内存库,因为小程序端后续需要认证的操作会需要频繁校验 cache.set(key, val) has_user = models.Wxuser.objects.filter(openid=data['openid']).first() # 用户不存在则创建用户 if not has_user: models.Wxuser.objects.create(openid=data['openid']) return Response({ \"code\": 200, \"msg\": \"ok\", \"data\": {\"login_key\": key} # 返回给小程序端 }) else: return Response({\"code\": 401, \"msg\": \"code无效\"}) else: return Response({\"code\": 401, \"msg\": \"缺少参数\"}) 用户信息授权 小程序端test.wxml &lt;!--用户信息授权--> &lt;button open-type=\"getUserInfo\" bindgetuserinfo=\"info\">授权登录&lt;/button> test.js Page({ info: function (res) { // console.log(res) wx.checkSession({ success() { //session_key 未过期，并且在本生命周期一直有效 wx.getUserInfo({ success: function (res) { // console.log(res) wx.request({ url: app.globalData.Url + \"/getinfo/\", data: { \"encryptedData\": res.encryptedData, \"iv\": res.iv, \"login_key\": wx.getStorageSync(\"login_key\") }, method: \"POST\", header: { \"content-type\": \"application/json\" }, success: function (res) { console.log(res) } }) } }) }) 后端 django wx/WXBizDataCrypt.py import base64 import json from Crypto.Cipher import AES from app01.wx import settings class WXBizDataCrypt: def __init__(self, appId, sessionKey): self.appId = appId self.sessionKey = sessionKey def decrypt(self, encryptedData, iv): # base64 decode sessionKey = base64.b64decode(self.sessionKey) encryptedData = base64.b64decode(encryptedData) iv = base64.b64decode(iv) cipher = AES.new(sessionKey, AES.MODE_CBC, iv) decrypted = json.loads(self._unpad(cipher.decrypt(encryptedData))) if decrypted['watermark']['appid'] != self.appId: raise Exception('Invalid Buffer') return decrypted def _unpad(self, s): return s[:-ord(s[len(s)-1:])] @classmethod def getInfo(cls,encryptedData,iv,session_key): return cls(settings.AppId,session_key).decrypt(encryptedData, iv) 项目/serializer.py from rest_framework.serializers import ModelSerializer from app01 import models class User_ser(ModelSerializer): class Meta: model=models.Wxuser fields=\"__all__\" 项目/views.py from app01.wx import WXBizDataCrypt from app01 import serializer from app01 import models class GetInfo(APIView): def post(self,request): param=request.data # 需要小程序端将 encryptedData iv login_key 的值传到后端 # encryptedData iv seesion_key 用于解密获取用户信息 # login_key 用于校验用户登录状态 if param['encryptedData'] and param['iv'] and param['login_key']: # 从redis中拿到login_key并切分拿到 openid 和 session_key openid,seesion_key=cache.get(param['login_key']).split(\"&amp;\") # 利用微信官方提供算法拿到用户的开放数据 data=WXBizDataCrypt.WXBizDataCrypt.getInfo(param['encryptedData'] ,param['iv'] ,seesion_key) save_data={ \"name\":data['nickName'], \"avatar\":data['avatarUrl'], \"language\":data['language'], \"province\":data['province'], \"city\":data['city'], \"country\":data['country'], } # 将拿到的用户信息更新到用户表中 models.Wxuser.objects.filter(openid=openid).update(**save_data) # 反序列化用户对象,并返回到小程序端 data=models.Wxuser.objects.filter(openid=openid).first() data=serializer.User_ser(instance=data,many=False).data return Response({\"code\":200,\"msg\":\"缺少参数\",\"data\":data}) else: return Response({\"code\":200,\"msg\":\"缺少参数\"}) 到此，微信小程序实现后端授权登录就结束了","categories":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"author":"天听"},{"title":"uvicorn+Gunicorn部署","slug":"uvicorn和Gunicorn","date":"2021-01-19T16:00:00.000Z","updated":"2021-01-19T16:00:00.000Z","comments":true,"path":"2021/01/20/uvicorn-he-gunicorn/","link":"","permalink":"http://godhearing.cn/2021/01/20/uvicorn-he-gunicorn/","excerpt":"","text":"什么是wsgi Wsgi是同步通信服务规范，客户端请求一项服务，并等待服务完成，只有当它收到服务的结果时，它才会继续工作。当然了，可以定义一个超时时间，如果服务在规定的时间内没有完成，则认为调用失败，调用方继续工作。 wsgi简单工作图 什么是asgiAsgi是异步通信服务规范。客户端发起服务呼叫，但不等待结果。调用方立即继续其工作，并不关心结果。如果调用方对结果感兴趣，有一些机制可以让其随时被回调方法返回结果。 简单总结一下：Asgi是异步的，Wsgi是同步的，而基于Wsgi的Flask是同步框架，基于Asgi的FastAPI是异步框架，就这么简单。 正题 普及一下小知识，下面我们进入正题，什么是Uvicorn，Uvicorn 是基于 uvloop 和 httptools 构建的非常快速的 ASGI 服务器。目前，Python 仍缺乏异步网关协议接口，ASGI 的出现填补了这一空白，现在开始，我们能够使用共同的标准为所有的异步框架来实现一些工具，ASGI 帮助 Python 在 Web 框架上和 Node.Js 及 Golang 相竞争，目标是获得高性能的 IO 密集型任务，ASGI 支持 HTTP2 和 WebSockets，WSGI 是不支持的。 进程管理器 使用进程管理器确保你以弹性方式运行运行多个进程，你可以执行服务器升级而不会丢弃客户端的请求。 一个进程管理器将会处理套接字设置，启动多个服务器进程，监控进程活动，监听进程重启、关闭等信号。 Uvicorn 提供一个轻量级的方法来运行多个工作进程，比如 --workers 4，但并没有提供进行的监控。 Gunicorn 是成熟的，功能齐全的服务器，Uvicorn 内部包含有 Guicorn 的 workers 类，允许你运行 ASGI 应用程序，这些 workers 继承了所有 Uvicorn 高性能的特点，并且给你使用 Guicorn 来进行进程管理。 这样的话，你可能动态增加或减少进程数量，平滑地重启工作进程，或者升级服务器而无需停机。 在生产环境中，Guicorn 大概是最简单的方式来管理 Uvicorn 了，生产环境部署我推荐使用 Guicorn 和 Uvicorn 简单部署fastapifrom fastapi import FastAPI from fastapi.responses import RedirectResponse app = FastAPI() @app.get(\"/items/\") async def update_item(assetid:str): print(assetid) assetid = assetid.strip() url = 'https://god_hearing.gitee.io/myhexo/?assetid=%s'%assetid print(url) return RedirectResponse(url=url) if __name__ == '__main__': import uvicorn uvicorn.run( app='transfer:app', host='0.0.0.0', port=8000, workers=5, debug=False ) 我们简单的写了一个接口，这里，我是使用了跳转重定向，可以直接return一个response 然后启动的这里，我们采用的是启动五个进程，并且关闭debug模式，不过我还没有发现这个debug关不关的区别在哪 这时候，我们直接运行即可。 但是，这样是阻塞式的，并且在关闭控制台之后就会停止，所以，我们使用gunicorn来解决 安装 pip install gunicorn 在项目文件下 gunicorn main:app -b 0.0.0.0:8000 -w 5 -k uvicorn.workers.UvicornH11Worker --daemon 然后，我们的项目就已经在运行了，然后访问一下你的域名+items/assetid=asfdg 关闭gunicron进程# 查询 pstree -ap|grep gunicorn 结果为： 关闭进程 kill -9 15624 ... ok，部署完成，就是这么简单","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"}],"author":"天听"},{"title":"vue生成图片并下载","slug":"vue生成图片并下载","date":"2021-01-16T16:00:00.000Z","updated":"2021-01-16T16:00:00.000Z","comments":true,"path":"2021/01/17/vue-sheng-cheng-tu-pian-bing-xia-zai/","link":"","permalink":"http://godhearing.cn/2021/01/17/vue-sheng-cheng-tu-pian-bing-xia-zai/","excerpt":"","text":"前言 最近，碰到了一个很尴尬的需求，在我使用python苦苦支撑却实现不了的时候，vue就像一道光，哗啦啦的就来了，这个需求就是合成一张图片，然后把这张图片下载下来，而python我又不会使用opencv等库，所以，只能使用PIL强行搞，但是效果并不好。然后，我求助了我的小伙伴，她很轻易的就帮助我搞定了，(太感谢我的小伙伴了。)，但是下载这个图片，成了一个事。所以，今天这篇攻略带来如何生成图片并且下载 html2canvas 这就是我们要使用到的库，我们通过npm或者yarn来进行下载 安装 npm install --save html2canvas 或者： yarn add html2canvas 将html2canvas引入到组件中 import html2canvas from \"html2canvas\" 将你想生成的区域转成图片，你需要让html2canvas获取到你想要转换的节点内容，因此，你需要添加ref标记。 &lt;div class=\"container\" ref=\"imageDom\">&lt;/div> imageDom需要是你想转换的页面内容的父容器，即你想转换的页面内容需要全部包含在imageDom节点内。 转换 clickGeneratePicture(){ html2canvas(this.$refs.imageDom).then(canvas => { // 转成图片，生成图片地址 this.imgUrl = canvas.toDataURL(\"image/png\"); console.log(this.imgUrl) }); }, 返回的canvas参数就是一个生成好的canvas元素，如果你想将他转成图片，直接使用toDataURL方法即可，将转换的图片地址赋值给你想显示的图片元素，就可以在页面上看到转换后的图片。 直接触发方法， 将图片下载下来 clickGeneratePicture(){ html2canvas(this.$refs.imageDom).then(canvas => { // 转成图片，生成图片地址 this.imgUrl = canvas.toDataURL(\"image/png\"); // 创建隐藏的可下载链接 var eleLink = document.createElement(\"a\"); eleLink.href = this.imgUrl; // 转换后的图片地址 eleLink.download = \"pictureName\"; // 触发点击 document.body.appendChild(eleLink); eleLink.click(); // 然后移除 document.body.removeChild(eleLink); }); }, 随便做个点击事件就好了，不过图片如果展示的过大的话，就会只下载显示的部分，还未解决，待补","categories":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"FastAPI学习之路-3  模型与校验","slug":"FastAPI学习之路-3  模型与校验","date":"2021-01-14T16:00:00.000Z","updated":"2021-01-14T16:00:00.000Z","comments":true,"path":"2021/01/15/fastapi-xue-xi-zhi-lu-3-mo-xing-yu-xiao-yan/","link":"","permalink":"http://godhearing.cn/2021/01/15/fastapi-xue-xi-zhi-lu-3-mo-xing-yu-xiao-yan/","excerpt":"","text":"预设值 如果你有一个接收路径参数的路径操作，但你希望预先设定可能的有效参数值，则可以使用标准的 Python Enum 类型。 就类似于choise枚举，举个栗子，你现在可能要接收一个性别 from enum import Enum class ModelName(str, Enum): nan = \"男\" nv = \"女\" lenet = \"未知\" @app.post(\"/Yawp/\") async def transfer(model_name: ModelName): return {'msg':model_name} 这样，我们需要传的model_name，只剩下这三个选项 同时，他也可以声明路径参数 补充关于参数的一点，如果你想要让这个参数是一个必填的参数，只需要将其不设置默认值即可，例如： async def read_user_item(item_id: str, needy: str=None): 像这种情况，item_id，就是必须填进的一个参数了，而needy有默认值，None，所以，他并不是一个必填的参数 另外一个小技巧，在fastapi交互式文档上操作，可以最大限度的避免一些基础错误，所以，要养成使用交互式文档的习惯哦 请求体 请求体就不用过多介绍了，这是基础中的基础了，如果不懂请自行百度 我们需要先导入BaseModel，来创建数据模型 from pydantic import BaseModel class Item(BaseModel): name:str description: Optional[str] = None price:float tax:Optional[float] = None async def main(item:Item): print(item.dict()) print(item.name) return 和声明查询参数时一样，当一个模型属性具有默认值时，它不是必需的。否则它是一个必需属性。将默认值设为 None 可使其成为可选属性。 我们可以在函数内通过某个字段来进行查看，或者直接dict，获取所有数据 值得说的一点是，请求体传值并不影响url传参，也就是说，我们依旧可以使用路径参数 函数参数将依次按如下规则进行识别： 如果在路径中也声明了该参数，它将被用作路径参数。 如果参数属于单一类型（比如 int、float、str、bool 等）它将被解释为查询参数。 如果参数的类型被声明为一个 Pydantic 模型，它将被解释为请求体。 如果你不想使用 Pydantic 模型，你还可以使用 Body 参数。稍后介绍 约束校验Query(查询参数)Query第一个参数用来定义默认值 可用于限制长度或者正则表达式 #q参数必须为字符串，默认值为None，如果为…,则这个参数必须给值，最小长度3，最大长度50async def reds(q:str = Query(None,min_length=3,max_length=50) async def read_items(q: Optional[List[str]] = Query(None)):这种情况，是可以接收多个q参数的，?q=foo&amp;q=bar 声明必须参数 之前说过，我们声明必须传的参数，只需要将其不设默认即可，现在可以有第二种方法，就是...， 将其默认值设置为...时，他就是一个必须传入的参数了 请求体字段验证与使用 Query、Path 和 Body 在路径操作函数中声明额外的校验和元数据的方式相同，你可以使用 Pydantic 的 Field 在 Pydantic 模型内部声明校验和元数据。像这样 class Item(BaseModel): name:str description: Optional[str] = Field(None, title=\"The description of the item\", max_length=300) price:float tax:Optional[float] = None 模型嵌套 在我们声明模型的时候，他的值不只是单一的字符串，数字等， 像一些列表，集合也同样可以定义，像这样 class Item(BaseModel): name: str description: Optional[str] = None price: float tax: Optional[float] = None tags: list = [] 这将使 tags 成为一个由元素组成的列表。不过它没有声明每个元素的类型。 但是 Python 有一种特定的方法来声明具有子类型的列表。 我们需要从typing导入List，将模型内部改为tags: List[str] = [] Pydantic 模型的每个属性都具有类型。 但是这个类型本身可以是另一个 Pydantic 模型。 因此，你可以声明拥有特定属性名称、类型和校验的深度嵌套的 JSON 对象。 上述这些都可以任意的嵌套。 最后，有一点很重要，我们如果希望传一种像这样的数据:[{a:1},{a:2}]，则可以在路径操作函数的参数中声明此类型，就像声明 Pydantic 模型一样：images: List[Image] from pydantic import BaseModel,Field,HttpUrl class img(BaseModel): url:HttpUrl name:str @app.post(\"/Yawp/\") async def transfer(*,a:List[img]): for i in a: print(i.name) print(i.url) # for i in a: # print(i) return {'msg':a}","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"}],"author":"天听"},{"title":"微信小程序授权登录","slug":"微信小程序授权登录","date":"2021-01-14T16:00:00.000Z","updated":"2021-03-16T12:48:42.543Z","comments":true,"path":"2021/01/15/wei-xin-xiao-cheng-xu-shou-quan-deng-lu/","link":"","permalink":"http://godhearing.cn/2021/01/15/wei-xin-xiao-cheng-xu-shou-quan-deng-lu/","excerpt":"","text":"前言 在开发小程序的过程中， 用户的认证是必不可少的，而微信又取消了小程序的自己认证，所以，我们只能通过微信进行授权登录，而如果将微信授权的信息录入到自己的表中，则需要做一点点的开发，官方文档 微信小程序的后端开发和普通的restful API 大致上相同，只不过要注意以下几点限制 必须使用HTTPS协议请求后端服务器 不支持COOKIE 不支持django内置的user登录, 因为它使用的是微信的用户系统 原理图 只获取openid，只调用wx.login获取code交给后台即可，如果还要获取用户详细信息还要接着调用wx.getUserInfo获取encryptedData，iv提交后台解密用户信息 @api_view(['GET','POST']) def wx_login(request): openidUrl = 'https://api.weixin.qq.com/sns/jscode2session?' code = request.GET['code'] res = requests.get( url = openidUrl, params = { 'appid':APPID, 'secret':APP_SECRET, 'js_code':code, 'grant_type':'authorization_code' } ).json() openid = res['openid'] return Response(openid 这样，我们便获取到了openid，然后返回给前端即可 如果要换取用户信息，则需要前端再次请求我们服务器，需要传入encryptedData iv和session_key来获取用户的信息， encryptedData就是获取到的用户信息，只是得解密一下，通过iv # 解密类 class WXBizDataCrypt: def __init__(self, appId, sessionKey): self.appId = appId self.sessionKey = sessionKey def decrypt(self, encryptedData, iv): # base64 decode sessionKey = base64.b64decode(self.sessionKey) encryptedData = base64.b64decode(encryptedData) iv = base64.b64decode(iv) cipher = AES.new(sessionKey, AES.MODE_CBC, iv) decrypted = json.loads(self._unpad(cipher.decrypt(encryptedData))) if decrypted['watermark']['appid'] != self.appId: raise Exception('Invalid Buffer') return decrypted def _unpad(self, s): return s[:-ord(s[len(s)-1:])] def main(): appId = '你的appid' sessionKey = '获取到的sessionkey' encryptedData = '获取openid那里的数据' iv = '获取openid那里的数据' pc = WXBizDataCrypt(appId, sessionKey) res = pc.decrypt(encryptedData, iv) print(res) res就是解密出来的内容，就是用户的信息 至此，前台将openid存下，而后台，获取了用户信息，该入库还是该做怎样的操作，就可以自定义了，我是将其入库了，完整代码如下： class Applets_RegisterView(APIView): def post(self,request): session_key = request.data.get('session_key') encryptedData = request.data.get(\"encryptedData\") iv = request.data.get(\"iv\") pc = WXBizDataCrypt(APPID, session_key) data = pc.decrypt(encryptedData, iv) username = data['nickName'] avatar = data['avatarUrl'] gender = data['gender'] user_data = { 'username':username, 'avatar':avatar, 'gender':gender, } with transaction.atomic(): try: userobj = RegisterSerializer(data=user_data) if userobj.is_valid() and secretobj.is_valid(): userobj.save() return Response({'code': 1000, 'msg': '注册成功'}) else: return Response({'code':1004,'msg':'网络错误，请稍后再试'}) except Exception as exc: print(exc) return Response({'code': 1004, 'msg': '网络错误，请稍后再试'}) 被一个小小的登录绊倒，我也是没想到的，感谢有位大神孜孜不倦的一直给我讲解。","categories":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"author":"天听"},{"title":"python获取微信公众号历史文章","slug":"python获取微信公众号历史文章","date":"2021-01-14T07:15:00.000Z","updated":"2021-01-14T07:15:00.000Z","comments":true,"path":"2021/01/14/python-huo-qu-wei-xin-gong-zhong-hao-li-shi-wen-zhang/","link":"","permalink":"http://godhearing.cn/2021/01/14/python-huo-qu-wei-xin-gong-zhong-hao-li-shi-wen-zhang/","excerpt":"","text":"前言 我们经常能够在微信公众号上看到各种的文章，而如果我们需要保存到自己本地，难道还需要一个个去复制粘贴吗，答案，根本不需要，只需要一个小小的调用接口，即可将文章悉数拿到，也算一个小爬虫吧 获取assetoken 我们通过微信的开发者平台，可以看到，我们只需要调用一个接口，即可获取assettoken def access_token(): appid = 'APPID' secret = 'SECRET' url = 'https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&amp;appid=%s&amp;secret=%s'%(appid,secret) res = requests.get(url=url) res = json.loads(res.text) return res['access_token'] appid和secret就不用细说了，获取这个是很简单的一件事，不仅仅是获取这个文章，调用任何微信公众号的接口，都需要先获取了这个asset_token 获取素材获取素材就非常的让我暴躁了，因为他的编码格式问题，让我头疼了好久，我们先来看一下这个官方的文档, 需要三个参数，type，offset，count，我们要获取的是图文，所以，type使用news，offset和count就是从哪一个开始返回和返回的个数，不过需要注意的是，返回的个数最大值是20 token = access_token() url = 'https://api.weixin.qq.com/cgi-bin/material/batchget_material?access_token=%s'%(token) data = { \"type\":'news', \"offset\":1, \"count\":1 } data = json.dumps(data) res = requests.post(url=url,data=data) r = res.text.encode('ISO-8859-1').decode('utf8') r = json.loads(r) 这里，我们接入他的接口，但是返回的是ISO-8859-1编码，所以我们需要进行解码 如何查看返回的编码，有一个小技巧，就是打印一下res.encoding 即可 由于这个编码的特殊性，我们需要先对其进行编码，在进行解码 我们再根据他的数据格式，存入到我们的数据库 完善代码 def obtain_article(): token = access_token() url = 'https://api.weixin.qq.com/cgi-bin/material/batchget_material?access_token=%s'%(token) data = { \"type\":'news', \"offset\":1, \"count\":1 } data = json.dumps(data) res = requests.post(url=url,data=data) r = res.text.encode('ISO-8859-1').decode('utf8') r = json.loads(r) conn = pymysql.connect(host='127.0.0.1',port=3306,user='root',password='mySQL',database='blue_lake_user',charset='utf8') cursor = conn.cursor() for i in range(len(r['item'])): title = r['item'][i]['content']['news_item'][0]['title'] link = r['item'][i]['content']['news_item'][0]['url'] img_url = r['item'][i]['content']['news_item'][0]['thumb_url'] create_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())) sql = \"insert into article(title,img,link,create_time,status) values(%s,%s,%s,%s,'1')\" % (conn.escape(title),conn.escape(img_url),conn.escape(link),conn.escape(create_time),) cursor.execute(sql) conn.commit() conn.close() 由于之前说过，数据过长的话，会导致错误，所以，我们采用了escape来进行处理 一个编码整半天，哎，心累","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"提问的智慧(下)","slug":"提问的智慧(下)","date":"2021-01-13T10:43:00.000Z","updated":"2021-01-13T10:43:00.000Z","comments":true,"path":"2021/01/13/ti-wen-de-zhi-hui-xia/","link":"","permalink":"http://godhearing.cn/2021/01/13/ti-wen-de-zhi-hui-xia/","excerpt":"","text":"前言接上节，较长，请耐心观看 低声下气不能代替你的功课有些人明白他们不该粗鲁或傲慢的提问并要求得到答复，但他们选择另一个极端 —— 低声下气：我知道我只是个可悲的新手，一个撸瑟，但...。这既使人困扰，也没有用，尤其是伴随着与实际问题含糊不清的描述时更令人反感。 别用原始灵长类动物的把戏来浪费你我的时间。取而代之的是，尽可能清楚地描述背景条件和你的问题情况。这比低声下气更好地定位了你的位置。 有时网页论坛会设有专为新手提问的版面，如果你真的认为遇到了初学者的问题，到那去就是了，但一样别那么低声下气。 描述问题症状而非你的猜测告诉黑客们你认为问题是怎样造成的并没什么帮助。（如果你的推断如此有效，还用向别人求助吗？），因此要确信你原原本本告诉了他们问题的症状，而不是你的解释和理论；让黑客们来推测和诊断。如果你认为陈述自己的猜测很重要，清楚地说明这只是你的猜测，并描述为什么它们不起作用。 蠢问题 我在编译内核时接连遇到 SIG11 错误， 我怀疑某条飞线搭在主板的走线上了，这种情况应该怎样检查最好？ 聪明问题 我的组装电脑是 FIC-PA2007 主机板搭载 AMD K6/233 CPU（威盛 Apollo VP2 芯片组）， 256MB Corsair PC133 SDRAM 内存，在编译内核时，从开机 20 分钟以后就频频产生 SIG11 错误， 但是在头 20 分钟内从没发生过相同的问题。重新启动也没有用，但是关机一晚上就又能工作 20 分钟。 所有内存都换过了，没有效果。相关部分的标准编译记录如下…。 由于以上这点似乎让许多人觉得难以配合，这里有句话可以提醒你：所有的诊断专家都来自密苏里州。 美国国务院的官方座右铭则是：让我看看（出自国会议员 Willard D. Vandiver 在 1899 年时的讲话：我来自一个出产玉米，棉花，牛蒡和民主党人的国家，滔滔雄辩既不能说服我，也不会让我满意。我来自密苏里州，你必须让我看看。） 针对诊断者而言，这并不是一种怀疑，而只是一种真实而有用的需求，以便让他们看到的是与你看到的原始证据尽可能一致的东西，而不是你的猜测与归纳的结论。所以，大方的展示给我们看吧！ 按发生时间先后列出问题症状问题发生前的一系列操作，往往就是对找出问题最有帮助的线索。因此，你的说明里应该包含你的操作步骤，以及机器和软件的反应，直到问题发生。在命令行处理的情况下，提供一段操作记录（例如运行脚本工具所生成的），并引用相关的若干行（如 20 行）记录会非常有帮助。 如果挂掉的程序有诊断选项（如 -v 的详述开关），试着选择这些能在记录中增加调试信息的选项。记住，多不等于好。试着选取适当的调试级别以便提供有用的信息而不是让读者淹没在垃圾中。 如果你的说明很长（如超过四个段落），在开头简述问题，接下来再按时间顺序详述会有所帮助。这样黑客们在读你的记录时就知道该注意哪些内容了。 描述目标而不是过程如果你想弄清楚如何做某事（而不是报告一个 Bug），在开头就描述你的目标，然后才陈述重现你所卡住的特定步骤。 经常寻求技术帮助的人在心中有个更高层次的目标，而他们在自以为能达到目标的特定道路上被卡住了，然后跑来问该怎么走，但没有意识到这条路本身就有问题。结果要费很大的劲才能搞定。 蠢问题 我怎样才能从某绘图程序的颜色选择器中取得十六进制的的 RGB 值？ 聪明问题 我正试着用替换一幅图片的色码（color table）成自己选定的色码，我现在知道的唯一方法是编辑每个色码区块（table slot）， 但却无法从某绘图程序的颜色选择器取得十六进制的的 RGB 值。 第二种提问法比较聪明，你可能得到像是建议采用另一个更合适的工具的回复。 别要求使用私人电邮回复黑客们认为问题的解决过程应该公开、透明，此过程中如果更有经验的人注意到不完整或者不当之处，最初的回复才能够、也应该被纠正。同时，作为提供帮助者可以得到一些奖励，奖励就是他的能力和学识被其他同行看到。 当你要求私下回复时，这个过程和奖励都被中止。别这样做，让回复者来决定是否私下回答 —— 如果他真这么做了，通常是因为他认为问题编写太差或者太肤浅，以至于对其它人没有兴趣。 这条规则存在一条有限的例外，如果你确信提问可能会引来大量雷同的回复时，那么这个神奇的提问句会是向我发电邮，我将为论坛归纳这些回复。试着将邮件列表或新闻群组从洪水般的雷同回复中解救出来是非常有礼貌的 —— 但你必须信守诺言。 清楚明确的表达你的问题以及需求漫无边际的提问是近乎无休无止的时间黑洞。最有可能给你有用答案的人通常也正是最忙的人（他们忙是因为要亲自完成大部分工作）。这样的人对无节制的时间黑洞相当厌恶，所以他们也倾向于厌恶那些漫无边际的提问。 如果你明确表述需要回答者做什么（如提供指点、发送一段代码、检查你的补丁、或是其他等等），就最有可能得到有用的答案。因为这会定出一个时间和精力的上限，便于回答者能集中精力来帮你。这么做很棒。 要理解专家们所处的世界，请把专业技能想像为充裕的资源，而回复的时间则是稀缺的资源。你要求他们奉献的时间越少，你越有可能从真正专业而且很忙的专家那里得到解答。 所以，界定一下你的问题，使专家花在辨识你的问题和回答所需要付出的时间减到最少，这技巧对你有用答案相当有帮助 —— 但这技巧通常和简化问题有所区别。因此，问我想更好的理解 X，可否指点一下哪有好一点说明？通常比问你能解释一下 X 吗？更好。如果你的代码不能运作，通常请别人看看哪里有问题，比要求别人替你改正要明智得多。 询问有关代码的问题时别要求他人帮你调试有问题的代码，不提示一下应该从何入手。张贴几百行的代码，然后说一声：它不能工作会让你完全被忽略。只贴几十行代码，然后说一句：在第七行以后，我期待它显示 ，但实际出现的是 比较有可能让你得到回应。 最有效描述程序问题的方法是提供最精简的 Bug 展示测试用例（bug-demonstrating test case）。什么是最精简的测试用例？那是问题的缩影；一小个程序片段能刚好展示出程序的异常行为，而不包含其他令人分散注意力的内容。怎么制作最精简的测试用例？如果你知道哪一行或哪一段代码会造成异常的行为，复制下来并加入足够重现这个状况的代码（例如，足以让这段代码能被编译/直译/被应用程序处理）。如果你无法将问题缩减到一个特定区块，就复制一份代码并移除不影响产生问题行为的部分。总之，测试用例越小越好（查看话不在多而在精一节）。 一般而言，要得到一段相当精简的测试用例并不太容易，但永远先尝试这样做的是种好习惯。这种方式可以帮助你了解如何自行解决这个问题 —— 而且即使你的尝试不成功，黑客们也会看到你在尝试取得答案的过程中付出了努力，这可以让他们更愿意与你合作。 如果你只是想让别人帮忙审查（Review）一下代码，在信的开头就要说出来，并且一定要提到你认为哪一部分特别需要关注以及为什么。 别把自己家庭作业的问题贴上来黑客们很擅长分辨哪些问题是家庭作业式的问题；因为我们中的大多数都曾自己解决这类问题。同样，这些问题得由你来搞定，你会从中学到东西。你可以要求给点提示，但别要求得到完整的解决方案。 如果你怀疑自己碰到了一个家庭作业式的问题，但仍然无法解决，试试在使用者群组，论坛或（最后一招）在项目的使用者邮件列表或论坛中提问。尽管黑客们会看出来，但一些有经验的使用者也许仍会给你一些提示。 去掉无意义的提问句避免用无意义的话结束提问，例如有人能帮我吗？或者这有答案吗？。 首先：如果你对问题的描述不是很好，这样问更是画蛇添足。 其次：由于这样问是画蛇添足，黑客们会很厌烦你 —— 而且通常会用逻辑上正确，但毫无意义的回答来表示他们的蔑视， 例如：没错，有人能帮你或者不，没答案。 一般来说，避免用 是或否、对或错、有或没有类型的问句，除非你想得到是或否类型的回答。 即使你很急也不要在标题写紧急这是你的问题，不是我们的。宣称紧急极有可能事与愿违：大多数黑客会直接删除无礼和自私地企图即时引起关注的问题。更严重的是，紧急这个字（或是其他企图引起关注的标题）通常会被垃圾信过滤器过滤掉 —— 你希望能看到你问题的人可能永远也看不到。 有半个例外的情况是，如果你是在一些很高调，会使黑客们兴奋的地方，也许值得这样去做。在这种情况下，如果你有时间压力，也很有礼貌地提到这点，人们也许会有兴趣回答快一点。 当然，这风险很大，因为黑客们兴奋的点多半与你的不同。譬如从 NASA 国际空间站（International Space Station）发这样的标题没有问题，但用自我感觉良好的慈善行为或政治原因发肯定不行。事实上，张贴诸如紧急：帮我救救这个毛绒绒的小海豹！肯定让你被黑客忽略或惹恼他们，即使他们认为毛绒绒的小海豹很重要。 如果你觉得这点很不可思议，最好再把这份指南剩下的内容多读几遍，直到你弄懂了再发文。 礼多人不怪，而且有时还很有帮助彬彬有礼，多用请和谢谢您的关注，或谢谢你的关照。让大家都知道你对他们花时间免费提供帮助心存感激。 坦白说，这一点并没有比清晰、正确、精准并合法语法和避免使用专用格式重要（也不能取而代之）。黑客们一般宁可读有点唐突但技术上鲜明的 Bug 报告，而不是那种有礼但含糊的报告。（如果这点让你不解，记住我们是按问题能教给我们什么来评价问题的价值的） 然而，如果你有一串的问题待解决，客气一点肯定会增加你得到有用回应的机会。 （我们注意到，自从本指南发布后，从资深黑客那里得到的唯一严重缺陷反馈，就是对预先道谢这一条。一些黑客觉得先谢了意味着事后就不用再感谢任何人的暗示。我们的建议是要么先说先谢了，然后事后再对回复者表示感谢，或者换种方式表达感激，譬如用谢谢你的关注或谢谢你的关照。） 问题解决后，加个简短的补充说明问题解决后，向所有帮助过你的人发个说明，让他们知道问题是怎样解决的，并再一次向他们表示感谢。如果问题在新闻组或者邮件列表中引起了广泛关注，应该在那里贴一个说明比较恰当。 最理想的方式是向最初提问的话题回复此消息，并在标题中包含已修正，已解决或其它同等含义的明显标记。在人来人往的邮件列表里，一个看见讨论串问题 X和问题 X - 已解决的潜在回复者就明白不用再浪费时间了（除非他个人觉得问题 X的有趣），因此可以利用此时间去解决其它问题。 补充说明不必很长或是很深入；简单的一句你好，原来是网线出了问题！谢谢大家 – Bill比什么也不说要来的好。事实上，除非结论真的很有技术含量，否则简短可爱的小结比长篇大论更好。说明问题是怎样解决的，但大可不必将解决问题的过程复述一遍。 对于有深度的问题，张贴调试记录的摘要是有帮助的。描述问题的最终状态，说明是什么解决了问题，在此之后才指明可以避免的盲点。避免盲点的部分应放在正确的解决方案和其它总结材料之后，而不要将此信息搞成侦探推理小说。列出那些帮助过你的名字，会让你交到更多朋友。 除了有礼貌和有内涵以外，这种类型的补充也有助于他人在邮件列表/新闻群组/论坛中搜索到真正解决你问题的方案，让他们也从中受益。 至少，这种补充有助于让每位参与协助的人因问题的解决而从中得到满足感。如果你自己不是技术专家或者黑客，那就相信我们，这种感觉对于那些你向他们求助的大师或者专家而言，是非常重要的。问题悬而未决会让人灰心；黑客们渴望看到问题被解决。好人有好报，满足他们的渴望，你会在下次提问时尝到甜头。 思考一下怎样才能避免他人将来也遇到类似的问题，自问写一份文件或加个常见问题（FAQ）会不会有帮助。如果是的话就将它们发给维护者。 在黑客中，这种良好的后继行动实际上比传统的礼节更为重要，也是你如何透过善待他人而赢得声誉的方式，这是非常有价值的资产。 如何解读答案RTFM 和 STFW：如何知道你已完全搞砸了有一个古老而神圣的传统：如果你收到RTFM （Read The Fucking Manual）的回应，回答者认为你应该去读他妈的手册。当然，基本上他是对的，你应该去读一读。 RTFM 有一个年轻的亲戚。如果你收到STFW（Search The Fucking Web）的回应，回答者认为你应该到他妈的网上搜索。那人多半也是对的，去搜索一下吧。（更温和一点的说法是 **Google 是你的朋友**！） 在论坛，你也可能被要求去爬爬论坛的旧文。事实上，有人甚至可能热心地为你提供以前解决此问题的讨论串。但不要依赖这种关照，提问前应该先搜索一下旧文。 通常，用这两句之一回答你的人会给你一份包含你需要内容的手册或者一个网址，而且他们打这些字的时候也正在读着。这些答复意味着回答者认为 你需要的信息非常容易获得； 你自己去搜索这些信息比灌给你，能让你学到更多。 你不应该因此不爽；依照黑客的标准，他已经表示了对你一定程度的关注，而没有对你的要求视而不见。你应该对他祖母般的慈祥表示感谢。 如果还是搞不懂如果你看不懂回应，别立刻要求对方解释。像你以前试着自己解决问题时那样（利用手册，FAQ，网络，身边的高手），先试着去搞懂他的回应。如果你真的需要对方解释，记得表现出你已经从中学到了点什么。 比方说，如果我回答你：看来似乎是 zentry 卡住了；你应该先清除它。，然后，这是一个很糟的后续问题回应：zentry 是什么？ 好的问法应该是这样：哦~~~我看过说明了但是只有 -z 和 -p 两个参数中提到了 zentries，而且还都没有清楚的解释如何清除它。你是指这两个中的哪一个吗？还是我看漏了什么？ 处理无礼的回应很多黑客圈子中看似无礼的行为并不是存心冒犯。相反，它是直接了当，一针见血式的交流风格，这种风格更注重解决问题，而不是使人感觉舒服而却模模糊糊。 如果你觉得被冒犯了，试着平静地反应。如果有人真的做了出格的事，邮件列表、新闻群组或论坛中的前辈多半会招呼他。如果这没有发生而你却发火了，那么你发火对象的言语可能在黑客社区中看起来是正常的，而你将被视为有错的一方，这将伤害到你获取信息或帮助的机会。 另一方面，你偶尔真的会碰到无礼和无聊的言行。与上述相反，对真正的冒犯者狠狠地打击，用犀利的语言将其驳得体无完肤都是可以接受的。然而，在行事之前一定要非常非常的有根据。纠正无礼的言论与开始一场毫无意义的口水战仅一线之隔，黑客们自己莽撞地越线的情况并不鲜见。如果你是新手或外人，避开这种莽撞的机会并不高。如果你想得到的是信息而不是消磨时光，这时最好不要把手放在键盘上以免冒险。 （有些人断言很多黑客都有轻度的自闭症或亚斯伯格综合症，缺少用于润滑人类社会正常交往所需的神经。这既可能是真也可能是假的。如果你自己不是黑客，兴许你认为我们脑袋有问题还能帮助你应付我们的古怪行为。只管这么干好了，我们不在乎。我们喜欢我们现在这个样子，并且通常对病患标记都有站得住脚的怀疑）。 Jeff Bigler 的观察总结和这个相关也值得一读 (tact filters)。 在下一节，我们会谈到另一个问题，当你行为不当时所会受到的冒犯。 如何避免扮演失败者在黑客社区的论坛中有那么几次你可能会搞砸 —— 以本指南所描述到的或类似的方式。而你会在公开场合中被告知你是如何搞砸的，也许攻击的言语中还会带点夹七夹八的颜色。 这种事发生以后，你能做的最糟糕的事莫过于哀嚎你的遭遇、宣称被口头攻击、要求道歉、高声尖叫、憋闷气、威胁诉诸法律、向其雇主报怨、忘了关马桶盖等等。相反地，你该这么做： 熬过去，这很正常。事实上，它是有益健康且合理的。 社区的标准不会自行维持，它们是通过参与者积极而公开地执行来维持的。不要哭嚎所有的批评都应该通过私下的邮件传送，它不是这样运作的。当有人评论你的一个说法有误或者提出不同看法时，坚持声称受到个人攻击也毫无益处，这些都是失败者的态度。 也有其它的黑客论坛，受过高礼节要求的误导，禁止参与者张贴任何对别人帖子挑毛病的消息，并声称如果你不想帮助用户就闭嘴。 结果造成有想法的参与者纷纷离开，这么做只会使它们沦为毫无意义的唠叨与无用的技术论坛。 夸张的讲法是：你要的是“友善”（以上述方式）还是有用？两个里面挑一个。 记着：当黑客说你搞砸了，并且（无论多么刺耳）告诉你别再这样做时，他正在为关心你和他的社区而行动。对他而言，不理你并将你从他的生活中滤掉更简单。如果你无法做到感谢，至少要表现得有点尊严，别大声哀嚎，也别因为自己是个有戏剧性超级敏感的灵魂和自以为有资格的新来者，就指望别人像对待脆弱的洋娃娃那样对你。 有时候，即使你没有搞砸（或者只是在他的想像中你搞砸了），有些人也会无缘无故地攻击你本人。在这种情况下，抱怨倒是真的会把问题搞砸。 这些来找麻烦的人要么是毫无办法但自以为是专家的不中用家伙，要么就是测试你是否真会搞砸的心理专家。其它读者要么不理睬，要么用自己的方式对付他们。这些来找麻烦的人在给他们自己找麻烦，这点你不用操心。 也别让自己卷入口水战，最好不要理睬大多数的口水战 —— 当然，这是在你检验它们只是口水战，并且未指出你有搞砸的地方，同时也没有巧妙地将问题真正的答案藏于其后（这也是有可能的）。 不该问的问题以下是几个经典蠢问题，以及黑客没回答时心中所想的： 问题：我能在哪找到 X 程序或 X 资源？ 问题：我怎样用 X 做 Y？ 问题：如何设定我的 shell 提示？ 问题：我可以用 Bass-o-matic 文件转换工具将 AcmeCorp 档案转换为 TeX 格式吗？ 问题：我的程序/设定/SQL 语句没有用 问题：我的 Windows 电脑有问题，你能帮我吗？ 问题：我的程序不会动了，我认为系统工具 X 有问题 问题：我在安装 Linux（或者 X ）时有问题，你能帮我吗？ 问题：我怎么才能破解 root 帐号/窃取 OP 特权/读别人的邮件呢？ 问题：我能在哪找到 X 程序或 X 资源？ 回答：就在我找到它的地方啊，白痴 —— 搜索引擎的那一头。天哪！难道还有人不会用 Google 吗？ 问题：我怎样用 X 做 Y？ 回答：如果你想解决的是 Y ，提问时别给出可能并不恰当的方法。这种问题说明提问者不但对 X 完全无知，也对 Y 要解决的问题糊涂，还被特定形势禁锢了思维。最好忽略这种人，等他们把问题搞清楚了再说。 问题：如何设定我的 shell 提示？？ 回答：如果你有足够的智慧提这个问题，你也该有足够的智慧去 RTFM，然后自己去找出来。 问题：我可以用 Bass-o-matic 文件转换工具将 AcmeCorp 档案转换为 TeX 格式吗？ 回答：试试看就知道了。如果你试过，你既知道了答案，就不用浪费我的时间了。 问题：我的{程序/设定/SQL 语句}不工作 回答：这不算是问题吧，我对要我问你二十个问题才找得出你真正问题的问题没兴趣 —— 我有更有意思的事要做呢。在看到这类问题的时候，我的反应通常不外如下三种 你还有什么要补充的吗？ 真糟糕，希望你能搞定。 这关我屁事？ 问题：我的 Windows 电脑有问题，你能帮我吗？ 回答：能啊，扔掉微软的垃圾，换个像 Linux 或 BSD 的开源操作系统吧。 注意：如果程序有官方版 Windows 或者与 Windows 有互动（如 Samba），你可以问与 Windows 相关的问题， 只是别对问题是由 Windows 操作系统而不是程序本身造成的回复感到惊讶， 因为 Windows 一般来说实在太烂，这种说法通常都是对的。 问题：我的程序不会动了，我认为系统工具 X 有问题 回答：你完全有可能是第一个注意到被成千上万用户反复使用的系统调用与函数库档案有明显缺陷的人，更有可能的是你完全没有根据。不同凡响的说法需要不同凡响的证据，当你这样声称时，你必须有清楚而详尽的缺陷说明文件作后盾。 问题：我在安装 Linux（或者 X ）时有问题，你能帮我吗？ 回答：不能，我只有亲自在你的电脑上动手才能找到毛病。还是去找你当地的 Linux 使用群组者寻求实际的指导吧（你能在这儿找到使用者群组的清单）。 注意：如果安装问题与某 Linux 的发行版有关，在它的邮件列表、论坛或本地使用者群组中提问也许是恰当的。此时，应描述问题的准确细节。在此之前，先用 Linux 和所有被怀疑的硬件作关键词仔细搜索。 问题：我怎么才能破解 root 帐号/窃取 OP 特权/读别人的邮件呢？ 回答：想要这样做，说明了你是个卑鄙小人；想找个黑客帮你，说明你是个白痴！ 好问题与蠢问题最后，我将透过举一些例子，来说明怎样聪明的提问；同一个问题的两种问法被放在一起，一种是愚蠢的，另一种才是明智的。 蠢问题： 我可以在哪儿找到关于 Foonly Flurbamatic 的资料？ 这种问法无非想得到 STFW 这样的回答。 聪明问题： 我用 Google 搜索过 “Foonly Flurbamatic 2600”，但是没找到有用的结果。谁知道上哪儿去找对这种设备编程的资料？ 这个问题已经 STFW 过了，看起来他真的遇到了麻烦。 蠢问题： 我从 foo 项目找来的源码没法编译。它怎么这么烂？ 他觉得都是别人的错，这个傲慢自大的提问者。 聪明问题： foo 项目代码在 Nulix 6.2 版下无法编译通过。我读过了 FAQ，但里面没有提到跟 Nulix 有关的问题。这是我编译过程的记录，我有什么做的不对的地方吗？ 提问者已经指明了环境，也读过了 FAQ，还列出了错误，并且他没有把问题的责任推到别人头上，他的问题值得被关注。 蠢问题： 我的主机板有问题了，谁来帮我？ 某黑客对这类问题的回答通常是：好的，还要帮你拍拍背和换尿布吗？，然后按下删除键。 聪明问题： 我在 S2464 主机板上试过了 X 、 Y 和 Z ，但没什么作用，我又试了 A 、 B 和 C 。请注意当我尝试 C 时的奇怪现象。显然 florbish 正在 grommicking，但结果出人意料。通常在 Athlon MP 主机板上引起 grommicking 的原因是什么？有谁知道接下来我该做些什么测试才能找出问题？ 这个家伙，从另一个角度来看，值得去回答他。他表现出了解决问题的能力，而不是坐等天上掉答案。 在最后一个问题中，注意告诉我答案和给我启示，指出我还应该做什么诊断工作之间微妙而又重要的区别。 事实上，后一个问题源自于 2001 年 8 月在 Linux 内核邮件列表（lkml）上的一个真实的提问。我（Eric）就是那个提出问题的人。我在 Tyan S2464 主板上观察到了这种无法解释的锁定现象，列表成员们提供了解决这一问题的重要信息。 通过我的提问方法，我给了别人可以咀嚼玩味的东西；我设法让人们很容易参与并且被吸引进来。我显示了自己具备和他们同等的能力，并邀请他们与我共同探讨。通过告诉他们我所走过的弯路，以避免他们再浪费时间，我也表明了对他们宝贵时间的尊重。 事后，当我向每个人表示感谢，并且赞赏这次良好的讨论经历的时候， 一个 Linux 内核邮件列表的成员表示，他觉得我的问题得到解决并非由于我是这个列表中的名人，而是因为我用了正确的方式来提问。 黑客从某种角度来说是拥有丰富知识但缺乏人情味的家伙；我相信他是对的，如果我像个乞讨者那样提问，不论我是谁，一定会惹恼某些人或者被他们忽视。他建议我记下这件事，这直接导致了本指南的出现。 如果得不到回答如果仍得不到回答，请不要以为我们觉得无法帮助你。有时只是看到你问题的人不知道答案罢了。没有回应不代表你被忽视，虽然不可否认这种差别很难区分。 总的来说，简单的重复张贴问题是个很糟的点子。这将被视为无意义的喧闹。有点耐心，知道你问题答案的人可能生活在不同的时区，可能正在睡觉，也有可能你的问题一开始就没有组织好。 你可以通过其他渠道获得帮助，这些渠道通常更适合初学者的需要。 有许多网上的以及本地的使用者群组，由热情的软件爱好者（即使他们可能从没亲自写过任何软件）组成。通常人们组建这样的团体来互相帮助并帮助新手。 另外，你可以向很多商业公司寻求帮助，不论公司大还是小。别为要付费才能获得帮助而感到沮丧！毕竟，假使你的汽车发动机汽缸密封圈爆掉了 —— 完全可能如此 —— 你还得把它送到修车铺，并且为维修付费。就算软件没花费你一分钱，你也不能强求技术支持总是免费的。 对像是 Linux 这种大众化的软件，每个开发者至少会对应到上万名使用者。根本不可能由一个人来处理来自上万名使用者的求助电话。要知道，即使你要为这些协助付费，和你所购买的同类软件相比，你所付出的也是微不足道的（通常封闭源代码软件的技术支持费用比开源软件的要高得多，且内容也没那么丰富）。 如何更好地回答问题态度和善一点。问题带来的压力常使人显得无礼或愚蠢，其实并不是这样。 对初犯者私下回复。对那些坦诚犯错之人没有必要当众羞辱，一个真正的新手也许连怎么搜索或在哪找常见问题都不知道。 如果你不确定，一定要说出来！一个听起来权威的错误回复比没有还要糟，别因为听起来像个专家很好玩，就给别人乱指路。要谦虚和诚实，给提问者与同行都树个好榜样。 如果帮不了忙，也别妨碍他。不要在实际步骤上开玩笑，那样也许会毁了使用者的设置 —— 有些可怜的呆瓜会把它当成真的指令。 试探性的反问以引出更多的细节。如果你做得好，提问者可以学到点东西 —— 你也可以。试试将蠢问题转变成好问题，别忘了我们都曾是新手。 尽管对那些懒虫抱怨一声 RTFM 是正当的，能指出文件的位置（即使只是建议个 Google 搜索关键词）会更好。 如果你决定回答，就请给出好的答案。当别人正在用错误的工具或方法时别建议笨拙的权宜之计（workaround），应推荐更好的工具，重新界定问题。 正面的回答问题！如果这个提问者已经很深入的研究而且也表明已经试过 X 、 Y 、 Z 、 A 、 B 、 C 但没得到结果，回答 试试看 A 或是 B 或者 试试 X 、 Y 、 Z 、 A 、 B 、 C 并附上一个链接一点用都没有。 帮助你的社区从问题中学习。当回复一个好问题时，问问自己如何修改相关文件或常见问题文件以免再次解答同样的问题？，接着再向文件维护者发一份补丁。 如果你是在研究一番后才做出的回答，展现你的技巧而不是直接端出结果。毕竟授人以鱼不如授人以渔。 结语这篇文章真的给予了我很大的启发，也希望能够帮助到任何观看到这篇文章的人，原文地址","categories":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/categories/Book/"}],"tags":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/tags/Book/"}],"author":"天听"},{"title":"提问的智慧(上)","slug":"提问的智慧(上)","date":"2021-01-13T10:41:00.000Z","updated":"2021-01-13T10:41:00.000Z","comments":true,"path":"2021/01/13/ti-wen-de-zhi-hui-shang/","link":"","permalink":"http://godhearing.cn/2021/01/13/ti-wen-de-zhi-hui-shang/","excerpt":"","text":"前言在黑客的世界里，当你拋出一个技术问题时，最终是否能得到有用的回答，往往取决于你所提问和追问的方式。本指南将教你如何正确的提问以获得你满意的答案。 不只是黑客，现在开源（Open Source）软件已经相当盛行，你常常也可以由其他有经验的使用者身上得到好答案，这是件好事；使用者比起黑客来，往往对那些新手常遇到的问题更宽容一些。然而，将有经验的使用者视为黑客，并采用本指南所提的方法与他们沟通，同样也是能从他们身上得到满意回答的最有效方式。 首先你应该明白，黑客们喜爱有挑战性的问题，或者能激发他们思维的好问题。如果我们并非如此，那我们也不会成为你想询问的对象。如果你给了我们一个值得反复咀嚼玩味的好问题，我们自会对你感激不尽。好问题是激励，是厚礼。好问题可以提高我们的理解力，而且通常会暴露我们以前从没意识到或者思考过的问题。对黑客而言，”好问题！”是诚挚的大力称赞。 尽管如此，黑客们有着蔑视或傲慢面对简单问题的坏名声，这有时让我们看起来对新手、无知者似乎较有敌意，但其实不是那样的。 我们不讳言我们对那些不愿思考、或者在发问前不做他们该做的事的人的蔑视。那些人是时间杀手 —— 他们只想索取，从不付出，消耗我们可用在更有趣的问题或更值得回答的人身上的时间。我们称这样的人为 失败者（撸瑟） （由于历史原因，我们有时把它拼作 lusers）。 我们意识到许多人只是想使用我们写的软件，他们对学习技术细节没有兴趣。对大多数人而言，电脑只是种工具，是种达到目的的手段而已。他们有自己的生活并且有更要紧的事要做。我们了解这点，也从不指望每个人都对这些让我们着迷的技术问题感兴趣。尽管如此，我们回答问题的风格是指向那些真正对此有兴趣并愿意主动参与解决问题的人，这一点不会变，也不该变。如果连这都变了，我们就是在降低做自己最擅长的事情上的效率。 我们（在很大程度上）是自愿的，从繁忙的生活中抽出时间来解答疑惑，而且时常被提问淹没。所以我们无情的滤掉一些话题，特别是拋弃那些看起来像失败者的家伙，以便更高效的利用时间来回答赢家（winner）的问题。 如果你厌恶我们的态度，高高在上，或过于傲慢，不妨也设身处地想想。我们并没有要求你向我们屈服 —— 事实上，我们大多数人非常乐意与你平等地交流，只要你付出小小努力来满足基本要求，我们就会欢迎你加入我们的文化。但让我们帮助那些不愿意帮助自己的人是没有效率的。无知没有关系，但装白痴就是不行。 所以，你不必在技术上很在行才能吸引我们的注意，但你必须表现出能引导你变得在行的特质 —— 机敏、有想法、善于观察、乐于主动参与解决问题。如果你做不到这些使你与众不同的事情，我们建议你花点钱找家商业公司签个技术支持服务合同，而不是要求黑客个人无偿地帮助你。 如果你决定向我们求助，当然你也不希望被视为失败者，更不愿成为失败者中的一员。能立刻得到快速并有效答案的最好方法，就是像赢家那样提问 —— 聪明、自信、有解决问题的思路，只是偶尔在特定的问题上需要获得一点帮助。 在提问之前在你准备要通过电子邮件、新闻群组或者聊天室提出技术问题前，请先做到以下事情： 尝试在你准备提问的论坛的旧文章中搜索答案。 尝试上网搜索以找到答案。 尝试阅读手册以找到答案。 尝试阅读常见问题文件（FAQ）以找到答案。 尝试自己检查或试验以找到答案。 向你身边的强者朋友打听以找到答案。 如果你是程序开发者，请尝试阅读源代码以找到答案。 当你提出问题的时候，请先表明你已经做了上述的努力；这将有助于树立你并不是一个不劳而获且浪费别人的时间的提问者。如果你能一并表达在做了上述努力的过程中所学到的东西会更好，因为我们更乐于回答那些表现出能从答案中学习的人的问题。 运用某些策略，比如先用 Google 搜索你所遇到的各种错误信息（搜索 Google 论坛和网页），这样很可能直接就找到了能解决问题的文件或邮件列表线索。即使没有结果，在邮件列表或新闻组寻求帮助时加上一句 我在 Google 中搜过下列句子但没有找到什么有用的东西 也是件好事，即使它只是表明了搜索引擎不能提供哪些帮助。这么做（加上搜索过的字串）也让遇到相似问题的其他人能被搜索引擎引导到你的提问来。 别着急，不要指望几秒钟的 Google 搜索就能解决一个复杂的问题。在向专家求助之前，再阅读一下常见问题文件（FAQ）、放轻松、坐舒服一些，再花点时间思考一下这个问题。相信我们，他们能从你的提问看出你做了多少阅读与思考，如果你是有备而来，将更有可能得到解答。不要将所有问题一股脑拋出，只因你的第一次搜索没有找到答案（或者找到太多答案）。 准备好你的问题，再将问题仔细的思考过一遍，因为草率的发问只能得到草率的回答，或者根本得不到任何答案。越是能表现出在寻求帮助前你为解决问题所付出的努力，你越有可能得到实质性的帮助。 小心别问错了问题。如果你的问题基于错误的假设，某个普通黑客（J. Random Hacker）多半会一边在心里想着蠢问题…， 一边用无意义的字面解释来答复你，希望着你会从问题的回答（而非你想得到的答案）中汲取教训。 绝不要自以为够格得到答案，你没有；你并没有。毕竟你没有为这种服务支付任何报酬。你将会是自己去挣到一个答案，靠提出有内涵的、有趣的、有思维激励作用的问题 —— 一个有潜力能贡献社区经验的问题，而不仅仅是被动的从他人处索取知识。 另一方面，表明你愿意在找答案的过程中做点什么是一个非常好的开端。谁能给点提示？、我的这个例子里缺了什么？以及我应该检查什么地方比请把我需要的确切的过程贴出来更容易得到答复。因为你表现出只要有人能指个正确方向，你就有完成它的能力和决心。 当你提问时慎选提问的论坛小心选择你要提问的场合。如果你做了下述的事情，你很可能被忽略掉或者被看作失败者： 在与主题不合的论坛上贴出你的问题。 在探讨进阶技术问题的论坛张贴非常初级的问题；反之亦然。 在太多的不同新闻群组上重复转贴同样的问题（cross-post）。 向既非熟人也没有义务解决你问题的人发送私人电邮。 黑客会剔除掉那些搞错场合的问题，以保护他们沟通的渠道不被无关的东西淹没。你不会想让这种事发生在自己身上的。 因此，第一步是找到对的论坛。再说一次，Google 和其它搜索引擎还是你的朋友，用它们来找到与你遭遇到困难的软硬件问题最相关的网站。通常那儿都有常见问题（FAQ）、邮件列表及相关说明文件的链接。如果你的努力（包括阅读 FAQ）都没有结果，网站上也许还有报告 Bug（Bug-reporting）的流程或链接，如果是这样，链过去看看。 向陌生的人或论坛发送邮件最可能是风险最大的事情。举例来说，别假设一个提供丰富内容的网页的作者会想充当你的免费顾问。不要对你的问题是否会受到欢迎做太乐观的估计 —— 如果你不确定，那就向别处发送，或者压根别发。 在选择论坛、新闻群组或邮件列表时，别太相信名字，先看看 FAQ 或者许可书以弄清楚你的问题是否切题。发文前先翻翻已有的话题，这样可以让你感受一下那里的文化。事实上，事先在新闻组或邮件列表的历史记录中搜索与你问题相关的关键词是个极好的主意，也许这样就找到答案了。即使没有，也能帮助你归纳出更好的问题。 别像机关枪似的一次”扫射”所有的帮助渠道，这就像大喊大叫一样会使人不快。要一个一个地来。 搞清楚你的主题！最典型的错误之一是在某种致力于跨平台可移植的语言、套件或工具的论坛中提关于 Unix 或 Windows 操作系统程序界面的问题。如果你不明白为什么这是大错，最好在搞清楚这之间差异之前什么也别问。 一般来说，在仔细挑选的公共论坛中提问，会比在私有论坛中提同样的问题更容易得到有用的回答。有几个理由可以支持这点，一是看潜在的回复者有多少，二是看观众有多少。黑客较愿意回答那些能帮助到许多人的问题。 可以理解的是，老练的黑客和一些热门软件的作者正在接受过多的错发信息。就像那根最后压垮骆驼背的稻草一样，你的加入也有可能使情况走向极端 —— 已经好几次了，一些热门软件的作者从自己软件的支持中抽身出来，因为伴随而来涌入其私人邮箱的无用邮件变得无法忍受。 Stack Overflow搜索，然后 在 Stack Exchange 问。 近年来，Stack Exchange community 社区已经成为回答技术及其他问题的主要渠道，尤其是那些开放源码的项目。 因为 Google 索引是即时的，在看 Stack Exchange 之前先在 Google 搜索。有很高的机率某人已经问了一个类似的问题，而且 Stack Exchange 网站们往往会是搜索结果中最前面几个。如果你在 Google 上没有找到任何答案，你再到特定相关主题的网站去找。用标签（Tag）搜索能让你更缩小你的搜索结果。 Stack Exchange 已经成长到超过一百个网站，以下是最常用的几个站： Super User 是问一些通用的电脑问题，如果你的问题跟代码或是写程序无关，只是一些网络连线之类的，请到这里。 Stack Overflow 是问写程序有关的问题。 Server Fault 是问服务器和网管相关的问题。 网站和 IRC 论坛本地的使用者群组（user group），或者你所用的 Linux 发行版本也许正在宣传他们的网页论坛或 IRC 频道，并提供新手帮助（在一些非英语国家，新手论坛很可能还是邮件列表）， 这些地方是开始提问的好首选，特别是当你觉得遇到的也许只是相对简单或者很普通的问题时。有广告赞助的 IRC 频道是公开欢迎提问的地方，通常可以即时得到回应。 事实上，如果程序出的问题只发生在特定 Linux 发行版提供的版本（这很常见），最好先去该发行版的论坛或邮件列表中提问，再到程序本身的论坛或邮件列表提问。（否则）该项目的黑客可能仅仅回复 “用我们的版本”。 在任何论坛发文以前，先确认一下有没有搜索功能。如果有，就试着搜索一下问题的几个关键词，也许这会有帮助。如果在此之前你已做过通用的网页搜索（你也该这样做），还是再搜索一下论坛，搜索引擎有可能没来得及索引此论坛的全部内容。 通过论坛或 IRC 频道来提供使用者支持服务有增长的趋势，电子邮件则大多为项目开发者间的交流而保留。所以最好先在论坛或 IRC 中寻求与该项目相关的协助。 在使用 IRC 的时候，首先最好不要发布很长的问题描述，有些人称之为频道洪水。最好通过一句话的问题描述来开始聊天。 第二步，使用项目邮件列表当某个项目提供开发者邮件列表时，要向列表而不是其中的个别成员提问，即使你确信他能最好地回答你的问题。查一查项目的文件和首页，找到项目的邮件列表并使用它。有几个很好的理由支持我们采用这种办法： 任何好到需要向个别开发者提出的问题，也将对整个项目群组有益。反之，如果你认为自己的问题对整个项目群组来说太愚蠢，也不能成为骚扰个别开发者的理由。 向列表提问可以分散开发者的负担，个别开发者（尤其是项目领导人）也许太忙以至于没法回答你的问题。 大多数邮件列表都会被存档，那些被存档的内容将被搜索引擎索引。如果你向列表提问并得到解答，将来其它人可以通过网页搜索找到你的问题和答案，也就不用再次发问了。 如果某些问题经常被问到，开发者可以利用此信息来改进说明文件或软件本身，以使其更清楚。如果只是私下提问，就没有人能看到最常见问题的完整场景。 如果一个项目既有”使用者” 也有”开发者”（或”黑客”）邮件列表或论坛，而你又不会动到那些源代码，那么就向”使用者”列表或论坛提问。不要假设自己会在开发者列表中受到欢迎，那些人多半会将你的提问视为干扰他们开发的噪音。 然而，如果你确信你的问题很特别，而且在”使用者” 列表或论坛中几天都没有回复，可以试试前往”开发者”列表或论坛发问。建议你在张贴前最好先暗地里观察几天以了解那里的行事方式（事实上这是参与任何私有或半私有列表的好主意） 如果你找不到一个项目的邮件列表，而只能查到项目维护者的电子邮件地址，尽管向他发信。即使是在这种情况下，也别假设（项目）邮件列表不存在。在你的电子邮件中，请陈述你已经试过但没有找到合适的邮件列表，也提及你不反对将自己的邮件转发给他人（许多人认为，即使没什么秘密，私人电子邮件也不应该被公开。通过允许将你的电子邮件转发他人，你给了相应人员处置你邮件的选择）。 使用有意义且描述明确的标题在邮件列表、新闻群组或论坛中，大约 50 字以内的标题是抓住资深专家注意力的好机会。别用喋喋不休的帮帮忙、跪求、急（更别说救命啊！！！！这样让人反感的话，用这种标题会被条件反射式地忽略）来浪费这个机会。不要妄想用你的痛苦程度来打动我们，而应该是在这点空间中使用极简单扼要的描述方式来提出问题。 一个好标题范例是目标 —— 差异式的描述，许多技术支持组织就是这样做的。在目标部分指出是哪一个或哪一组东西有问题，在差异部分则描述与期望的行为不一致的地方。 蠢问题：救命啊！我的笔记本电脑不能正常显示了！ 聪明问题：X.org 6.8.1 的鼠标光标会变形，某牌显卡 MV1005 芯片组。 更聪明问题：X.org 6.8.1 的鼠标光标，在某牌显卡 MV1005 芯片组环境下 - 会变形。 编写目标 —— 差异 式描述的过程有助于你组织对问题的细致思考。是什么被影响了？ 仅仅是鼠标光标或者还有其它图形？只在 X.org 的 X 版中出现？或只是出现在 6.8.1 版中？ 是针对某牌显卡芯片组？或者只是其中的 MV1005 型号？ 一个黑客只需瞄一眼就能够立即明白你的环境和你遇到的问题。 总而言之，请想像一下你正在一个只显示标题的存档讨论串（Thread）索引中查寻。让你的标题更好地反映问题，可使下一个搜索类似问题的人能够关注这个讨论串，而不用再次提问相同的问题。 如果你想在回复中提出问题，记得要修改内容标题，以表明你是在问一个问题， 一个看起来像 Re: 测试 或者 Re: 新 bug 的标题很难引起足够重视。另外，在不影响连贯性之下，适当引用并删减前文的内容，能给新来的读者留下线索。 对于讨论串，不要直接点击回复来开始一个全新的讨论串，这将限制你的观众。因为有些邮件阅读程序，比如 mutt ，允许使用者按讨论串排序并通过折叠讨论串来隐藏消息，这样做的人永远看不到你发的消息。 仅仅改变标题还不够。mutt 和其它一些邮件阅读程序还会检查邮件标题以外的其它信息，以便为其指定讨论串。所以宁可发一个全新的邮件。 在网页论坛上，好的提问方式稍有不同，因为讨论串与特定的信息紧密结合，并且通常在讨论串外就看不到里面的内容，故通过回复提问，而非改变标题是可接受的。不是所有论坛都允许在回复中出现分离的标题，而且这样做了基本上没有人会去看。不过，通过回复提问，这本身就是暧昧的做法，因为它们只会被正在查看该标题的人读到。所以，除非你只想在该讨论串当前活跃的人群中提问，不然还是另起炉灶比较好。 使问题容易回复以请将你的回复发送到……来结束你的问题多半会使你得不到回答。如果你觉得花几秒钟在邮件客户端设置一下回复地址都麻烦，我们也觉得花几秒钟思考你的问题更麻烦。如果你的邮件程序不支持这样做，换个好点的；如果是操作系统不支持这种邮件程序，也换个好点的。 在论坛，要求通过电子邮件回复是非常无礼的，除非你认为回复的信息可能比较敏感（有人会为了某些未知的原因，只让你而不是整个论坛知道答案）。如果你只是想在有人回复讨论串时得到电子邮件提醒，可以要求网页论坛发送给你。几乎所有论坛都支持诸如追踪此讨论串、有回复时发送邮件提醒等功能。 用清晰、正确、精准且语法正确的语句我们从经验中发现，粗心的提问者通常也会粗心的写程序与思考（我敢打包票）。回答粗心大意者的问题很不值得，我们宁愿把时间耗在别处。 正确的拼写、标点符号和大小写是很重要的。一般来说，如果你觉得这样做很麻烦，不想在乎这些，那我们也觉得麻烦，不想在乎你的提问。花点额外的精力斟酌一下字句，用不着太僵硬与正式 —— 事实上，黑客文化很看重能准确地使用非正式、俚语和幽默的语句。但它必须很准确，而且有迹象表明你是在思考和关注问题。 正确地拼写、使用标点和大小写，不要将its混淆为it's，loose搞成lose或者将discrete弄成discreet。不要全部用大写，这会被视为无礼的大声嚷嚷（全部小写也好不到哪去，因为不易阅读。Alan Cox 也许可以这样做，但你不行）。 更白话的说，如果你写得像是个半文盲[译注：小白]，那多半得不到理睬。也不要使用即时通信中的简写或火星文，如将的简化为d会使你看起来像一个为了少打几个键而省字的小白。更糟的是，如果像个小孩似地鬼画符那绝对是在找死，可以肯定没人会理你（或者最多是给你一大堆指责与挖苦）。 如果在使用非母语的论坛提问，你可以犯点拼写和语法上的小错，但决不能在思考上马虎（没错，我们通常能弄清两者的分别）。同时，除非你知道回复者使用的语言，否则请使用英语书写。繁忙的黑客一般会直接删除用他们看不懂语言写的消息。在网络上英语是通用语言，用英语书写可以将你的问题在尚未被阅读就被直接删除的可能性降到最低。 如果英文是你的外语（Second language），提示潜在回复者你有潜在的语言困难是很好的： [译注：以下附上原文以供使用] English is not my native language; please excuse typing errors. 英文不是我的母语，请原谅我的错字或语法。 If you speak $LANGUAGE, please email/PM me; I may need assistance translating my question. 如果你说某语言，请寄信/私讯给我；我需要有人协助我翻译我的问题。 I am familiar with the technical terms, but some slang expressions and idioms are difficult for me. 我对技术名词很熟悉，但对于俗语或是特别用法比较不甚了解。 I’ve posted my question in $LANGUAGE and English. I’ll be glad to translate responses, if you only use one or the other. 我把我的问题用某语言和英文写出来，如果你只用一种语言回答，我会乐意将其翻译成另一种。 使用易于读取且标准的文件格式发送问题如果你人为地将问题搞得难以阅读，它多半会被忽略，人们更愿读易懂的问题，所以： 使用纯文字而不是 HTML (关闭 HTML 并不难）。 使用 MIME 附件通常是可以的，前提是真正有内容（譬如附带的源代码或 patch），而不仅仅是邮件程序生成的模板（譬如只是信件内容的拷贝）。 不要发送一段文字只是一行句子但自动换行后会变成多行的邮件（这使得回复部分内容非常困难）。设想你的读者是在 80 个字符宽的终端机上阅读邮件，最好设置你的换行分割点小于 80 字。 但是，对一些特殊的文件不要设置固定宽度（譬如日志档案拷贝或会话记录）。数据应该原样包含，让回复者有信心他们看到的是和你看到的一样的东西。 在英语论坛中，不要使用Quoted-Printable MIME 编码发送消息。这种编码对于张贴非 ASCII 语言可能是必须的，但很多邮件程序并不支持这种编码。当它们处理换行时，那些文本中四处散布的=20符号既难看也分散注意力，甚至有可能破坏内容的语意。 绝对，永远不要指望黑客们阅读使用封闭格式编写的文档，像微软公司的 Word 或 Excel 文件等。大多数黑客对此的反应就像有人将还在冒热气的猪粪倒在你家门口时你的反应一样。即便他们能够处理，他们也很厌恶这么做。 如果你从使用 Windows 的电脑发送电子邮件，关闭微软愚蠢的智能引号功能 （从[选项] &gt; [校订] &gt; [自动校正选项]，勾选掉智能引号单选框），以免在你的邮件中到处散布垃圾字符。 在论坛，勿滥用表情符号和HTML功能（当它们提供时）。一两个表情符号通常没有问题，但花哨的彩色文本倾向于使人认为你是个无能之辈。过滥地使用表情符号、色彩和字体会使你看来像个傻笑的小姑娘。这通常不是个好主意，除非你只是对性而不是对答案感兴趣。 如果你使用图形用户界面的邮件程序（如微软公司的 Outlook 或者其它类似的），注意它们的默认设置不一定满足这些要求。大多数这类程序有基于选单的查看源代码命令，用它来检查发送文件夹中的邮件，以确保发送的是纯文本文件同时没有一些奇怪的字符。 精确地描述问题并言之有物 仔细、清楚地描述你的问题或 Bug 的症状。 描述问题发生的环境（机器配置、操作系统、应用程序、以及相关的信息），提供经销商的发行版和版本号（如：Fedora Core 4、Slackware 9.1等）。 描述在提问前你是怎样去研究和理解这个问题的。 描述在提问前为确定问题而采取的诊断步骤。 描述最近做过什么可能相关的硬件或软件变更。 尽可能的提供一个可以重现这个问题的可控环境的方法。 尽量去揣测一个黑客会怎样反问你，在你提问之前预先将黑客们可能遇到的问题回答一遍。 以上几点中，当你报告的是你认为可能在代码中的问题时，给黑客一个可以重现你的问题的环境尤其重要。当你这么做时，你得到有效的回答的机会和速度都会大大的提升。 Simon Tatham 写过一篇名为《如何有效的报告 Bug》的出色文章。强力推荐你也读一读。 话不在多而在精你需要提供精确有内容的信息。这并不是要求你简单的把成堆的出错代码或者资料完全转录到你的提问中。如果你有庞大而复杂的测试样例能重现程序挂掉的情境，尽量将它剪裁得越小越好。 这样做的用处至少有三点。 第一，表现出你为简化问题付出了努力，这可以使你得到回答的机会增加； 第二，简化问题使你更有可能得到有用的答案； 第三，在精炼你的 bug 报告的过程中，你很可能就自己找到了解决方法或权宜之计。 别动辄声称找到 Bug当你在使用软件中遇到问题，除非你非常、非常的有根据，不要动辄声称找到了 Bug。提示：除非你能提供解决问题的源代码补丁，或者提供回归测试来表明前一版本中行为不正确，否则你都多半不够完全确信。这同样适用在网页和文件，如果你（声称）发现了文件的Bug，你应该能提供相应位置的修正或替代文件。 请记得，还有许多其它使用者没遇到你发现的问题，否则你在阅读文件或搜索网页时就应该发现了（你在抱怨前已经做了这些，是吧？）。这也意味着很有可能是你弄错了而不是软件本身有问题。 编写软件的人总是非常辛苦地使它尽可能完美。如果你声称找到了 Bug，也就是在质疑他们的能力，即使你是对的，也有可能会冒犯到其中某部分人。当你在标题中嚷嚷着有Bug时，这尤其严重。 提问时，即使你私下非常确信已经发现一个真正的 Bug，最好写得像是你做错了什么。如果真的有 Bug，你会在回复中看到这点。这样做的话，如果真有 Bug，维护者就会向你道歉，这总比你惹恼别人然后欠别人一个道歉要好一点。 未完待续….","categories":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/categories/Book/"}],"tags":[{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/tags/Book/"}],"author":"天听"},{"title":"python使用RSA非对称加密","slug":"python使用RSA非对称加密","date":"2021-01-07T10:00:00.000Z","updated":"2021-01-07T10:00:00.000Z","comments":true,"path":"2021/01/07/python-shi-yong-rsa-fei-dui-cheng-jia-mi/","link":"","permalink":"http://godhearing.cn/2021/01/07/python-shi-yong-rsa-fei-dui-cheng-jia-mi/","excerpt":"","text":"前言 RSA公钥加密算法是1977年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的。RSA就是他们三人姓氏开头字母拼在一起组成的。RSA是目前最有影响力的公钥加密算法，它能够抵抗到目前为止已知的绝大多数密码攻击，RSA算法基于一个十分简单的数论事实：将两个大质数相乘十分容易，但是想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥。 RSA算法的原理，目前网络上有许多优秀的文章，特别推荐阅读阮一峰老师的文章： RSA算法原理1 RSA算法原理2 本文主要描述如何使用RSA来对文件进行不对称加密。 安装crypto这个模块的安装比较特殊，可能会有点小坑，安装时需要注意。 crypto，pycrypto，pycryptodome的功能是一样的。crypto与pycrypto已经没有维护了，后面可以使用pycryptodome。 但是，三个名字，再分Linux和Windows操作系统，再分Python2和Python3，叠加起来就有12种情况了，非常复杂。 在 Windows 中，不管是 Python2 和 Python3 ，都不能用 crypto 和 pycrypto ，可以用 pycryptodome 。 在 Linux 中，不管是 Python2 和 Python3 ，都不能用 crypto ，可以用 pycrypto 和 pycryptodome 。 所以，总结一句话，直接都用 pycryptodome 就行了，(保证pip正常能用的情况下)安装命令如下： pip install pycryptodome 安装之前，最好先把 crypto 和 pycrypto 卸载了(uninstall)，避免不必要的麻烦。 生成RSA公钥和私钥from Crypto import Random from Crypto.PublicKey import RSA def generate_Key(): random_generator = Random.new().read rsa = RSA.generate(2048, random_generator) private_key = rsa.exportKey() # 写入私钥文件 with open('private_rsa_key.bin', 'wb') as f: f.write(private_key) public_key = rsa.publickey().exportKey() # 写入公钥文件 with open('rsa_public.pem', 'wb') as f: f.write(public_key) 上面的代码中，生成了一对公钥和私钥。 导入了 RSA ，通过 RSA 的 generate() 方法实例化一个对象 rsa 。再通过 rsa 的 exportKey() 方法和 publickey() 方法生成公钥和私钥。 执行这个函数，可以看到，生成了一对公钥和私钥 当然每次运行的结果都不一定，公钥是公开的，任何人都可以看到，但是私钥一定要保存好，否则一旦泄露，意味着你的信息也不安全了。 进行加密和解密from Crypto import Random from Crypto.PublicKey import RSA from Crypto.Cipher import PKCS1_v1_5 as PKCS1_cipher import base64 # 加密 def encryption(keys): with open('rsa_public.pem') as f: key = f.read() pub_key = RSA.importKey(str(key)) cipher = PKCS1_cipher.new(pub_key) rsa_text = base64.b64encode(cipher.encrypt(bytes(keys.encode(\"utf8\")))) return rsa_text.decode('utf-8') # 解密 def decrypt(keys): with open('private_rsa_key.bin') as f: key = f.read() pri_key = RSA.importKey(key) cipher = PKCS1_cipher.new(pri_key) back_text = cipher.decrypt(base64.b64decode(keys),0) return back_text.decode('utf-8') 上面的函数，我们将保存的公钥和私钥读出来，然后进行加密和解密的过程，不过千万要注意的是，私钥不要泄露。 签名和验签 RSA还可以用来签名和验签，正好与加密相反的是，他是使用私钥来签名，公钥来验签 废话不多说，上代码 from Crypto.PublicKey import RSA from Crypto.Hash import SHA import base64 from Crypto.Signature import PKCS1_v1_5 as PKCS1_signature message = \"需要加密的信息\" # 使用私钥生成签名 with open('private_rsa_key.bin') as f: key = f.read() pri_key = RSA.importKey(key) signer = PKCS1_signature.new(pri_key) digest = SHA.new() digest.update(message.encode(\"utf8\")) sign = signer.sign(digest) signature = base64.b64encode(sign) print(signature.decode('utf-8')) # 使用公钥验证签名 with open('rsa_public.pem') as f: key = f.read() pub_key = RSA.importKey(key) verifier = PKCS1_signature.new(pub_key) digest = SHA.new() digest.update(message.encode(\"utf8\")) print(verifier.verify(digest, base64.b64decode(signature))) 继续使用之前写入文件中的私钥和公钥，使用私钥来生成信息的签名，然后使用公钥来验证签名。 生成签名时，使用 RSA 的 importKey() 方法对(从文件中读取的)私钥字符串进行处理，处理成可用的私钥用于生成签名。 从 Crypto.Signature 中导入 PKCS1_v1_5 ，重命名为 PKCS1_signature 。然后实例化一个签名对象 signer ，传入的参数是私钥，通过的 signer 的 sign() 方法对信息生成签名。 信息需要先转换成 sha 字符串，使用 Crypto.Hash 中的 SHA 来转换。生成的签名是字节串，为了显示得更友好，可以将结果转换成 base64 字符串。 验证签名时，使用 RSA 的 importKey() 方法对(从文件中读取的)公钥字符串进行处理，处理成可用的公钥用于验证签名。 实例化一个验证对象 verifier ，传入的参数是公钥，通过的 verifier 的 verify() 方法对签名进行验证。验证结果是一个布尔值，验证成功返回 True , 不成功返回 False 。","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"RSA","slug":"RSA","permalink":"http://godhearing.cn/tags/RSA/"}],"author":"天听"},{"title":"pymysql的坑以及django多个数据库迁移","slug":"mysql的坑","date":"2020-12-30T10:00:00.000Z","updated":"2020-12-30T10:00:00.000Z","comments":true,"path":"2020/12/30/mysql-de-keng/","link":"","permalink":"http://godhearing.cn/2020/12/30/mysql-de-keng/","excerpt":"","text":"前言 今日有个爬虫的需求，就在我把各个组件都测试完毕，将他们自信的组装到一起，结果，除了意外，这个意外就是显示sql语句有错误，可是，在我反复的在终端试验，却没有任何的错误，于是我开始了漫长的资料查询。 原因分析 pymysql在储存长文本，有时会出现转义字符，可能导致两个连续的数据会被挤到一起 解决办法 我研究了很多的解决办法，选出两种最为方便的 在连接时，将编码格式改为utf8mb4 在需要转义的对象用连接.escape(value)方法转义 sql = \"insert into article(title,img,link,create_time,status) values(%s,%s,%s,%s,'1')\" % (conn.escape(title),conn.escape(img),conn.escape(link),conn.escape(create_time),) django多数据库的迁移 首先，我们需要在settings中链接上多个数据库，并且指定app 然后指定哪个模型使用哪个数据库 DATABASE_APPS_MAPPING = { # 'app_name':'database_name', 'user': 'user', } 如果不想每个应用同步到特定数据库，可以定义 database router ，它实施限制特定模型可用性的策略。 在迁移时，生成迁移文件，命令不变 python manage.py makemigrations 迁移时需要指定哪个app进行数据迁移 python manage.py migrate --database=user","categories":[{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/categories/mysql/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"}],"author":"天听"},{"title":"二维码与python","slug":"二维码与python","date":"2020-12-24T09:20:00.000Z","updated":"2020-12-24T09:20:00.000Z","comments":true,"path":"2020/12/24/er-wei-ma-yu-python/","link":"","permalink":"http://godhearing.cn/2020/12/24/er-wei-ma-yu-python/","excerpt":"","text":"前言 在二维码泛滥无比的今天，我们作为开发者，怎样生成二维码可谓是基础了，今天带来一篇python生成二维码的攻略 开始qrcode首先安装这个库 pip install qrcode 这个库的GitHub地址也给大家，方便大家更进一步了解 https://github.com/lincolnloop/python-qrcode 简单使用用这个库来生成二维码，简直不要太简单了，只需一行代码！ img = qrcode.make(data=\"你好\") 当然我们还需要导入库以及保存为图片格式： import qrcode # 生成二维码 img = qrcode.make(data=\"昊烨向你问好\") # 将二维码保存为图片 with open('test.png', 'wb') as f: img.save(f) 运行一下代码，就可以看到生成了一张test.png的图片： 当然你不仅仅能放文本在里面，你还能让它跳转链接，你只需要把data里的数据改为url即可。 高级使用除了简单生成二维码外，我们还能够自定义一些属性。 import qrcode qr = qrcode.QRCode( version=20, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=4, ) qr.add_data('Some data') qr.make(fit=True) img = qr.make_image(fill_color=\"black\", back_color=\"white\") with open('test1.png', 'wb') as f: img.save(f) 可以看到，属性都在QRCode对象中设置，下面我来简单介绍一下上面的参数。 version：二维码大小，用1~40之间的整数来设置。最小的version=1，是一个21x21的矩阵。如果你想自动生成，将值设置为 None 并使用 fit=True 参数即可。 error_correction: 二维码的纠错范围，可以选择4个常量： ERROR_CORRECT_L 7%以下的错误会被纠正 ERROR_CORRECT_M (default) 15%以下的错误会被纠正 ERROR_CORRECT_Q 25 %以下的错误会被纠正 ERROR_CORRECT_H. 30%以下的错误会被纠正 box_size：每一个点中的像素个数 border：二维码距图像外围边框距离，默认为4，而且相关规定最小为4 fill_color和back_color：可以更改QR的背景和绘画颜色。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"seaweedfs","slug":"seaweedfs","date":"2020-12-22T10:54:00.000Z","updated":"2020-12-22T10:54:00.000Z","comments":true,"path":"2020/12/22/seaweedfs/","link":"","permalink":"http://godhearing.cn/2020/12/22/seaweedfs/","excerpt":"","text":"前言 Saeweedfs是一个由golang语言开发的分布式对象存储系统，很适合做图片服务器，性能很好，并且可兼容挂载提供路径访问的方式，使得文件储存在云端变得非常方便。 seaweedfs的特点： 可以成存储上亿的文件（根据你硬盘大小变化） 速度刚刚的 github地址 安装安装呢，需要从官网上下载对应系统的版本，地址，我以windows为例，可以看到，下载下来的是个压缩包，解压后就是一个exe安装包，但是我们直接点是没有用的，所以，我们通过命令来进行 首先呢，我们查看一下所有的命令，得知，可以使用master命令来进行启动 weed master -ip=127.0.0.1 master节点设置好了之后，继续看文档，来设置volume节点 如果没有data这个文件夹，就自己创建一个 mserver是master节点地址 weed volume -dir=\"./data\" -max=10 -mserver=\"127.0.0.1:9333\" -port=9000 -index=leveldb 然后访问一下http://localhost:9333/即可，好，一切ok，再查看一下状态 \"http://127.0.0.1:9333/cluster/status?pretty=y\" 使用继续看文档，如果要写入文件，则需要发送请求到/dir/assign来获取fid和volume服务器url 然后将fid拼接，发送文件，这里用postman测试一下 发送成功，这时候你可以将fid存下 fid： 开头的数字3表示卷ID。逗号后是一个文件密钥01和一个文件cookie 637037d6。 卷ID是32位无符号整数。文件密钥是一个无符号的64位整数。文件cookie是32位无符号整数，用于防止URL猜测。 文件密钥和文件cookie均以十六进制编码。可以使用自己的格式存储&lt;卷ID，文件密钥，文件cookie&gt;元组，也可以将fid存储。 然后，怎么访问呢，当然是通过刚刚保存的fid啦，不过要注意，一定要先查找卷服务器的url，http://localhost:9333/dir/lookup?volumeId=卷ID，然后通过返回的publicUrl这个域名来进行查找 要注意，这时，不只是可以用这个方式来访问，还可以使用 # 卷id/密钥+文件cookie/文件名 http://localhost:8080/3/01637037d6/my_preferred_name.jpg # 卷id/密钥+文件你cookie.后缀名 http://localhost:8080/3/01637037d6.jpg 还有其他方法，不一一列举啦 http://localhost:8080/3,01637037d6.jpg http://localhost:8080/3/01637037d6 http://localhost:8080/3,01637037d6 如果要获取图像的缩放版本，则可以添加一些参数： http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200 http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fit http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fill 要更新，请发送另一个具有更新文件内容的POST请求，就是fid不变的情况下，再发一个新的文件，它就会自动给覆盖掉 删除的话，直接发送delete请求到相同的地址，删除成功会给你返回一个size，文件不存在的话这个size的值为0 何谓分布式，当然是可以分布啦，它本身提供了机架感知和数据中心感知复制，SeaweedFS在卷级别应用复制策略。因此，在获取文件ID时，可以指定复制策略。例如： curl http://localhost:9333/dir/assign?replication=001 复制参数选项包括： 000:无复制 001:在同一机架上复制一次 010:在不同机架上复制一次，但数据中心相同 100:在不同的数据中心复制一次 200:在两个不同的数据中心复制两次 110:在不同的机架上复制一次，在不同的数据中心复制一次 还可以在启动主服务器时设置默认复制策略。额..暂时没有搞，稍后补 python中使用seaweedfs 我们如果要在python中使用这个seaweedfs，则需要先安装一个包pip install pyseaweed 使用的方法，非常的简单，在GitHub上，作者也给写的相当明白了 from pyseaweed import WeedFS # File upload w = WeedFS(\"localhost\", 9333) # weed-fs master address and port fid = w.upload_file(\"n.txt\") # path to file # Get file url file_url = w.get_file_url(fid) # Delete file res = w.delete_file(fid) # res is boolean (True if file was deleted) 我们简单的封装一下 from pyseaweed import WeedFS # 上传文件 def upload_file(seaweed_address,file,name): w = WeedFS(*seaweed_address) fid = w.upload_file(stream=file,name=name) if fid: return fid else: return False # # 查看文件的域名 def get_file_url(seaweed_address, fid): w = WeedFS(*seaweed_address) file_url = w.get_file_url(fid) if file_url: return file_url else: return '该资源不存在' def del_file(seaweed_address,fid): w = WeedFS(*seaweed_address) res = w.delete_file(fid) return res # True or False 这里为什么要指定stream和name呢，这是因为如果传一个参数，就必须是文件的路径，如果不传文件路径，就只能将他的数据流和名字传进来。 我们使用视图来测试一下吧,seaweed_address是我自己配置的一个地址，这个可以写死，也可以写成元组，然后像我这样使用 class avatar_upload(APIView): def post(self,request): avatar = request.FILES.get('avatar') # 调用文件存储系统，将返回的fid存下 for chunk in avatar.chunks(): fid = upload.upload_file(USER_AVATAR,chunk,avatar.name) print(fid) return Response({}) ok，完美，不过需要注意的是，在本地是没法测的，我也在找这个问题，服务器上是没有问题","categories":[{"name":"文件","slug":"文件","permalink":"http://godhearing.cn/categories/%E6%96%87%E4%BB%B6/"}],"tags":[{"name":"文件","slug":"文件","permalink":"http://godhearing.cn/tags/%E6%96%87%E4%BB%B6/"}],"author":"天听"},{"title":"bigchaindb安装","slug":"bigchaindb安装","date":"2020-12-20T08:06:00.000Z","updated":"2020-12-20T08:06:00.000Z","comments":true,"path":"2020/12/20/bigchaindb-an-zhuang/","link":"","permalink":"http://godhearing.cn/2020/12/20/bigchaindb-an-zhuang/","excerpt":"","text":"前言 在很久没有更新博客以及自己技术的前提下，感受到了来自编程界的毒打，接受新事物，本是开发者应该作为本能的一件事，但实际上，我并没有坚持多久，很拒绝的接受新事物，所以，今日起，重铸开发荣光，我辈义不容辞。 今天带来的是BigChainDB，这是一个可用的去中心数据库，听到这个词，是不是意识到另一样某个同样是去中心的东西，是的，区块链。 BigChainDB是一个区块链数据库，它具有每秒百万次写操作，储存PB级别的数据和亚秒级响应时间的性能。BigChainDB的设计起始于分布式数据库，通过创新加入了很多区块链的特性，像区中心控制、不可改变性、数字资产的创建和移动。 BigChainDB继承了现代分布式数据库的特性：吞吐量和容量都是与节点数量线性相关，功能齐全的NoSQL查询语言，高效的查询和权限管理。因为构建在已有的分布式数据库上，它在代码层面也继承了企业级的健壮性。可扩展的容量意味着具有法律效力的合同和认证可以直接存储在区块链数据库里。 安装 安装我采用的是docker 在安装了docker的前提下，直接拉取镜像 docker pull bigchaindb/bigchaindb:all-in-one 大概有300M左右，可以执行命令启动一下 docker run -it -d -p 9984:9984 镜像id 访问一下9984，可以看到你的版本号 安装一些依赖 sudo apt-get update sudo apt-get install libffi-dev pip install --upgrade setuptools pip3 install pytest-runner 然后安装驱动，对应你的版本号 pip install -U bigchaindb-driver==版本号 不过，可能会找不到这个包啊这些错误，没关系，我们直接去github上拉取他源码git clone https://github.com/bigchaindb/bigchaindb-driver.git 然后进入这个文件夹，执行python setup.py install，强行给他装上，不只是这个包，任何包都可以通过这个办法来进行安装 之后，在docker内重新启动服务，命令为： $ docker run \\ --detach \\ --name bigchaindb \\ --publish 9984:9984 \\ --publish 9985:9985 \\ --publish 27017:27017 \\ --publish 26657:26657 \\ --volume $HOME/bigchaindb_docker/mongodb/data/db:/data/db \\ --volume $HOME/bigchaindb_docker/mongodb/data/configdb:/data/configdb \\ --volume $HOME/bigchaindb_docker/tendermint:/tendermint \\ bigchaindb/bigchaindb:all-in-one 含义为： docker run 告诉Docker运行一些映像 --detach 在后台运行容器 publish 9984:9984 将主机端口映射9984到容器端口9984 （BigchainDB API服务器） 9985 BigchainDB Websocket服务器 27017 MongoDB的默认端口 26657 Tendermint RPC服务器 --volume \"$HOME/bigchaindb_docker/mongodb:/data\" 将主机目录映射 $HOME/bigchaindb_docker/mongodb 到容器目录 /data ; 这使我们能够将数据持久保存在主机上，您可以在官方Docker文档中阅读更多 内容 $HOME/bigchaindb_docker/tendermint:/tendermint 保留Tendermint数据。 bigchaindb/bigchaindb:all-in-one要使用的图像。容器名称之后的所有选项都将传递到容器内部的入口点。 使用 官方文档可以说是非常详细了，我们从创建开始 # 创建用户的密钥与公钥 alice, bob = generate_keypair(), generate_keypair() # 链接 def conn(): bdb = BigchainDB('http://127.0.0.1:9984') return bdb # 创建一个链接 bdb = conn() # 构造交易数据 bicycle_asset = { 'data': { 'bicycle': { 'serial_number': 'abcd1234', 'manufacturer': 'bkfab' }, }, } # 元数据 bicycle_asset_metadata = { 'planet': 'earth' } # 准备交易(operation是操作，create代表创建，singners是公钥，创建出来供谁拥有，asset是交易的东西，metadata是元数据) prepared_creation_tx = bdb.transactions.prepare( operation='CREATE', signers=alice.public_key, asset=bicycle_asset, metadata=bicycle_asset_metadata ) # 通过私钥签名来完成交易(拥有者的私钥来进行交易签名) fulfilled_creation_tx = bdb.transactions.fulfill( prepared_creation_tx, private_keys=alice.private_key ) # 发送到bigchaindb节点 sent_creation_tx = bdb.transactions.send_commit(fulfilled_creation_tx) # 完成创建 txid = fulfilled_creation_tx['id'] 这样，我们就创建好了一个可交易的资产","categories":[{"name":"bigchaindb","slug":"bigchaindb","permalink":"http://godhearing.cn/categories/bigchaindb/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"bigchaindb","slug":"bigchaindb","permalink":"http://godhearing.cn/tags/bigchaindb/"}],"author":"天听"},{"title":"MobaXterm操作prometheus和grafana搭建监控预警","slug":"MobaXterm操作prometheus和grafana搭建监控预警","date":"2020-05-15T07:33:00.000Z","updated":"2020-05-15T07:33:00.000Z","comments":true,"path":"2020/05/15/mobaxterm-cao-zuo-prometheus-he-grafana-da-jian-jian-kong-yu-jing/","link":"","permalink":"http://godhearing.cn/2020/05/15/mobaxterm-cao-zuo-prometheus-he-grafana-da-jian-jian-kong-yu-jing/","excerpt":"","text":"前言 MobalXterm又名MobalXVT，是一款增强型终端，X服务器和Unix命令集工具箱，可以用它来连接服务器 MobaXterm主要功能： 支持各种连接 SSH，X11，RDP，VNC，FTP，MOSH 支持 Unix 命令(bash，ls，cat，sed，grep，awk，rsync，…) 连接 SSH 终端后支持 SFTP 传输文件 各种丰富的插件(git/dig/aria2…) 可运行 Windows 或软件 小问题官网地址，安装过程不再阐述，解压会比较慢，后续就正常了 然后打开，连接服务器 先点击session，连接设置可以按需选择，这里我用了SSH，输入你服务器的公网IP即可 然后登录，就可以进入你的服务器内 左边是主机文件，右边是操作台，也就是终端勾选左下角的 Follow terminal folder可以让两个的工作路径保持一致。 文件传输和下载，可以采用直接拖拽的方式，或者采用鼠标右键选择相应功能。 但是，有一个问题，就是编码，编码问题是一个难以看出的问题，基本上，一些根本找不出错误的代码，一运行就出错，这就有可能是编码的问题，今天我在服务器上修改docker容器内的配置文件，但是由于想偷个懒，在主机上将配置文件拖出来，然后修改完再挂载回去，可是呢，报了一个及其恶心的错误 level=error ts=2018-12-07T07:16:42.642Z caller=main.go:808 err=\"error loading config from \\\"/etc/prometheus/prometheus.yml\\\": one or more errors occurred while applying the new configuration (--config.file=\\\"/etc/prometheus/prometheus.yml\\\")\" 这个呢，找了很久也没有找到问题，最后灵机一动，是不是编码有问题，然后又转头回来整编码，结果还是没用，没办法，只能使用服务器里唯一的一个神一样的编辑器VIM，来进行一个字一个字的敲，最后神奇的是，问题解决了 虽然呢，编码都是utf8，但是，总有些说不上来的错误，反正呢，乖乖的用内部的VIM，一点问题没有。 还有一点是，在使用MobaXerm中的vim时，在修改文件的时候，经常会遇到readonly option is set （add！to override）这样的错误，解决办法也非常简单，如果是root权限，可以直接:wq!，或者按下ESC，输入set noreadonly，然后就正常保存就可以了 在服务器上部署，用docker是很方便的，我之前也有过几篇攻略，虽然可能说的很磕磕绊绊，但是还是希望能帮到大家。 进入正题Prometheus，是自动化运维领域很常用的软件，它中文名为普罗米修斯，它的作用就是监控告警 话不多说，路程还长，首先是安装问题，安装呢，我们在MobalXterm的终端中，使用docker来进行安装 #升级yum sudo yum update #卸载旧版本docker sudo yum remove docker docker-common docker-selinux docker-engine #安装依赖 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 #设置源 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sudo yum makecache fast #安装docker sudo yum install docker-ce #启动服务 sudo systemctl start docker 安装成功后查看版本: docker -v 显示版本号即为成功 然后可以换国内源来提升下载速度 1.先执行 sudo vim /etc/docker/daemon.json 2.添加如下代码 { \"registry-mirrors\": [\"https://d7grpode.mirror.aliyuncs.com\"] } 重启docker sudo systemctl restart docker 随后拉取普罗米修斯的镜像 docker pull prom/prometheus:latest 这里我们以监控Redis数据库为例子，所以还需要拉取redis和redis状态收集器两个镜像： docker pull redis docker pull oliver006/redis_exporter:latest 然后启动一下 docker run -d --name redis -p 6379:6379 redis docker run -d --name redis_exporter -p 9121:9121 oliver006/redis_exporter:latest --redis.addr redis://公网ip:6379 随后创建普罗米修斯的配置文件 # 执行这条命令 vim /tmp/prometheus.yml # 加入下面代码 scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'redis' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['公网ip:9121'] 这里每隔5秒获取一下运行的信息，然后启动 docker run -d -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus:latest 然后可以访问一下http://公网ip:9090/targets 然后，我们新创建一个机器人 需要注意的是，要将公网ip写上 然后再拉取一个告警模块以及顶顶机器人的插件 如果普罗米修斯的告警模块检测到异常，就会通过钉钉机器人的webhook的地址发送消息 编写告警配置文件 global: resolve_timeout: 5m route: receiver: webhook group_wait: 30s group_interval: 5m repeat_interval: 5m group_by: [alertname] routes: - receiver: webhook group_wait: 10s receivers: - name: webhook webhook_configs: - url: http://公网ip:8060/dingtalk/webhook1/send send_resolved: true 编写警告规则 # 运行这条命令 vim /tmp/redis.rules # 添加代码 groups: - name: redis rules: - alert: redis expr: up{job=\"redis\"} == 0 for: 15s labels: severity: 1 team: node annotations: summary: \"Redis服务在您的帮助下已经成功死机，老板限你一小时之内把它救回来。\" 修改普罗米修斯的配置文件，将告警设置配置好 # 执行这条命令 vim /tmp/prometheus.yml # 追加这些代码 alerting: alertmanagers: - static_configs: - targets: - 公网ip:9093 rule_files: - \"/etc/prometheus/redis.rules\" 重启普罗米修斯服务，我们需要再次挂载一个redis.rules。 docker run -d -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml -v /tmp/redis.rules:/etc/prometheus/redis.rules prom/prometheus:latest 启动告警模块 docker run -d --name alertmanager -p 9093:9093 -v /tmp/alertmanager.yml:/etc/alertmanager/alertmanager.yml prom/alertmanager:latest 启动钉钉插件 docker run -d -p 8060:8060 --name webhook timonwong/prometheus-webhook-dingtalk --ding.profile=\"webhook1=https://oapi.dingtalk.com/robot/send?access_token=钉钉token\" 然后访问一下http://公网ip:9090/rules 然后我们关闭redis_exporter，来模拟redis宕机 docker stop redis_exporter 普罗米修斯的监控立刻就发现了问题 然后可以看到，机器人会发送一个消息 ok，完毕 Grafana可视化界面这是一个很酷炫的可视化界面，可以将我们的监控界面导入到里面，隔壁小孩崇拜的很 首先拉取镜像： sudo docker pull grafana/grafana 然后启动 sudo docker run -d -p 3000:3000 --name grafana grafana/grafana 登录http://公网ip:3000 默认账号和密码都是admin 写入公网IP和端口 ok完美，莫得问题","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"},{"name":"MobaXerm","slug":"MobaXerm","permalink":"http://godhearing.cn/tags/MobaXerm/"}],"author":"天听"},{"title":"ubuntu系统下安装mysql8.0以及Django与mysql交互的问题","slug":"ubuntu系统下安装mysql8.0以及Django与mysql交互的问题","date":"2020-05-14T16:00:00.000Z","updated":"2020-05-15T07:33:00.000Z","comments":true,"path":"2020/05/15/ubuntu-xi-tong-xia-an-zhuang-mysql8.0-yi-ji-django-yu-mysql-jiao-hu-de-wen-ti/","link":"","permalink":"http://godhearing.cn/2020/05/15/ubuntu-xi-tong-xia-an-zhuang-mysql8.0-yi-ji-django-yu-mysql-jiao-hu-de-wen-ti/","excerpt":"","text":"Django 运行时出错总结一、利用mysql作为数据库时，会报的错：#macos pip install Django #version==3.1 pip install PyMySQL #Pure Python MySQL Driver version==0.10.0 brew install mysql pip install mysqlclient # version==2.0.1 直接运行会报错: version_info, _mysql.version_info, _mysql.__file__ NameError: name '_mysql' is not defined 原因：Django连接MySQL时默认使用MySQLdb驱动，但MySQLdb不支持Python3，因此这里将MySQL驱动设置为pymysql 解决方法：需要在Django项目的__init__.py文件添加以下 import pymysql pymysql.install_as_MySQLdb() 此时，仍会报错： 'mysqlclient 1.4.0 or newer is required; you have 0.10.0.' 是因为将MySQL驱动设置为pymysql，MySQLdb的版本为0.10.0，所以不够，但实际上不影响使用。因此将报错的位置注释掉就可以了。 出错文件在：/Users/xinzipanghuang/Anaconda/anaconda3/lib/python3.7/site-packages/django/db/backends/mysql/base.py 注释掉这几句: if version &lt; (1, 4, 0): raise ImproperlyConfigured('mysqlclient 1.4.0 or newer is required; you have %s.' % Database.__version__) 即可。 二、运行时可能出现的问题1、python manage.py runserver，打开时产生的链接会报错： You're accessing the development server over HTTPS, but it only supports HTTP. 浏览器会出现：This site can’t provide a secure connection 解决方法：setting.py中INSTALLED_APPS添加'sslserver';运行命令改为:python manage.py runsslserver 2、pycharm可以新建一个Django项目，做个小APP改动几个地方就可以。 新建views.py在app目录下，定义返回函数（html内容）同时在templates目录下，添加返回的html。修改urls.py中urlpatterns,添加刚加入的views.py函数 3、css和js路径设置问题 Django静态style文件会放在static文件夹下，需要在setting.py中添加： STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static/')] html文件也需要相对应改为/static/***.css ubuntu升级mysql1 升级前的检查准备在终端运行命令： mysqlcheck -u root -p --all-databases --check-upgrade 返回结果如果全部都是OK, 就表示检查没问题，进入下一步。如果有错误，看这里：https://dev.mysql.com/doc/refman/8.0/en/upgrade-prerequisites.html 2 备份备份数据库之前都没有使用过备份数据库，参考的这个命令： mkdir mysql-backup cd mysql-backup mysqldump --all-databases --single-transaction --quick --lock-tables=false &gt; full-backup-$(date +%F).sql -u root -p 该命令各个参数解释： –all-databases ： 表示导出全部数据库 –single-transaction ： 这个不太懂，网上的解释也没看明白，需要补充补充数据库基础了。。 –quick ： 导出时不会将数据加载到缓存，而是直接输出。默认就是启用状态。可以使用–skip-quick 取消该选项。 –lock-tables：开始导出前，锁定所有表。当导出多个数据库时，–lock-tables分别为每个数据库锁定表。 $(date+%F)表示获取当前时间，在Linux Shell中： date //获取时间 2019年 08月 08日 星期四 11:40:20 CST date +%F //获取日期 2019-08-08 date +%s //获取时间戳 1565235736 –user, –u指定连接的用户名 –password, –p连接数据库密码 备份MySQL设置 cd mysql-backup sudo tar zcvf mysql_config.tar.gz /etc/mysql/ 3 添加MySQL APT下载mysql-apt-config_0.8.13-1_all.deb：https://dev.mysql.com/downloads/repo/apt/ 安装： sudo dpkg -i mysql-apt-config_0.8.13-1_all.deb sudo apt-get update 4 安装新版的MySQLservice mysql stop # 先停了MySQL service sudo apt-get install mysql-server sudo service mysql start # 开启MySQL service 最后检查下升级后的版本： mysql -V 显示： mysql Ver 8.0.17 for Linux on x86_64 (MySQL Community Server - GPL) 已经升级成功。","categories":[{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/categories/mysql/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"}],"author":"天听"},{"title":"PostgrelSQL","slug":"PostgrelSQL","date":"2020-04-13T06:09:00.000Z","updated":"2021-03-16T13:04:24.658Z","comments":true,"path":"2020/04/13/postgrelsql/","link":"","permalink":"http://godhearing.cn/2020/04/13/postgrelsql/","excerpt":"","text":"前言 自从mysql被oracle收购以后，PostgrelSQL逐渐成为开源关系型数据库的首选。并且就算mysql没有被收购，它所暴露出的问题也很难让人不追求新的数据库 比如，Emoji表情坑随后推出这个utfmb4 与时俱进之下，我们不采用普通的安装方式，而使用Docker来进行安装使用 Docker安装PostgrelSQL第一步，拉取镜像 docker pull postgres:11.1 然后，启动 docker run -d --name dev-postgres -e POSTGRES_PASSWORD=root -p 6432:5432 postgres:11.1 这里的POSTGRES_PASSWORD是PostgrelSQL的用户密码，自己制定一个就可以了。 进入命令行操作一下 docker exec -it dev-postgres bash psql -h localhost -U postgres 这样就进入了容器内部的命令行 下面来介绍一下PostgrelSQL的一些命令 PostgrelSQL命令数据库操作建立数据库 create database 库名 使用数据库 \\c 库名 查看所有数据库 \\l 查看所有数据库，并且展示详细信息 \\l+ 删除数据库 drop database 库名 表操作创建表 create table 表名(字段 类型 属性) 例子： create table empsal( id int4 not null, name varchar(20) not null, primary key(id) )with(oids=FALSE); primary key代表的是主键，和其他的关系型数据库一致，不过，他可以建立一个或者多个主键， 查看所有表 \\d 删除表 drop table 表名 更新表结构 alter table 表名 add 字段名 类型 属性; 例子： alter table article add email varchar(40); 查看表结构 \\d 表名 PostgreSQL 模式（schema） PostgreSQL 模式（SCHEMA）可以看着是一个表的集合。一个模式可以包含视图、索引、数据类型、函数和操作符等。相同的对象名称可以被用于不同的模式中而不会出现冲突，例如 schema1 和 myschema 都可以包含名为 mytable 的表。 使用模式的优势： 允许多个用户使用一个数据库并且不会互相干扰。 将数据库对象组织成逻辑组以便更容易管理。 第三方应用的对象可以放在独立的模式中，这样它们就不会与其他对象的名称发生冲突。 模式类似于操作系统层的目录，但是模式不能嵌套。 创建模式 create schema myschema; 在指定模式里创建表，如： CREATE TABLE myschema.user ( ... ); 删除一个空的schema，如： drop schema myschema; 删除一个模式以及模式里面所有的对象，如： drop schema myschema CASCADE; 在创建表时，如果没有指定schema，则会自动被归属到一个叫做public的模式中，也就是公共的。 所以，我们创建表时，这两种创建方式是一样的效果。 create table tablename(); create table public.tablename(); 模式权限的更改 缺省时，用户看不到模式中不属于他们所有的对象。为了让他们看得见，模式的所有者需要在模式上赋予 USAGE 权限。为了让用户使用模式中的对象，我们可能需要赋予额外的权限， 只要是适合该对象的。 用户也可以允许在别人的模式里创建对象。要允许这么做， 我们需要赋予在该模式上的 CREATE 权限。 请注意，缺省时，每个人都在 public 模式上 有 CREATE 权限。这样就允许所有可以连接到 指定数据库上的用户在这里创建对象。如果你不允许这么做， 你可以撤销这个权限： REVOKE create on schema public from PUBLIC; 我们可以通过GRANT和REVOKE命令来分别的添加或撤销模式中的相应权限。这条命令中，我们移除了public模式中的CREATE权限。注意，我们使用了两个public，分别为一个大写和一个小写。小写的public指的是模式，实际使用中可以被替换为数据库中其他任意有效的模式名。而大写的PUBLIC是一个特殊的关键字，代表了all users。实际使用中可以被替换为一个特定的角色名或者以逗号分隔的角色名列表。使用\\dn+命令查询修改后的权限，发现权限信息中第二行里的C权限已经被移除，证明之前的REVOKE命令是有效的。现在，除了postgres用户以外的用户将不再能在public模式中创建表、视图或其他对象。 数据操作插入数据 insert into 表名 values(对应数据) 也可以根据字段来进行添加 insert into 表名(字段名) values(对应数据) 这里要提一点，如果想使用可视化工具来操作PostgrelSQL的话，(例如navicat)，无法像mysql一样手动设置属性自增长，而是使用序列的形式来实现自增长 创建序列 create sequence serial start 1 这句话的意思是，创建一个名为serial的序列，从1开始计数 查看结构 随后，将需要设置的字段的默认值设为序列增长即可 alter table 表名 alter column 要自增长的字段 set default nextval(序列名) 例子： ALTER TABLE \"public\".\"article\" ALTER COLUMN \"id\" SET DEFAULT nextval('serial'); 删除数据 delete from 表名 where 条件 例子： delete from user where name='李四' 更新数据 update 表名 set 修改后的字段对应的数据 where 条件 例子： update user set name='张三' where name='李四' 窗口函数 聚合函数相信大家都用过，像sum、min、avg等，一般是和group by搭配使用，窗口函数和聚合函数类似，也是计算一些行集合的数据，和常规的聚合函数不同的是，窗口函数不会将参与计算的行合并成一行输出，而是将计算出来的结果带回到了计算行上。 注意，窗口函数必须和over字句搭配使用，over字句包含partition by和order by两部分，分别用来分组和确定组内输出顺序，他们都是可选的 如果两个都省略，整个表会被作为一个分组 我们来举个例子吧，假设我们有一张表，分别有部门，员工，工资三个字段 假设我们要查询每个部门的平均工资以及所有员工的对比，在传统做法下，我们需要先通过group by分组，再用表连接统计一下上面的结果 而使用窗口函数，可以简化很多 而使用order by的话，会造成怎样的结果呢 可以看到，第一个是人事部门的平均工资，第二个，是研发和人事的平均工资，第三个，就是所有部门的平均工资。使用的时候要特别注意。 python使用PostgrelSQL和使用mysql一样，先装三方库 pip3 isntall psycopg2 然后和mysql一样使用 import psycopg2.extras conn = psycopg2.connect(host='localhost', port=6432, user='postgres', password='root', database='mytest') cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor) cursor.execute('SELECT * FROM empsal;') result = cursor.fetchone() print(result) ok，完成","categories":[{"name":"PostgrelSQL","slug":"PostgrelSQL","permalink":"http://godhearing.cn/categories/PostgrelSQL/"}],"tags":[{"name":"PostgrelSQL","slug":"PostgrelSQL","permalink":"http://godhearing.cn/tags/PostgrelSQL/"}],"author":"天听"},{"title":"WebSocket","slug":"WebSocket","date":"2020-04-09T13:09:00.000Z","updated":"2020-04-09T13:09:00.000Z","comments":true,"path":"2020/04/09/websocket/","link":"","permalink":"http://godhearing.cn/2020/04/09/websocket/","excerpt":"","text":"1.什么是websocket websocket是HTML5出的协议，和HTTP协议没有什么关系，它的目的在于，在浏览器和服务器之间建立一个不受限制的双向实时通信的通道，比如服务器可以任意时刻向浏览器主动的推送消息 它基于TCP，先通过HTTP/HTTPS协议发起一条特殊的HTTP请求进行握手后创建一个用于交换数据的TCP连接 他们是并行的关系 HTTP和WebScoket的区别 HTTP不支持常久的连接(长连接\\循环连接等不算) WebScket和服务器只要一次握手动作，HTTP协议每次链接都需要三次握手才能发送消息 服务器和浏览器都可以在任意时刻相互的推送消息，HTTP的缺点就在于此，浏览器不主动请求，服务器就没法给浏览器发送数据 HTTP协议每次请求都要发送请求头，websocket一旦建立连接，之后请求都不用发送请求头 虽然HTTP可以用轮询或者comet机制实现WebSocket的功能，但是，每次轮询，就算是再快，也需要间隔时间，这样就造成了它的实时性不够，而且， 频繁的发送请求，会给服务器造成很大的压力，一般情况下，没人会这么做 轮询：js启动定时发送请求，间隔性的请求服务器是否有新的数据 comet：请求没有超过预定时间或者没有返回数据，一直保持连接状态，等有了数据再进行推送 comet虽然是实时性够了，但是，长时间挂起线程，会浪费服务器的资源，如果长时间没有数据，链路上的任何一个网关都可能关闭这个链接。 所以，会造成两个后果，要么，你不知道什么时候断开了，要么，你就定期发ping，检查连接是否正常 2.什么是三次握手，四次挥手这是基于TCP协议的规则，建立TCP需要三次握手才能建立，而断开连接则需要四次挥手 三次握手第一次握手，客户端发送一个待SYN表之的TCP报文到服务器 第二次握手，服务器回应客户端，发送一个带有SYN和ACK标志的报文，表示它对客户端SYN报文的回应，同时询问客户端是否准备好进行数据通讯 第三次握手，客户端必须再次回应服务器一个ACK报文 四次挥手第一次挥手，客户端发送一个FIN，用来关闭客户到服务器的数据传送 第二次挥手，服务器收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。 第三次挥手，服务器关闭客户端的连接，发送一个FIN给客户端 第四次挥手，客户段发回ACK报文确认，并将确认序号设置为收到序号加1 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 3.WebSocket的使用1.首先需要一个dwebsocket依赖，安装非常简单，直接pip install dwebsocket 2.在settings中需要注册加载，INSTALLED_APPS注册dwebsocket dwebsocket在django3.1之后自动加载，在这之前，都需要进行注册这一步骤 3.导包from dwebsocket.decorators import accept_websocket 4.写一个视图函数，需要用**@accept_websocket**，这样，声明了他是一个webscoket连接，这只是链接，不是发送消息，这里，定义了一个公共字典clients，一会儿要用，记住它 clients = {{ @accept_websocket def websocketlink(request): if request.is_websocket(): userid = uuid.uuid1() while True: message = request.websocket.wait() if not message: break else: clients[userid] = request.websocket dwebsocket有两种装饰器：require_websocket和accept_websocekt使用require_websocket装饰器会导致视图函数无法接收导致正常的http请求，一般情况使用accept_websocket方式就可以了 5.在前端，发送一个连接，注意，此时后端的链接不是http，而是ws // websocket连接 if('WebSocket' in window){ // 生成websocket链接 var ws = new WebSocket('ws://127.0.0.1:8000/websocketlink/'); // 发送链接 ws.onopen = function(){ ws.send('你好啊世界'); { // 发送消息 ws.onmessage=(evt)=>{ // 将获取信息打印 var received_msg = evt.data; this.$notification.open({ message:received_msg {) ws.onclose = function(){ console.log('链接已关闭') { { 6.推送消息接口，一个函数视图举例： def sendmessage(request): # 获取消息 msg = request.GET.get('msg') # 循环公共字典clients，遍历内部所有的链接， for client in clients: clients['client'].send(msg.encode('utf-8')) return HttpResponse({'message':\"ok\"{) 然后，链接成功后，再新打开一个页面，访问这个sendmessage方法，就可以在你的前端页面看到你发送的数据啦","categories":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"Dockerhub部署Centos+Nginx+gunicorn+django独立架构","slug":"Dockerhub部署Centos+Nginx+gunicorn+django独立架构","date":"2020-04-07T14:28:00.000Z","updated":"2020-04-07T14:28:00.000Z","comments":true,"path":"2020/04/07/dockerhub-bu-shu-centos-nginx-gunicorn-django-du-li-jia-gou/","link":"","permalink":"http://godhearing.cn/2020/04/07/dockerhub-bu-shu-centos-nginx-gunicorn-django-du-li-jia-gou/","excerpt":"","text":"前言 在win10简单的使用Docker打包那纯属玩蛇，真正的部署还是要在Centos，这里为什么要用gunicorn呢，因为uwsgi还要编写配置文件，比较麻烦，所以我偷个懒使用gunicorn，日后有时间再讲解uwsgi Nginx呢，就好像一层缓冲墙，使用Nginx来反向代理，就相当于用户不能直接访问服务器，而是先访问Nginx，由Nginx来负责分发。 Nginx的默认策略就是轮询，按照顺序发送，如果服务器要横向扩展，按照顺序一个个的发送，如果服务器配置高低不一，可以将策略改为加权。根据服务器的配置来设置阈值，增加容灾性。 开始操作 首先呢，就是将项目打包，具体流程不再赘述了，请观阅拙作：Docker部署Django项目 然后，注册Dockerhub，类似gitee和github的仓库：[地址][https://hub.docker.com/] 创建仓库 然后呢，我们需要对本地的镜像名重命名 docker tag 本地镜像名 仓库拥有者/仓库名 例如： docker tag mydjango godhearing/mydjango 之后在命令行输入命令 docker login 使用dockerhub的账号密码登录 这里我是登录过了，输入账号密码后出来这个就是登录成功了 然后将本地镜像上传到dockerhub中 docker push godhearing/mydjango 然后登录你的云服务器，然后安装docker服务 #升级yum sudo yum update #卸载旧版本docker sudo yum remove docker docker-common docker-selinux docker-engine #安装依赖 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 #设置源 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sudo yum makecache fast #安装docker sudo yum install docker-ce #启动服务 sudo systemctl start docker 安装完成后键入 docker -v 如果出来版本号就说明没有问题 然后从dockerhub把镜像拉下来 采用后台守护进程模式起服务 sudo docker run -d -p 8000:8000 --name 自定义镜像名 镜像名 例如： sudo docker run -d -p 8000:8000 --name testdjango godhearing/mydjango 可以使用docker ps来查看是否运行成功 然后访问一下 ok，没有问题，额。。。也不能叫没有问题，因为我的mysql是本地的，没有更改，不过也还可以啦，总之，项目呢，是部署的没有问题了。不要在意这些细节 然后呢，我们安装Nginx服务 docker pull nginx 然后启动 docker run -d -p 80:80 nginx 现在，我们将运行Nginx容器里的配置文件copy到宿主机里面(拉到本地) docker cp 容器id:/etc/nginx/conf.d/default.conf /root/default.conf 复制出来之后，输入命令修改这个nginx配置 vim /root/default.conf 将Gunicorn配置加到里面 server { listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { proxy_pass http://你的服务器公网ip:5000; # 这里是指向 gunicorn host 的服务地址 proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } 修改完配置文件之后，关掉运行的nginx服务容器，并且删掉它 docker stop 容器id docker rm $(docker ps -a -q) 然后再次启动nginx，通过-v来进行挂载，就是将宿主机的文件替换Docker容器内部的文件，达到修改的效果 docker run --name mynginx -d -p 80:80 -v /root/default.conf:/etc/nginx/conf.d/default.conf nginx 重新启动成功后，访问服务器ip即可 ok完毕，比原始命令行shell安装不知快了多少倍","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"Docker-compons搭建redis集群-哨兵模式","slug":"Docker-compons搭建redis集群-哨兵模式","date":"2020-04-01T14:32:00.000Z","updated":"2020-04-01T14:32:00.000Z","comments":true,"path":"2020/04/01/docker-compons-da-jian-redis-ji-qun-shao-bing-mo-shi/","link":"","permalink":"http://godhearing.cn/2020/04/01/docker-compons-da-jian-redis-ji-qun-shao-bing-mo-shi/","excerpt":"","text":"redis集群 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施。 它有几个好处： 高性能，集群的性能和单节点的性能是同一个级别，并不会降低 高可用，在使用集群承担高负载的同时，也能进行高可用的容灾机制 易扩展，向集群添加新的节点，或者移除节点，都是透明化操作，不需要停机 原生，不需要其他代理或工具，和单机redis几乎完全兼容 何为哨兵模式 哨兵模式，是监控redis集群状态的工具，就好像一个监控器，监控着主从服务器是否健康，是redis的高可用性解决方案，当某个主服务器宕机时，它会选举出一个最优的从服务器来充当主服务器，这样，用户体验不会改变 需要注意的是，虽然哨兵模式支持创建多个，但是一般建议采取奇数台，道理很简单，假如选举的两个从服务器票数一样，就会使场面瞬间尬住，主从间的切换有可能会崩掉 还有一点是，哨兵也不是不会出错的，如果哨兵出错，没有下一个哨兵出战，那么程序有可能会出错 使用docker创建镜像 建一个文件夹，进入项目文件夹，再建一个专门存放哨兵的脚本sentinel，cd sentinel 建立sentinel.conf配置文件 sentinel monitor mymaster redis-master 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 5000 该配置的意思是，监控主服务器的6379端口并启动两个实例，如果5秒内没有收到主节点的心跳，则哨兵就认为主节点宕机了，默认是30秒，如果5秒以上连接不上主库同步，则在5秒后进行选举，对其他从服务器进行角色转换 建立sentinel-entypoint.sh脚本 #!/bin/sh sed -i \"s/$SENTINEL_QUORUM/$SENTINEL_QUORUM/g\" /etc/redis/sentinel.conf sed -i \"s/$SENTINEL_DOWN_AFTER/$SENTINEL_DOWN_AFTER/g\" /etc/redis/sentinel.conf sed -i \"s/$SENTINEL_FAILOVER/$SENTINEL_FAILOVER/g\" /etc/redis/sentinel.conf exec docker-entrypoint.sh redis-server /etc/redis/sentinel.conf --sentinel 该脚本文件会对配置文件进行同步，用来启动哨兵 建立Dockerfile指定基础镜像，同时拷贝配置文件到镜像内部 FROM redis EXPOSE 26379 ADD sentinel.conf /etc/redis/sentinel.conf RUN chown redis:redis /etc/redis/sentinel.conf COPY sentinel-entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/sentinel-entrypoint.sh ENTRYPOINT [\"sentinel-entrypoint.sh\"] 最后退到项目的根目录，建docker-compose.yml配置文件 master: image: redis ports: - \"6379:6379\" slave1: image: redis command: redis-server --slaveof redis-master 6379 links: - master:redis-master ports: - \"6380:6379\" slave2: image: redis command: redis-server --slaveof redis-master 6379 links: - master:redis-master ports: - \"6381:6379\" sentinel1: build: sentinel environment: - SENTINEL_DOWN_AFTER=5000 - SENTINEL_FAILOVER=5000 links: - master:redis-master - slave1 sentinel2: build: sentinel environment: - SENTINEL_DOWN_AFTER=5000 - SENTINEL_FAILOVER=5000 links: - master:redis-master - slave2 意思是，我们起三台redis服务，分别跑在6379,6380,6381 ，一主两从，并且有两个哨兵实例来监控他们 在项目根目录下，启动服务： docker-compose up --force-recreate # 如果希望在后台运行可以加-d参数 测试一下，打开三个终端，分别进入redis-cli -p 6379，6380，6381在主服务器6379下，添加一个数据，然后分别get 测试哨兵手动停止主库的容器进程，来模拟宕机的情况 docker stop redissentinel_master_1 使用info命令查询服务器的角色，可以看到，原本是slave，现在成为了master","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://godhearing.cn/tags/Redis/"}],"author":"天听"},{"title":"WebHook机制","slug":"WebHook机制","date":"2020-03-17T12:00:00.000Z","updated":"2020-03-17T12:00:00.000Z","comments":true,"path":"2020/03/17/webhook-ji-zhi/","link":"","permalink":"http://godhearing.cn/2020/03/17/webhook-ji-zhi/","excerpt":"","text":"1.什么是web hook webhook是一个API概念，并且变得越来越流行，我们能用事件描述的事物越来越多，webhook的作用范围也就越大， webhook作为一个清凉的事件处理应用，正变得越来越有用 我们在网上做的工作，大部分其实就是事件，webhooks已经成为了连接系统的主要方式，不管是用户创建，支付成功，DockerHub镜像推送或者Git仓库上的合并请求，这些都是非常有用并且轻量级的共享信息的方式 简单来说，就是一种反向API，类似于触发器一样 打个比方，比如项目A要从项目B获取数据，通常需要项目B提供一个API接口，然后项目A去请求项目B的接口，获取数据 通过webhook机制，对客户端和服务端的模式进行逆转 继续新增加一个需求，项目A需要从项目B获取实时数据，如果按照传统做法，不停的去请求接口，就算是实现了这个功能，但是，效率和性能都非常的低 通过webhook机制，我们在项目B中加上一个触发器，在项目B每次创建新数据或者修改了某个数据，就会自动触发， 便会向项目B的hook地址进行请求，项目A收到项目B的请求，然后对数据进行处理 2.mysql外键的选择 所有的web开发都离不开性能和效率，我们就来聊聊mysql物理外键的性能问题 为何说外键有性能问题，让我们慢慢分析一下 数据库需要维护外键的内部管理，这样就给数据库增加了一些性能负担，尽管是些小负担，但是在数据量庞大的时候，和优化过的数据库差距也是很大的 外键等于把数据的一致性事务实现，全部交给数据库服务器完成，同样也会增加数据库服务器的压力 有了外键，当做一些设计外键字段的增删改等操作之后，需要触发相关操作去检查，从而不得不消耗资源 外键还会因为需要的请求对其他表内部加锁而容易出现死锁的情况 虽然很多人不推荐你使用物理外键，但你听到更多的是mysql，而不是SQL server或者其他，比较公认的是，他的外键设计得确实不怎么好，限制多功能不强大等等 还有，最重要的一点是，万一主键所在的表需要拆分，重构，那么，物理外键对应的表，也要进行一系列繁琐的操作，另一方面，数据库帮你保证级联关系，自己不保证思路清晰啊 所以，逻辑外键在业界是比较成熟的，不适用物理外键，我们也可以约定逻辑外键，不再数据库中声明，外键，只在程序中实现关联 那么，逻辑外键实现了物理外键的功能，这，才是开发人员的第一选择 说到这里，在数据库的策略上，我们也可以选择逻辑删除 逻辑删除，就是，只对表进行更新增加操作，不进行删除，不再使用的历史数据，定期的归档来减少压力","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"},{"name":"WebHook","slug":"WebHook","permalink":"http://godhearing.cn/tags/WebHook/"}],"author":"天听"},{"title":"手写Celery","slug":"手写Celery","date":"2020-02-27T13:42:00.000Z","updated":"2020-02-27T13:42:00.000Z","comments":true,"path":"2020/02/27/shou-xie-celery/","link":"","permalink":"http://godhearing.cn/2020/02/27/shou-xie-celery/","excerpt":"","text":"前言 众所周知，celery是一个基于python开发的任务消息队列，轻松实现任务的异步处理，如果对celery不了解，请观阅之前一篇文章Celery 关于celery，它的底层也不是很难，假如，我们在一个需要celery的场景下，例如，我们发送邮件，使用celery是不是有些过重了。这时候，与其调用celery，还不如自己动手去实现一下，这样既轻量，又好用 我们需要用到的东西，第一就是多线程，因为我们要进行异步操作，使用多线程模拟是最为合理的，然后就是redis，当然了，用List也是可以的，但是我们为了能够贴近真实操作，使用redis中的列表模拟 实现celery 要知道，celery的本质，是队列，所以，我们手动写一个队列 class MyQueue: def __init__(self,kename:str,**redis_kwargs): # 链接redis decode_response的作用是，字符串不会转换成bytes self.__db = redis.Redis(**redis_kwargs,decode_responses=True) self.key = kename self.queue = [] # 添加数据 def push(self,x:str) -> None: self.__db.rpush(self.key,x) # 删除数据 def pop(self) -> int: return self.queue.pop(0) # 获取数据 def peek(self) -> str: return self.__db.lpop(self.key) # 判断是否执行完毕 def empty(self)->bool: return self.__db.llen(self.key) 实例化 task_queue = MyQueue('myqueue') 我们就以模拟发送邮件为生产者 result = ['1111111@qq.com','22222@qq.com','3333333@qq.com','4444444444@qq.com','55555555555@qq.com'] for i in result: if i[0]: task_queue.push(i[0]) 然后，我们使用多线程来模拟消费者 def task(): # 如果队列空了，证明任务完成 while task_queue.empty() != 0: # 取出任务，模拟消费 print(task_queue.peek()) time.sleep(1) return '执行完毕' if __name__ == '__main__': t1 = threading.Thread(target=task) t2 = threading.Thread(target=task) t1.start() t2.start() t1.join() t2.join() 需要几个消费者，就可以开启多个线程，当然了，也可以通过继承threading.Thread创建新的子类，实例化后调用start方法启动新线程，即它调用了线程的run()方法 class Thread_test(threading.Thread): def __init__(self,queue_task): threading.Thread.__init__(self) self.queue = queue_task def run(self) -> None: print(self.queue) if __name__ == '__main__': # 队列为空，即为停止 while task_queue.empty() != 0: # 创建线程 for i in range(2): a = Thread_test(task_queue.peek()) a.start() time.sleep(1) 如此，我们便简单的实现了celery的底层 总结，任何名字听起来高大上的工具，不要因为它的触不可及而放弃，我们也可以用很简单的方式实现它们该有的功能。","categories":[{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/categories/Celery/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/tags/Celery/"}],"author":"天听"},{"title":"Vue组件","slug":"Vue组件","date":"2019-11-30T12:28:00.000Z","updated":"2019-11-30T12:28:00.000Z","comments":true,"path":"2019/11/30/vue-zu-jian/","link":"","permalink":"http://godhearing.cn/2019/11/30/vue-zu-jian/","excerpt":"","text":"1.组件定义 定义组件并引用 父组件向子组件传值 子组件向父组件传值 组件间传值：vuex 1.1 什么是组件 Html中有组件，是一段可以被复用的结构代码 Css中有组件，是一段可以被复用的样式 Js中有组件，是一段可以被复用的功能 Vue中的组件，指的就是一个模块，是一个独立的，完整的(包含html，cssm，js等)，可以直接拿来用 1.2 组件特性 组件的实例化对象，跟vue实例化对象一样，因此，我们也可以将vue实例化对象看成组件 组件间是独立的，因此数据要独立存储，方法要独立定义， **彼此间不能共享 **。 2.父组件向子组件传值2.1 components/Child.vue 定义子组件&lt;template> &lt;div style=\"color: red\"> &lt;h1>子组件内容&lt;/h1> &lt;p>{{data{{&lt;/p> &lt;/div> &lt;/template> &lt;script> export default { props: ['data'], // 接收父组件给子组件定义的属性 { &lt;/script> 子组件要使用父组件的数据，只需要一步，在 props中接收父组件的属性 2.2 components/Father.vue 定义父组件&lt;template> &lt;div> &lt;h1>父组件内容&lt;/h1> 父组件显示:{{msg{{ &lt;!--3.第三步：把父组件的某一个属性传递给子组件--> &lt;Child :data='msg' >&lt;/Child> &lt;/div> &lt;/template> &lt;script> // @指定的是src路径 import Child from '@/components/Child' // 1.第一步：在父组件中导入子组件 export default { // 2.第二步：父组件中注册子组件 components: { Child {, data() { return { msg: '父组件的信息' { {, methods: { { { &lt;/script> components注册子组件 2.3 router/index.js 中注册路由import Father from '@/components/Father' // @修饰符指的是 src目录 export default new Router({ routes: [{ path: '/component', name: 'Father', component: Father {,] {) 子组件中可以通过 定义 props 属性来接收父组件的数据 3.子组件向父组件传值3.1 components/Child.vue 子组件通过触发方法, 向父组件传值&lt;template> &lt;div> &lt;h1>子组件页面&lt;/h1> &lt;span>子组件{{data{{&lt;/span> &lt;button @click=\"f1\">调用父&lt;/button> &lt;/div> &lt;/template> &lt;script> export default { //用props接收父组件的属性 props:['data'], methods:{ // 接收父组件给子组件定义的属性 f1(){ //1.子组件调用父组件方法，并传值 // $emit 触发当前实例上的事件，也可以简单的理解为触发父组件上的事件（向上冒泡） this.$emit('changeMsg','子的信息') { { { &lt;/script> 3.2 components/Father.vue给子组件添加事件及事件处理方法&lt;template> &lt;div> &lt;h1>父组件页面&lt;/h1> {{msg{{ &lt;Child :data='msg' @changeMsg='f2'>&lt;/Child> &lt;/div> &lt;/template> &lt;script> import Child from '@/components/Child' export default { components:{ Child {, data(){ return{ msg:'父的数据' { {, methods:{ // 在父组件中定义一个f2方法，可以在子组件中触发并传值给父组件 f2(data){ // data接收是子组件中传递的数据 this.msg = data// 更新父组件的内容 { { { &lt;/script>","categories":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"vue优化之代码层面的优化","slug":"vue优化之代码层面的优化","date":"2019-11-19T06:38:00.000Z","updated":"2019-11-19T06:38:00.000Z","comments":true,"path":"2019/11/19/vue-you-hua-zhi-dai-ma-ceng-mian-de-you-hua/","link":"","permalink":"http://godhearing.cn/2019/11/19/vue-you-hua-zhi-dai-ma-ceng-mian-de-you-hua/","excerpt":"","text":"Vue 框架通过数据双向绑定和虚拟 DOM 技术，帮我们处理了前端开发中最脏最累的 DOM 操作部分， 我们不再需要去考虑如何操作 DOM 以及如何最高效地操作 DOM；但 Vue 项目中仍然存在项目首屏优化、Webpack 编译配置优化等问题，所以我们仍然需要去关注 Vue 项目性能方面的优化，使项目具有更高效的性能、更好的用户体验。 代码层面的优化在vue中，有很多的功能相似，有略微区别的代码，正确的在使用场景下使用适合它的元素，会让代码运行的更加丝滑 1.1 v-if和v-show使用场景v-if 是 真正 的条件渲染，因为它会确保在切换过程中条件块内的事件监听器和子组件适当地被销毁和重建；是惰性的：如果在初始渲染时条件为假，则什么也不做——直到条件第一次变为真时，才会开始渲染条件块。 v-show 就简单得多， 不管初始条件是什么，元素总是会被渲染，并且只是简单地基于 CSS 的 display 属性进行切换。 所以，v-if 适用于在运行时很少改变条件，不需要频繁切换条件的场景；v-show 则适用于需要非常频繁切换条件的场景。 比如，你的电脑 v-if就等于是你完全的将他关闭了，需要用时，再将他打开 v-show相当于是睡眠模式，他始终都开着，只是你看不到 1.2 computed和watch使用场景computed： 是计算属性，依赖其它属性值，并且 computed 的值有缓存，只有它依赖的属性值发生改变，下一次获取 computed 的值时才会重新计算 computed 的值； 简单来说就是，对计算结果会进行缓存，如果原始数据不改变，不需要重新计算，直接从缓存中读取结果 但是，方法必须有返回值，减少计算量，牺牲空间，来换取时间，对一些复杂的运算，使用计算属性会非常的有效率 假如，你经常的在一家商店购买十箱苹果， 在普通的函数计算中，需要每次取出苹果价格和数量，在计算属性中，只要你价格和数量没变化，就不需要计算，只需要从缓存中取值就好了 watch： 更多的是「观察」的作用，类似于某些数据的监听回调 ，每当监听的数据变化时都会执行回调进行后续操作； 监听属性简单来说就是数据一旦发生改变，会自动触发执行，watch属性中的对应的函数 如果函数中，有两个参数 a和b，a就是修改之后的新值，b是修改之前的旧值 当我们需要在数据变化时执行异步或开销较大的操作时，应该使用 watch，使用 watch 选项允许我们执行异步操作 ( 访问一个 API )，限制我们执行该操作的频率，并在我们得到最终结果前，设置中间状态。这些都是计算属性无法做到的。 1.3 v-for遍历必须为item添加key，且避免同时使用v-if 在列表数据进行遍历渲染时，需要为每一项 item 设置唯一 key 值，方便 Vue内部机制精准找到该条列表数据。当 状态更新时，新的状态值和旧的状态值对比，较快地定位到这个循环 。 v-for 比 v-if 优先级高，如果每一次都需要遍历整个数组，将会影响速度，尤其是当之需要渲染很小一部分的时候，必要情况下应该替换成 computed 属性。 // 推荐 &lt;ul> &lt;li v-for=\"user in activeUsers\" :key=\"user.id\"> {{ user.name {{ &lt;/li> &lt;/ul> computed: { activeUsers: function () { return this.users.filter(function (user) { return user.isActive {) { { // 不推荐 &lt;ul> &lt;li v-for=\"user in users\" v-if=\"user.isActive\" :key=\"user.id\"> {{ user.name {{ &lt;/li> &lt;/ul> 1.4 长列表性能的优化 Vue 会通过 Object.defineProperty 对数据进行劫持，来实现视图响应数据的变化，然而有些时候我们的组件就是纯粹的数据展示，不会有任何改变，我们就不需要 Vue 来劫持我们的数据，在大量数据展示的情况下，这能够很明显的减少组件初始化的时间，那如何禁止 Vue 劫持我们的数据呢？可以通过 Object.freeze 方法来冻结一个对象，一旦被冻结的对象就再也不能被修改了。 export default { data: () => ({ users: {{ {), async created() { const users = await axios.get(\"/api/users\"); this.users = Object.freeze(users); { {; 1.5 事件的销毁 Vue 组件销毁时，会自动清理它与其它实例的连接，解绑它的全部指令及事件监听器，但是仅限于组件本身的事件。 如果在 js 内使用 addEventListene 等方式是不会自动销毁的，我们需要在组件销毁时手动移除这些事件的监听，以免造成内存泄露 created() { addEventListener('click', this.click, false) {, beforeDestroy() { removeEventListener('click', this.click, false) { 1.6 图片懒加载对于图片过多的页面，为了加速页面的加载速度，所以很多时候我们需要将页面内未出现在可视区域内的图片先不做加载，等到滚动到可视区域再去加载，这样对于页面加载性能上会有很大的提升，也提高了用户体验。我们在项目中使用 Vue 的 vue-lazyload 插件 安装插件 npm install vue-lazyload --save-dev 在入口文件main.js中引入并使用 import VueLazyload from 'vue-lazyload' 在vue中直接使用 Vue.use(VueLazyload) 或者添加自定义选项 Vue.use(VueLazyload, { preLoad: 1.3, error: 'dist/error.png', loading: 'dist/loading.gif', attempt: 1 {) 在 vue 文件中将 img 标签的 :src 属性直接改为 v-lazy ，从而将图片显示方式更改为懒加载显示 &lt;img v-lazy=\"/static/img/1.png\"> 1.7 提取公共代码 一个项目，不可能只有一个页面，而多个页面中，有相同资源的，重复加载会浪费用户的流量和服务器的成本，每个页面的加载资源太大，导致网页加载缓慢，影响用户体验 所以，我们需要提取公共的代码，抽离成单独的文件，以组件的方式在页面中灵活运用，组件间的应用，请参考这里","categories":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"支付宝支付","slug":"支付宝支付","date":"2019-09-13T14:32:00.000Z","updated":"2019-09-13T14:32:00.000Z","comments":true,"path":"2019/09/13/zhi-fu-bao-zhi-fu/","link":"","permalink":"http://godhearing.cn/2019/09/13/zhi-fu-bao-zhi-fu/","excerpt":"","text":"前言 微信与支付宝，国内无现金交易的两个巨头，在我们做项目的过程中，肯定少不了与这两个打交道，但是，涉及到钱的问题，稍微的不谨慎，就会造成不可磨灭的影响，所以，今天带来支付宝支付的攻略 openssl首先，安装一个openssl，地址 选择64位还是32位，看自己电脑 然后，在命令行输入openssl，进入openssl程序，然后分别执行以下命令： genrsa -out rsa_private_key.pem 2048 # 生成私钥 rsa -in rsa_private_key.pem -pubout -out rsa_public_key.pem # 生成公钥 这样就在当前目录生成了两个文件rsa_private_key.pem和rsa_public_key.pem，将这两个秘钥集成到项目中，就可以进行支付宝支付业务的处理了，当然也可以根据阿里提供的支付开放平台来进行生成，具体流程可自行google，并不算难 然后我们后台选择的是django，在Django中集成支付接口的前置操作就是需要安装alipay的sdk，顺便导入加密模块 pip install python-alipay-sdk pip install Crypto 然后将之前生成好的私钥和公钥(注意这里的公钥指的是支付宝公钥)，放入到项目目录中，改个名字好区分 app_public.txt # 应用公钥 app_private.txt # 应用私钥 alipay_public.txt # 支付宝公钥 其中，填写上自己对应的公钥与私钥，格式为： alipay_public.txt -----BEGIN PUBLIC KEY----- 密钥 -----END PUBLIC KEY----- app_public.txt -----BEGIN PUBLIC KEY----- 密钥 -----END PUBLIC KEY----- app_private.txt -----BEGIN RSA PRIVATE KEY----- 密钥 -----END RSA PRIVATE KEY----- 然后在settings中配置： ALIPAY_PUBLIC = os.path.join(BASE_DIR,'keys','alipay_public.txt') APP_PUBLIC = os.path.join(BASE_DIR,'keys','app_public.txt') APP_PRIVATE = os.path.join(BASE_DIR,'keys','app_private.txt') 注意路径 新建个pay.py文件 from datetime import datetime from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1_v1_5 from Crypto.Hash import SHA256 from urllib.parse import quote_plus from urllib.parse import urlparse, parse_qs from base64 import decodebytes, encodebytes import json import requests class AliPay(object): \"\"\" 支付宝支付接口(PC端支付接口) \"\"\" def __init__(self, appid, app_notify_url, app_private_key_path, alipay_public_key_path, return_url, debug=False): self.appid = appid self.app_notify_url = app_notify_url self.app_private_key_path = app_private_key_path self.app_private_key = None self.return_url = return_url with open(self.app_private_key_path) as fp: self.app_private_key = RSA.importKey(fp.read()) self.alipay_public_key_path = alipay_public_key_path with open(self.alipay_public_key_path) as fp: self.alipay_public_key = RSA.importKey(fp.read()) if debug is True: self.__gateway = \"https://openapi.alipaydev.com/gateway.do\" else: self.__gateway = \"https://openapi.alipay.com/gateway.do\" def direct_pay(self, subject, out_trade_no, total_amount, return_url=None, **kwargs): biz_content = { \"subject\": subject, \"out_trade_no\": out_trade_no, \"total_amount\": total_amount, \"product_code\": \"FAST_INSTANT_TRADE_PAY\", # \"qr_pay_mode\":4 } biz_content.update(kwargs) data = self.build_body(\"alipay.trade.page.pay\", biz_content, self.return_url) return self.sign_data(data) def build_body(self, method, biz_content, return_url=None): data = { \"app_id\": self.appid, \"method\": method, \"charset\": \"utf-8\", \"sign_type\": \"RSA2\", \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"version\": \"1.0\", \"biz_content\": biz_content } if return_url is not None: data[\"notify_url\"] = self.app_notify_url data[\"return_url\"] = self.return_url return data def sign_data(self, data): data.pop(\"sign\", None) # 排序后的字符串 unsigned_items = self.ordered_data(data) unsigned_string = \"&amp;\".join(\"{0}={1}\".format(k, v) for k, v in unsigned_items) sign = self.sign(unsigned_string.encode(\"utf-8\")) # ordered_items = self.ordered_data(data) quoted_string = \"&amp;\".join(\"{0}={1}\".format(k, quote_plus(v)) for k, v in unsigned_items) # 获得最终的订单信息字符串 signed_string = quoted_string + \"&amp;sign=\" + quote_plus(sign) return signed_string def ordered_data(self, data): complex_keys = [] for key, value in data.items(): if isinstance(value, dict): complex_keys.append(key) # 将字典类型的数据dump出来 for key in complex_keys: data[key] = json.dumps(data[key], separators=(',', ':')) return sorted([(k, v) for k, v in data.items()]) def sign(self, unsigned_string): # 开始计算签名 key = self.app_private_key signer = PKCS1_v1_5.new(key) signature = signer.sign(SHA256.new(unsigned_string)) # base64 编码，转换为unicode表示并移除回车 sign = encodebytes(signature).decode(\"utf8\").replace(\"\\n\", \"\") return sign def _verify(self, raw_content, signature): # 开始计算签名 key = self.alipay_public_key signer = PKCS1_v1_5.new(key) digest = SHA256.new() digest.update(raw_content.encode(\"utf8\")) if signer.verify(digest, decodebytes(signature.encode(\"utf8\"))): return True return False def verify(self, data, signature): if \"sign_type\" in data: sign_type = data.pop(\"sign_type\") # 排序后的字符串 unsigned_items = self.ordered_data(data) message = \"&amp;\".join(u\"{}={}\".format(k, v) for k, v in unsigned_items) return self._verify(message, signature) #请求退款接口 def api_alipay_trade_refund(self,refund_amount,out_trade_no=None,trade_no=None,**kwargs): #构造参数体 biz_content = { \"refund_amount\":refund_amount} #传递可选参数 biz_content.update(**kwargs) #判断使用站外订单还是支付宝订单 if out_trade_no: biz_content[\"out_trade_no\"] = out_trade_no if trade_no: biz_content[\"trade_no\"] = trade_no #构造支付接口地址 data = self.build_body(\"alipay.trade.refund\",biz_content) #构造url url = self.__gateway+\"?\" + self.sign_data(data) #请求接口 r = requests.get(url) html = r.content.decode(\"utf-8\") return html 在视图中： #导入支付基类 from testdjango.settings import BASE_DIR from testdjango import settings import time from django.shortcuts import render, redirect, HttpResponse from .pay import AliPay import os # 支付宝初始化 app_private_key_string = os.path.join(BASE_DIR,'keys','app_private.txt') alipay_public_key_string = os.path.join(BASE_DIR,'keys','alipay_public.txt') def get_order_code(): order_no = str(time.strftime('%Y%m%d%H%M%S', time.localtime(time.time()))) return order_no #初始化阿里支付对象 def get_ali_object(): app_id = \"2016102600762844\" # APPID （沙箱应用） # 支付完成后，跳转的地址。 return_url = \"http://localhost:8000/alipayreturn/\" alipay = AliPay( appid=app_id, app_notify_url=return_url, return_url=return_url, app_private_key_path=app_private_key_string, alipay_public_key_path=alipay_public_key_string, # 支付宝的公钥，验证支付宝回传消息使用，不是你自己的公钥 debug=True, # 默认False, ) return alipay def page1(request): # 根据当前用户的配置，生成URL，并跳转。 money = request.POST.get('money') alipay = get_ali_object() # 生成支付的url query_params = alipay.direct_pay( subject=\"test\", # 商品简单描述 out_trade_no=get_order_code(), total_amount=1, # 交易金额(单位: 元 保留俩位小数) ) pay_url = \"https://openapi.alipaydev.com/gateway.do?{0}\".format(query_params) # 支付宝网关地址（沙箱应用） print(pay_url) return redirect(pay_url) def alipay_return(request): alipay = get_ali_object() params = request.GET.dict() out_trade_no = request.GET.get(\"out_trade_no\") print(out_trade_no) sign = params.pop('sign', None) status = alipay.verify(params, sign) print('==================开始==================') print('GET验证', status) print('==================结束==================') return HttpResponse('支付成功') 搞完收工，访问一下的视图即可跳转到支付页面，然后回调这些记得和网页环境一致即可。 退款 退款业务，也是基于订单号。在之前的pay中已经添加过了，我们只需要增加一个视图即可 def refund(request): #实例化支付类 alipay = get_ali_object() #调用退款方法 order_string = alipay.api_alipay_trade_refund( #订单号，一定要注意，这是支付成功后返回的唯一订单号 out_trade_no=\"20201216102213\", #退款金额，注意精确到分，不要超过订单支付总金额 refund_amount=\"1.00\", #回调网址 notify_url='http://localhost:8000/alipayreturn' ) return HttpResponse(order_string) 以上就是支付宝与django的操作，感谢观看","categories":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"支付","slug":"支付","permalink":"http://godhearing.cn/tags/%E6%94%AF%E4%BB%98/"}],"author":"天听"},{"title":"验证码系列(邮箱、短信)","slug":"验证码系列","date":"2019-08-31T13:38:00.000Z","updated":"2019-08-31T13:38:00.000Z","comments":true,"path":"2019/08/31/yan-zheng-ma-xi-lie/","link":"","permalink":"http://godhearing.cn/2019/08/31/yan-zheng-ma-xi-lie/","excerpt":"","text":"python3利用腾讯云发送短信如何发送短信 首先，注册一个腾讯云的账号，然后点击这里 注册成功后，腾讯云会自动生成一个短信应用，如果没有默认应用，需要新建一下 记录一下应用的 appid 以及 appkey 一会要用到 配置短信的签名，用来限制短信接口的权限，防止被恶意调用 然后配置模板 好了，进入正题，下面安装腾讯云短信的sdkpip install qcloudsms_py 按照官网文档，准备必要的参数 # 短信应用 SDK AppID appid = 1400009099 # SDK AppID 以1400开头 # 短信应用 SDK AppKey appkey = \"9ff91d87c2cd7cd0ea762f141975d1df37481d48700d70ac37470aefc60f9bad\" # 需要发送短信的手机号码 phone_numbers = [\"21212313123\", \"12345678902\", \"12345678903\"] # 短信模板ID，需要在短信控制台中申请 template_id = 7839 # NOTE: 这里的模板 ID`7839` 只是示例，真实的模板 ID 需要在短信控制台中申请 # 签名 sms_sign = \"腾讯云\" # NOTE: 签名参数使用的是`签名内容`，而不是`签名ID`。这里的签名\"腾讯云\"只是示例，真实的签名需要在短信控制台中申请 指定模板ID单发短信 from qcloudsms_py import SmsSingleSender from qcloudsms_py.httpclient import HTTPError ssender = SmsSingleSender(appid, appkey) params = [\"5678\"] # 当模板没有参数时，`params = []`可以发送随机数，用来做短信验证码,如果指定为[\"5678\",'5']，则过期时间为5分钟 try: result = ssender.send_with_param(86, phone_numbers[0], template_id, params, sign=sms_sign, extend=\"\", ext=\"\") except HTTPError as e: print(e) except Exception as e: print(e) print(result) 如果要群发短信，phone_numbers不取0值，发送全部 点击发送短信验证码视图定义，获取手机号，这里，我是把上面的脚本封装成函数phone_s了，两个参数，分别为手机号和随机数验证码 class phoneView(APIView): def post(self,request): phone = request.POST.get('phone') # 随机数，用来做验证码 uuid = random.randint(1000,9999) # 调用发送短信的接口，传入手机号和随机数验证码 pwg = phone_s(phone,uuid) if pwg: # 连接redis redis_client = get_redis_connection('phone') redis_phone = redis_client.get(phone) # 防止频繁发送，如果redis中有，就不必再发 if redis_phone: return Response({'code': 1003, 'msg': '频繁发送'{) # 存入redis,邮箱为键，uuid为值 redis_client = get_redis_connection('phone') # 获取redis客户端 redis_client.setex(phone, 60 * 5, uuid) return Response({'code':1000,'msg':'发送成功'{) return Response({'code':1004,'msg':'发送失败'{) 至于验证，只需要从redis中取出验证码来比对即可 邮箱验证码发送邮箱验证码，我们使用的是QQ邮箱，打开QQ邮箱，打开设置，在里面找到 POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务 打开POP3/SMTP服务 获取到授权码之后，我们就可以直接可以开始 #定义参数 my_mail = \"你申请授权的邮箱\" #授权码 my_pass = \"授权码\" #定义发送邮件的方法 def mail(subject,content,mailaddr): #声明邮件对象 msg = MIMEText(content,'plain','utf-8') #设置发送方对象 msg['From'] = formataddr(['在线教育平台',my_mail]) #设置收件方对象 msg['To'] = formataddr(['尊敬的客户',mailaddr]) #设置标题 msg['Subject'] = subject #设置smtp服务器 server = smtplib.SMTP_SSL(\"smtp.qq.com\",465) #登录邮箱 server.login(my_mail,my_pass) #发送邮件 server.sendmail(my_mail,[mailaddr],msg.as_string()) #关闭smtp链接 server.quit() 然后定义视图 class Go_emailView(APIView): def post(self,request): # 验证邮箱有效性 email = request.POST.get('email',None) try: re.match(r'[a-zA-Z0-9]{0,19{@(qq|163|126)\\.(com|cn|net)$', email) except Exception as e: return Response({'code': 1002, 'msg': '电子邮箱不正确'{) # 查询redis中是否有这个数据，如果有，则不需要再次发送 # 验证邮箱验证码 redis_client = get_redis_connection('email') # 获取redis中的库 redis_email = redis_client.get(email) if redis_email: return Response({'code':1003,'msg':'频繁发送'{) uuid = random.randint(0,9999) try: # 调用发送邮件的函数 mail('龙潭技术博客验证', '您的验证码是{{，有限期为2分钟'.format(uuid), email) # 存入redis,邮箱为键，uuid为值 redis_client = get_redis_connection('email') # 获取redis客户端 redis_client.setex(email, 60 * 2, uuid) return Response({'code':1000,'msg':'发送成功'{) except Exception as e: print(e) return Response({'code':1001,'msg':'发送失败'{)","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"ABAC权限模型","slug":"ABAC权限模型","date":"2019-08-29T13:41:00.000Z","updated":"2019-08-29T13:41:00.000Z","comments":true,"path":"2019/08/29/abac-quan-xian-mo-xing/","link":"","permalink":"http://godhearing.cn/2019/08/29/abac-quan-xian-mo-xing/","excerpt":"","text":"ABAC权限模型，是基于属性的权限模型，ABAC通过动态计算一个或一组属性来满足某种条件来进行授权判断(可以编写简单的逻辑)，属性通常来说分为四类，用户属性(例如年龄，性别等)，环境属性(比如时间，地点)，操作属性(比如读取)，对象属性(如一篇文章，又称为资源属性),所以理论上能够实现非常灵活的控制权限,几乎能满足所有类型的需求 例如规则：“允许所有班主任在上课时间自由进出校门”这条规则，其中，“班主任”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。为了实现便捷的规则设置和规则判断执行，ABAC通常有配置文件（XML、YAML等）或DSL配合规则解析引擎使用。XACML（eXtensible Access Control Markup Language）是ABAC的一个实现，但是该设计过于复杂，我还没有完全理解，故不做介绍。 总结一下，ABAC有如下特点 集中化管理 可以按需求实现不同颗粒度的权限控制 不需要预定以判断逻辑，减轻了权限系统的维护成本，特别是需求经常变化的系统中 定义权限时，不能直观的看出用户和对象间的关系 规则如果稍微复杂一点，或者设计混乱，就会给管理者维护和追查带来麻烦 权限判断需要实时执行，规则过多会导致性能问题","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"ACL与RBAC访问权限模型","slug":"ACL与RBAC","date":"2019-08-28T11:37:00.000Z","updated":"2019-08-28T11:37:00.000Z","comments":true,"path":"2019/08/28/acl-yu-rbac/","link":"","permalink":"http://godhearing.cn/2019/08/28/acl-yu-rbac/","excerpt":"","text":"什么是ACL 以前非常盛行的一种权限设计，它的核心主要在于用户和权限直接挂钩。 它的原理非常的简单，每一项资源，都配有一个列表，这个列表记录的就是哪些用户可以对这项资源执行CRUD中的那些操作。当系统试图访问这项资源时，会首先检查这个列表中是否有关于当前用户的访问权限，从而确定当前用户可否执行相应的操作。总得来说，ACL是一种面向资源的访问控制模型，它的机制是围绕“资源”展开的。 它的优点是：简单易用，开发便捷 但同样，它的缺点也很明显，用户和权限直接挂钩，导致在授予权限时的复杂性，比较分散，不便于管理 使用场景： 比较小的用户管理系统 常见的文件系统权限设计，直接给用户加权限 什么是RBAC RBAC是基于角色的访问控制系统，权限和角色相联系，用户通过成为某个角色而获取该角色拥有的权限 它的原理就是将用户按照角色进行归类，通过用户的角色来确定用户有没有对某项资源访问的权限 这样做的好处是，简化了用户与权限的管理，在一定程度上简化了授予时的复杂度，易扩展 易于维护 缺点：开发对比于ACL相对复杂，而且，比较僵硬，如果要修改某个用户所拥有的权限，只能修改它的角色，这样，如果有不想让他拥有的权限，RBAC就显得很僵硬。 使用场景： 数据量比较庞大时授予权限 比较明确的角色分明时 小结如果要写一个用户管理系统，数据表之间的关联关系适当的添加关联关系甚至可以不添加那些看起来关联住的表，这样，查询时，完全可以通过多表联查来实现某些目的，不仅会使维护变得简单许多，还会将这个框给划开，而不是牵一发而动全身。 左：ACL 右：RBAC 无论是ACL还是RBAC，都不能过于复杂，规则过多，否则，维护性和性能会下降","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"聊天机器人","slug":"聊天机器人","date":"2019-08-16T12:21:00.000Z","updated":"2019-08-16T12:21:00.000Z","comments":true,"path":"2019/08/16/liao-tian-ji-qi-ren/","link":"","permalink":"http://godhearing.cn/2019/08/16/liao-tian-ji-qi-ren/","excerpt":"","text":"什么是聊天机器人 好吧其实没什么好解释的，就是能够一天二十四小时陪伴你到老的机器人，具有一定的学习能力，越来越像人，让你在没人陪的时候也能有人聊聊天解解闷 由于我还不太熟悉花里胡哨的机器学习，虽然对这方面有着很大的兴趣，但是奈何需要一定的门槛，不过没关系，我们可以先做一个调包侠 这篇教程，让我们一起用python来实现三款免费而且好用的机器人 从最简单的青云客开始吧 青云客首先呢，青云客可谓是相当的简单了，也不需要注册，不需要登录，直接上代码吧 url = 'http://api.qingyunke.com/api.php?key=free&amp;appid=0&amp;msg=%s'%(urllib.parse.quote('你好呀')) html = requests.get(url) print(html.json()['content']) 唯一需要注意的一点是，中文在url里，必须得转码，避免报错的可能，使用urllib 微软小冰 微软小冰是领先的跨平台人工智能机器人。微软小冰注重人工智能在拟合人类情商维度的发展，强调人工智能情商，而非任务完成在人机交互中的基础价值。 首先需要先领养小冰，通过微博关注小冰，然后给她发个消息 领养完成之后，按F12打开调试窗口，通过chat/里的Cookie，找到SUB值，注意不要手动退出，手动退出会刷新SUB的 之后，随便再发一条消息，找到new.json数据包，找到uid和source 最后构造参数 def xiaobing(): uid = '你的uid' source = '你的source' SUB = '你的SUB' url_send = 'https://api.weibo.com/webim/2/direct_messages/new.json' data = { 'text': 你要说的话, 'uid': uid, 'source': source } headers = { 'cookie': 'SUB='+SUB, 'Content-Type': 'application/x-www-form-urlencoded', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36', 'Referer': 'https://api.weibo.com/chat/' } response = requests.post(url_send, data=data, headers=headers).json() sendMsg = response['text'] time.sleep(1) while True: url_get = 'https://api.weibo.com/webim/2/direct_messages/conversation.json?uid={}&amp;source={}'.format(uid, source) response = requests.get(url=url_get,headers=headers).json() getMsg = response['direct_messages'][0]['text'] if sendMsg == getMsg: time.sleep(1) else: return getMsg 也是调包侠的日常，没啥难度 腾讯闲聊 这个也和小冰类似 先创建应用 拿到ID和KEY 欧克，准备工作完成，上代码 def tencent(msg): APPID = '123' APPKEY = '123' url = 'https://api.ai.qq.com/fcgi-bin/nlp/nlp_textchat' params = { 'app_id': APPID, 'time_stamp': str(int(time.time())), 'nonce_str': ''.join(random.choice(string.ascii_letters + string.digits) for x in range(16)), 'session': '10000'.encode('utf-8'), 'question': msg.encode('utf-8') } sign_before = '' for key in sorted(params): # 键值拼接过程value部分需要URL编码，URL编码算法用大写字母，例如%E8。quote默认大写。 sign_before += '{}={}&amp;'.format(key, urllib.parse.quote(params[key], safe='')) # 将应用密钥以app_key为键名，拼接到字符串sign_before末尾 sign_before += 'app_key={}'.format(APPKEY) # 对字符串sign_before进行MD5运算，得到接口请求签名 sign = hashlib.md5(sign_before.encode('UTF-8')).hexdigest().upper() params['sign'] = sign # print(params) html = requests.post(url, data=params).json() return html['data']['answer'] msg= '我好看吗' print(\"原话>>\", msg) res = tencent(msg) print(\"腾讯>>\", res)","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"python数据结构与算法","slug":"python数据结构与算法","date":"2019-08-05T13:36:00.000Z","updated":"2019-08-05T13:36:00.000Z","comments":true,"path":"2019/08/05/python-shu-ju-jie-gou-yu-suan-fa/","link":"","permalink":"http://godhearing.cn/2019/08/05/python-shu-ju-jie-gou-yu-suan-fa/","excerpt":"","text":"数据结构物理结构 又称存储结构 逻辑结构 集合结构：(同属一个整体，但是每个元素之间没有关系)​ 线性结构：队尾元素没有直接后继，队头元素没有直接前驱,其他元素有唯一的直接前驱和后继（一对一）​ 树形结构：除了根元素，其他元素都有一个前驱和多个后继（一对多）​ 图形结构：每个元素都有多个前驱和后继（多对多） 重点:线性结构 如果既是线性结构，又是链式结构，这种结构成为链表 如果既是线性结构，又是顺序结构，这种结构成为顺序表 链表又分为：单向链表 双向链表 单向循环链表 单向链表:当连接表中的每个节点只包含一个指针时，他只能指向下一个节点地址，这种只含有一个指针域的链表，称为单向链表 双向链表：它的连接表中，每个节点，都有两个指针，指向了直接前驱和直接后继，从双向链表中的任意一个节点开始访问，都能很方便的访问到它的前驱节点和后继节点，这种结构，称为双向链表 单向循环链表：普通的单向链表，末尾节点(也叫叶子节点)的指针，不再指向NULL，而是指向第一个节点，即开始节点 为什么要用单向循环链表，打个比方，我们要对单向链表中的某个节点进行访问，只能从头开始访问，而单向循环链表，可以从任意一个节点开始，因为它末尾的的指针指向了第一个节点，极大的增加了其灵活性 接下来，我们用代码来展示一下 ''' 单向链表：指向空的节点为尾结点 单向循环链表：指向头结点的节点为尾结点 ''' # 创建节点类 class Node: def __init__(self, data): \"\"\"节点类\"\"\" self.data = data self.pointer = None # 创建链表类 class SCLL: def __init__(self): \"\"\" 初始化函数\"\"\" self.head = Node(None) self.head.pointer = self.head # 判断是否位空 '''需要判断头结点是否指向自身，从而确定是否为空''' def is_empty(self): if self.head.pointer == self.head: return True # 针对空链表可返回None；针对非空链表即采用循环操作，结束条件当前位置是尾结点 def traversal(self): \"\"\" 遍历链表\"\"\" if self.is_empty(): return False else: counter = 1 current =self.head.pointer # 当前指针不指向头结点时进行循环 while(current != self.head): print(\" Element {{ is {{ \".format(counter, current.data)) counter += 1 current = current.pointer return True 链表和顺序表的区别：链表插入删除方便，修改查找不方便顺序表修改和查找方便，插入删除不方便 顺序结构：逻辑结构相邻，物理结构也相邻​链式结构：逻辑相邻，物理不一定相邻 算法 官方说法为：解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制 通俗的讲： 算法是特定解决问题的方法步骤 算法特性 输入：有零个或多个输入 输出：有一个或多个输出 有穷性：有限的时间或有限的步骤可以结束算法 确定性：每个步骤只有唯一的意思，不会产生歧义 可行性：可用现有的条件可以实现 算法复杂度 时间复杂度:算法运行所需要的时间 用大O表示法list：pop()删除末尾元素：O(1)pop(0)删除第一个元素：O(n)sort()排序：O(nlogn)insert()插入元素 O(n)append()末尾添加元素:O(1)字典：除了循环 复制 O(n)删除、添加、修改、查询元素O(1) 空间复杂度：算法运行所需要的空间","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"}],"author":"天听"},{"title":"爬虫的相关知识","slug":"爬虫的相关知识","date":"2019-08-01T14:32:00.000Z","updated":"2019-08-01T14:32:00.000Z","comments":true,"path":"2019/08/01/pa-chong-de-xiang-guan-zhi-shi/","link":"","permalink":"http://godhearing.cn/2019/08/01/pa-chong-de-xiang-guan-zhi-shi/","excerpt":"","text":"1.虚拟环境 虚拟环境就是一个隔离的python环境，不同的项目应该使用不同的虚拟环境(可以使用同一个虚拟环境) 虚拟环境不会导致环境之间的污染 1.1 虚拟环境管理模块virtualenvwrapper 安装 pip install virtualenvwrapper-win virtualenvwrapper的使用 查看所有虚拟环境：lsvirtualenv 创建虚拟环境：mkvirtualenv 环境名 激活虚拟环境：workon 环境名 查看当前虚拟环境下的模块： 进入当前虚拟环境 pip list 退出虚拟环境:deactivate 虚拟环境中安装模块:pycharm中,选中虚拟环境，然后添加模块,也可以通过pip install安装 删除虚拟环境：rmvirtualenv 环境名 pycharm如果没有直接显示虚拟环境，则 settings中选解释器处show all ,加号添加，创建的虚拟环境在 C:\\Users\\22742\\Envs\\目录下，选中Scripts下的python.exe 1.2 环境一致性 保证开发与生产环境一致，需要将模块等同一致 在开发机的虚拟环境中，运行命令:生成模块和其版本pip freeze &gt; requirements.txt 将requirements中生成的模块版本进行安装pip listall -r ./requirements.txt 1.3 查看包的详细信息pip show 包名 1.4 打包pyinstall -F XXX.py 2.爬虫2.1 爬虫的概念 爬虫又称网页蜘蛛或者网页机器人 模拟人操作客户端，向服务器发起网络请求，抓取数据的自动化程序和脚本 通用爬虫是通过抓取数据实现检索服务 爬虫分为聚焦爬虫和通用爬虫 自动化，数据量较小时可以人工获取数据，但往往在公司中爬取的量都在百万级千万级，所以要程序自动化获取数据 B/S架构：Browser/Server，类似淘宝，没有第三方中转，客户端和服务器直接交互 C/S：Client/server，类似微信，将微信后端作为中转站，和其他人对话时，需要在中转站传话 2.1.1 pyinstallerpyinstaller可以将python文件编译成一个程序，类似go语言的编译 2.2 通用爬虫百度，360，搜狐等搜索引擎 原理： 抓取网页 采集数据 数据处理 提供检索服务 通用爬虫抓取新网站的方式 主动提交url 设置友情连接 百度会和DNS服务商合作，抓取新网站 检索排名: 竞价排名 根据PageRank值，访问量、点击量 (SEO) 2.3 robots协议robots.txt:如果不想让百度爬取，可以编写robots.txt,这个协议只是口头上的协议，自己写的爬虫程序不需要遵从 2.4 聚焦爬虫 根据特定的需求，抓取指定的数据 思路： 代替浏览器上网 url，发起请求，获取响应 解析内容，提取数据 将数据存储到本地，数据持久化 2.5 requests模块#导包 import requests url = '*****' #res是获取的响应数据 res = requests.get(url) #响应数据的获取方式 1.文本形式：res.text 2.json形式：res.json() 3.流形式：res.content #数据持久化(mysql入库) #1.导包 import pymysql #2.创建链接 conn = pymysql.connect(host='127.0.0.1',port=3306,user='root',password='root',charset='utf8',database='***') #3.创建游标 cursor = conn.cursor() #4.构造sql语句 sql = 'insert into *** values(数据)' #5.执行sql语句 try: cursor.execute(sql) #提交事务 conn.commit() except Exception as e: print(e) #回滚 conn.rollback() params参数 get方式传参的拼接，将参数拼接到目标url中 2.6 OSI七层模型应用层： https/http/ftp http协议：明文传输，端口80 https协议：加密传输，端口443 表示层 会话层 传输层：UDP/TCP 网络层：IP 数据链路层：ARP 物理层：以太网协议 2.7 TCP/IP五层模型应用层：https/http/ftp/ssh/Sftp/ 传输层：UDP/TCP 网络层：IP 数据链路层：ARP 物理层：以太网协议 2.8 TCP和UDPTCP协议是一种面向连接的，可靠的，基于字节流的传输通信协议 有序性：数据包编号，判断数据包的正确次序 正确性：使用checksum函数检查数据包是否损坏，发送和接收时都会计算校验和 可靠性：发送端由超时重发，并有确认机制识别错误和数据的丢失 可控性：滑动窗口协议与拥塞控制算法控制数据包的发送速度 UDP协议是用户数据报协议，面向无连接的传输层协议，传输相对于TCP来说，不可靠 无连接：数据可能丢失或损坏 报文小，传输速度快 吞吐量大的网络传输，可以在一定成都上承受数据丢失 2.9 ARP协议通过IP获取目标计算机的mac地址的协议 交换机不能识别IP地址 2.9.1 ssh 远程登录会话 2.9.2 服务器创建的默认端口ftp:21 ssh:22 mySQL:3306 MongoDB:27017 Redis:6379 2.9.3 http与HTTPS协议的区别 https协议需要到ca申请证书，因而需要一定费用，现阶段国内各大厂商也提供免费的证书 http是超文本传输协议，信息是铭文传输，https则是具有安全性的ssl加密传输协议 http和https使用的是完全不同的连接方式，端口号也不一样,前者是80，后者是443 http的连接很简单，是无状态的，https协议是由ssl+http协议构建的可进行加密传输，身份认证的网络协议，比http协议安全(尽管HTTPS安全，但是传输的效率没有http高) 3.请求url:请求的网址，即统一资源定位符，它可以唯一确定我门向请求的资源 3.1 请求过程 客户端，通常指(web浏览器或APP)向服务器发起请求，服务器接收到的请求进行处理，并向客户端发起响应 请求由客户端向服务器发出的，可以分为四部分：请求方法(Request Method),请求网址(Request.URL),请求头(Request Headers) 请求体(Request Body) 3.2 请求方法常见的有八种 GET：请求页面，并返回内容 POST：用于提交表单数据或者文件等，数据包含在请求体中 PUT：从客户端向服务器传送的数据取代指定文档中的内容 DELETE：请求服务器删除指定的页面 HEAD：类似于GET请求，只不过返回的响应中没有具体的内容，用于获取头 CONNECT：把服务器当跳板，让服务器代替客户端访问其他网页 OPTIONS：允许客户端查看服务器的性能 TRACE：会回显服务器收到的请求，主要用于测试或诊断 GET和POST请求的区别 GET请求中的参数包含在URL里，数据可以在URL中看到，而POST请求的URL一般不会包含这些数据 GET请求提交的数据最多只有1024字节，而POST方法没有限制 POST比 GET相对安全 3.3 请求头和请求体Accept：请求报头域，用于指定客户端可接收哪些类型的信息 Cookie：页常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存在用户本地的数据，它的主要功能时维持当前访问会话，cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上cookies并将其发送给服务器，服务器通过cookies识别出是我们自己，并且查出当前是登录状态，所以返回的数据是登录之后网页内容 Referer:此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如来源统计，防盗链处理等 User-Agent：简称UA，它是一个页数的字段串头，可以使服务器识别客户使用的操作系统及版本，浏览器及版本等信息，做爬虫时加上此信息，可以伪装浏览器 x-requested-with：XMHttpRequest 代表ajax请求 Accept-Language：指定客户端可接受的语言类型 Accept-Encoding：指定客户端可接受的内容编码 Content-type：也叫互联网媒体类型(Internet Media Type)或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息,例：text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型 请求体一般承载的内容时POST请求中的表单数据，GET请求没有请求体，为空 3.4 反爬机制与反反爬策略#反爬机制： 为了不让数据泄露，设置了各种阻碍，这就是反爬机制 #反反爬策略 针对网站的反爬机制，采取不同策略 1.脚本：直接忽略 2.scrapy框架：修改配置文件，让爬虫不遵守robots协议 4.响应 响应是由服务端返回给客户端的，可 以分为三部分：响应状态码，响应头，响应体 响应体，响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码，我们要爬虫请求网页后，要解析的内容就是响应体 常见的状态码200：成功 301：永久重定向 302：临时重定向 400：错误的请求 401：未授权 403：服务器拒绝此请求 404：未找到 500：服务器内部错误 501：服务器不具备完成请求的功能 502：错误的网关，服务器走位网关或代理，从上游服务器收到无效响应 504：网关超时，服务器作为网关或代理，但是没有及时从上游服务器收到请求 505：HTTP版本不支持 状态码不能完全代表相应状态，部分网站的状态码是自定义的，一切以响应数据为准 4.1 响应数据的几种形式res = requests.get(url='https://www.guidaye.com/cp/') res.text >>>将响应对象转化为str类型 res.json() >>> 将响应对象转化为python中的dict类型，形式(类json) res.content >>>流形式(数据流，图片就是流形式) 如果响应数据中文乱码，可以用content.decode('utf-8')来解决 4.2 uuid 通用唯一标识符，时间戳，命名空间，随机数，伪随机数来保证生成ID的唯一性 python的uuid模块提供UUID类和函数uuid1(), uuid3(), uuid4(), uuid5() 来生成1, 3, 4, 5各个版本的UUID( 需要注意的是: python中没有**uuid2()**这个函数) uuid1：基于时间戳 uuid3：基于名字的MD5散列值 uuid4：基于随机数，有一定重复概率 uuid5：基于名字的SHA=1散列值 5.正则表达式5.1 元字符.:任意字符，换行符除外 \\d:任意数字 \\w:任意数字字母下划线 \\s:空白符 #如果是大写的s，w，d，代表'非' 5.2 字符组[a-z]: [A-Z]: [0-9]: [^...] #匹配非其中元素，举例：[^abc]--->匹配除了abc之外的字符 5.3 量词*:匹配0次或多次 +:匹配1次或多次 ?:匹配0次或1次 #非贪婪匹配 {m}:m次 {m,}:至少m次 {m,n}:m-n次 {,n}:最多n次 5.4 边界修饰 ^匹配开始 $匹配结尾 5.5 分组import re s = \"&lt;a href='asdsdjfiohssdbfkjsdbkjsd'>\" res = re.findall(r\"href='(.*?)'>\",s) 5.6 贪婪与非贪婪 贪婪，尽可能多的匹配 非贪婪，尽可能往少了匹配 5.7 rere.findall(r'正则表达式','str'),结果是一个列表，匹配整个字符串 re.search(r'正则表达式','str')匹配到第一个结果就返回，返回的是一个对象，使用group取值 re.match(r'正则表达式','str')从字符串开始进行匹配，返回一个对象，使用group取值，如果未匹配到，返回None re.complie将正则表达式编译为对象，在需要按正则表达式匹配是可以在直接使用该对象调用以上方法 6. requests高阶应用6.1 文件处理import requests #打开文件，注意要以rb形式打开 f = open('chn.jpg','rb') files = { 'file':f } res = requests.post(url='***',files = files) 文件也是一种数据，所以，可以通过files参数来进行文件的上传 6.2 会话维持from requests import Session #1.实例化一个对象 session = Session() #2.url url = '*****' #3.session.get()或者session.post(url=url.headers=headers) res = session.post(url=url.headers=headers) 6.3 ssl证书验证https是http的安全版本，HTTPS在http的基础上多了一个ssl安全套接层 requests提供了证书验证的功能，当发起HTTP请求时，模块会检查SSL证书，但检查的行为可以用verify参数来控制 添加了一个参数verify=false ---&gt;不检查ssl证书,如果等于True，则检查SSL证书 #ssl证书验证 #添加一个verify=false参数，禁止证书验证 import requests url = '******' #阻止抛出警告 requests.packages.urllib3.disable_warinings() res = requests.get(url=url,verify=false) 简单来说，在爬取网站时，有可能网站的证书是有问题的，这时如果使用requests模块去请求时，会报错，所以需要ssl证书验证 6.4 代理设置 代理IP是指在请求的过程中使用非本机ip进行请求，避免大数据量频繁请求的过程中出现IP封禁，限制数据的爬取 透明代理ip：服务器知道你使用了代理，服务器能够获取爬虫真实的ip 匿名代理ip：服务器知道你使用了代理，服务器不能获取爬虫真实的ip 高匿代理ip：服务器不知道使用了代理，服务器不能获取爬虫真实ip 代理类别：基于接口的，基于隧道的 反爬：ip封禁--->使用代理ip import requests url = '*********' proxies = { #或者是https 'http':'http://ip地址:端口号', #无论是http还是https，后面一定是http 'https':'http://ip地址:端口号' } res = requests.get(url=url,proxies = proxies) 6.5 超时设置添加了一个参数，以秒计量timeout=0.1 #添加timeout参数，秒数 import requests res=requests.get(url=url,timeout=0.1) 给予爬虫与服务器连接的时间限定，设置一个时间，在指定的时间内完成了正常的连接，不报错，如果没有完成，就会报错 requests模块发送请求可以设置超时时间，在超时时间内未得到响应，便会抛出异常 好处：一方面减少了请求的阻塞时间，一方面，可以进行异常处理，执行相应的操作 如果规定时间完成了和服务器连接，之后爬取数据的时间并不算在超时设置的时间内 6.6 UA检测UA是用户的身份表示，可以表示用户的系统及浏览器信息 在请求过程中，添加headers参数 6.7 cookie的处理(session) 在同一个关联网页中，为了保存登录状态和各种信息，可以通过cookie来保持 三种方法 手动在headers中添加cookie的键值对 cookiejar对象 自动封装cookie的类：Session #cookie的处理 #1.url = 'https://www.baidu.com/' headers = { 'Cookie':'BIDUPSID=B63BDB40304991E9FF3159864CC9C302; PSTM=1586308511; BAIDUID=B63BDB40304991E9CC4E4ECFFCFFB23D:FG=1; BD_UPN=12314753; BDUSS=VWNmZu', 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36' } res = requests.get(url=url,headers=headers) #2.cookiejar对象 from requests.cookies import RequestCookieJar 1.首先需要获取Cookies Cookie = *********** 2.实例化一个jar对象 jar = RequestsCookieJar() 3.处理Cookies，封装进jar对象中 for i in Cookie.split(','): #再次分割，分成dict的键值，每分割一次添加一次 k,v = i.split('=',1) jar.set(k,v) #3.Session类，会话维持 from requests import Session 1.实例化一个对象 session = Session() 2.url url = '*****' 3.session.get()或者session.post(url=url.headers=headers) res = session.post(url=url.headers=headers) from requests import Session session = Session() data = { 'username':\"天听\", 'password':'123456' } res = session.post(url=url,headers=headers,data=data) 7.lxm库 从响应数据中抽取出目标数据的过程，就叫做数据解析数据解析：re,xpath,BS4,Pyquery DOM树与xpath解析原理 HTML页面标签存在层级关系，即DOM树，在获取目标数据时可以根据网页层级关系定位标签，再获取标签的文本或属性 xpath解析原理：根据DOM节点的结构关系，进行定位 7.1 xpath基本语法.:当前节点 /:根节点 //:代表任意位置 .//:从当前节点向下的任意位置匹配 nodename:节点名定位 nodename[@attribute='value']:根据节点的属性进行定位 @attribue：获取节点的属性值，比如获取a标签的href属性，直接可以/a/@href text():获取节点的文本//div[@class='asdds']/p/text() 7.2 属性匹配 单属性多值匹配：当节点的一个属性有多个值时，根据其中一个进行定位，使用contai ns函数 '//div[contains(@class,\"属性值\")]' 多属性匹配：用节点的多个属性共同定位节点and '//div[@class=\"asds\" and @name=\"adsadasd\"]' 7.3 按序选择 索引定位：[6] 注意，索引从1开始，跟python有区别 位置函数：position例：/li[position()&gt;2] last()函数：定位最后一个，last()-1代表倒数第二个 7.4 流程加载本地html，需要有etree.HTMLParser参数，注意要加括号例：tree = etree.parser('./xpath.html',etree.HTMLParser()) 加载网页html,直接使用HTML例：tree = etree.HTML() 然后些xpath语法tree.xpath('//ul[@class=\"pli\"]/li/div/a/img/@src') xpath获得的结果是一个列表2 #编码流程 from lxml import etree res = requests.get(...) tree = etree.HTML(res.text) #etree加载的是响应数据的文本形式 tree.xpath('xpath表达式') 7.5 补充res1 = tree.xpath(\"//div[@id='007']/text()\") res2 = tree.xpath(\"//div[@id='007']//text()\") ''' res1展示的是以divid007为根节点的结果，其div下的其他标签不显示 res2展示的是以divid007任意位置的结果，其div下的其他标签内容也一同显示 ''' 8.动态数据加载 网页HTML上，有些数据是通过js代码填充，所以如果直接使用爬虫，只会爬取到一个标签，并没有其中的元素 requests模块和scrapy框架在发起请求爬取数据的过程中，不能执行js代码 8.1 selenium selenium是一个web端自动化测试框架，可以通过代码来控制浏览器，比如打开关闭，点击等行为 作用：帮助抓取动态加载的数据，避免反爬 8.2 selenium安装与配置与操作 Chrome浏览器 selenium框架：pip install selenium 3.驱动程序：下载 http://npm.taobao.org/mirrors/chromedriver/ 查看浏览器版本 选择对应的版本 编码流程： #首先需要将下载的chromedriver.exe放到代码文件夹下 #导包 from seleniumi import webdriver #调用chromedriver.exe bro = webdriver.Chrome('./chromedriver.exe') #访问 bro.get('https://www.iqiyi.com/') #获取网页源代码 (对象)bro.page_source --->字符串 #如何获取网页的元素 #根据标签内属性定位，一般用id定位 find_element_by_id('id') find_element_by_name('name') find_element_by_class_name('class')#根据class属性定位 find_element_by_xpath()#根据xpath定位节点 find_element_by_css_selector()#css选择器 find_element_by_link_text()#根据超链接文本定位 find_element_by_partial_link_text()#根据超链接文本的一部分定位 #执行js脚本 execute_script(js) #节点交互操作： 1.输入内容：对象.send_keys() 2.清空内容：对象.clear() 3.点击操作：对象.click() 4.退出浏览器：对象.quit() #获取网页的数据 获取元素属性：get_attribute() 获取元素文本：get_text() 获取元素位置：element.location 获取元素尺寸：element.size 获取网页源码：browser.page_source(*****) #执行js脚本 js = 'window.scrollTo(0,300)'#向下滚动300距离 js = 'window.scrollTo(0,document.body.scrollHeight)'#滚动到底部 对象.execute_script(js) iframe标签跳转switch_to.frname('frameid') switch_to.default_content() 实例： from selenium import webdriver from time import sleep from selenium.webdriver.chrome.options import Options options = Options() options.add_experimental_option('excludeSwitches',['enable-automation']) #调用chromedriver.exe bro = webdriver.Chrome('./chromedriver.exe',options=options) bro.get('https://www.baidu.com/') #根据id-->kw获取input输入框 input_tag = bro.find_element_by_id('kw') #根据id-->su获取百度一下点击按钮 button_baidu = bro.find_element_by_id('su') #输入框输入 input_tag.send_keys('黑洞') #点击 button_baidu.click() #睡眠两秒之后清空输入框 sleep(2) input_tag.clear() input_tag.send_keys('抖动') button_baidu.click() sleep(3) input_tag.clear() input_tag.send_keys('翻转') button_baidu.click() sleep(3) input_tag.clear() bro.quit() 8.3 子级页面和父级页面 HTML页面嵌套另一个HTML页面 #子页面的跳转 switch_to.frame('id') #跳转到父级页面 switch_to_default.conent() 注意，selenium默认操作父级页面 8.4 防检测from selenium.webdriver.chrome.options import Options options = Options() options.add_experimental_option('excludeSwitches',['enable-automation']) bro = webdriver.Chrome('./chromedriver.exe',options=options) 9.多线程爬虫 在爬取数据量大的数据时，耗费时间较长，为了提高效率，可采用多线程爬虫，提高效率，但是，多线程不是原子性，操纵数据可能会导致数据紊乱，不安全 9.1 并发与并行 并行：在同一时刻，多个任务同时执行 并发：在同一时间段内，多个任务同时执行并发时，一般采用了时间片轮转法，即在一个时间段内，给每个程序添加一个时间片，也可以叫做进度条，进度条走完，下一个程序再继续运行，不过时间片轮转时，停顿时间非常的短，所以会造成多个任务同时运行的错误 9.2 示例from threading import Thread from queue import Queue import requests from lxml import etree import pymongo from threading import Lock #负责爬取的类 class SpiderThread(Thread): def __init__(self, name, url_queue, data_queue): super().__init__() self.name = name self.url_queue = url_queue self.headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36' } self.data_queue = data_queue def run(self): base_url = 'http://xiaohua.zol.com.cn/lengxiaohua/%s.html' while 1: try: page = self.url_queue.get(block=False) print('%s正在爬取数据' % self.name ) res = requests.get(url=base_url % page, headers=self.headers) self.data_queue.put(res.text) except: break #负责解析的类 class ParseThread(Thread): def __init__(self, name, data_queue, lock): super().__init__() self.name = name self.data_queue = data_queue self.lock = lock def run(self): # 调用解析方法 while 1: try: html = self.data_queue.get(block=False) print('%s 正在解析数据' % self.name) self.parse(html) except: break def parse(self, html): tree = etree.HTML(html) li_list = tree.xpath('//li[@class=\"article-summary\"]') for li in li_list: title = li.xpath('.//span[@class=\"article-title\"]/a/text()') content = ''.join(li.xpath('.//div[@class=\"summary-text\"]//text()')) if title and content: data = { 'title': title[0], 'content': content } with self.lock: self.save(data) def save(self, data): # 简历连接 conn = pymongo.MongoClient() db = conn.lilong table = db.liuyueyang table.insert_one(data) 10.无头浏览器与BS410.1 无头浏览器 什么是无头浏览器（headless browser），简单来说是一种没有界面的浏览器。既然是浏览器那么浏览器该有的东西它都应该有，只是看不到界面而已。我们日常使用浏览器的步骤为：启动浏览器、打开一个网页、进行交互。而无头浏览器指的是我们使用脚本来执行以上过程的浏览器，能模拟真实的浏览器使用场景。 from selenium import webdriver from selenium .webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument('--headless') chrome_options.add_argument('--dissble-gpu') bro = webdriver.Chrome(chrome_options = chrome_options) bro.get(******) print(bro.page_source) 10.2 BS4语法 编码流程： 导包from bs4 import BeautifulSoup 实例化对象，传两个参数，一个文本，一个解析器，一般为lxmlsuop = BeautifulSoup(res.text,'lxml') 选择器解析 #bs4编码流程 from bs4 import BeautifulSoup suop = BeautifulSoup(res.text,'lxml') tag = soup.select('css选择器表达式') tag = soup.节点() #节点选择器 tag = soup.findall() # 方法选择器 #节点选择器 from bs4 import BeautifulSoup soup = BeautifulSoup(res.text,'lxml') tag = soup.a #取a标签，只取一个 #方法选择器 find_all(name,attrs,text,limit): soup.findall(name='***') #根据节点名字定位 soup.findall(attrs={'属性名(scr,class等)':'值'})#根据属性定位，多个属性时，一个即可定位 soup.findall(text=res.compile(r'***'))#根据节点文本定位，返回文本 soup.findall(name='***',limit=2)#只返回两个结果 find(name,attrs,text,limit):区别于find_all，find返回的是一个对象结果 find_all(name=节点名,{attrs:属性值})返回的是一个列表 #css选择器 属性选择器： 1.根据节点名定位标签：标签选择器 soup.select('***(title,a,p等)') 2.根据节点的class属性定位：css选择器 soup.select('.***') 3.根据id定位 soup.select('#***') 4.嵌套选择： ss = soup.select('ul')#得到的是一个列表 for i in ss: print(i.select(\"li\")) 5.层级选择器 soup.select('div > ul > li') #单层级选择器，按照顺序找到直属li soup.select('div li') #多层级选择器，包含了div下的所有li #获取节点的文本或属性 obj.string:获取直接子文本，如果节点内有平行的节点，则结果是None obj.get_text()：获取子孙节点的所有文本 obj['***(属性)']：获取节点属性 11.快代理网站的模拟登录from requests import Session #实例化session对象 session = Session() #登录的url url = 'https://www.kuaidaili.com/login/' #构造数据 data = { 'next': '', 'kf5_return_to': '', 'username': '2274201339@qq.com', 'passwd': 'o66.' } #浏览器头 headers = { \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36' } #用post请求将数据传进去，此时，已经模拟了登录，访问这个网站的其他网页时，会保存登录状态 res = session.post(url=url,headers=headers,data=data) #访问时，可以获得到返回的数据，用户名在其中，模拟登录完成 ret = session.get(url='https://www.kuaidaili.com/api/checkuser/',headers=headers) print(ret.json()) 11.1 第三方打码平台 我们在模拟登录时，时常会遇到一些验证码代码无法准确的识别不同的验证码，这时，就用到了打码平台，它会将图片上的字符或者数字转成字符串返回给你 流程： 下载验证码图片 传给第三方打码平台 进行识别，识别完成之后，传回ret 把ret拿回，构造数据 超级鹰平台 12.Scrapy框架 scrapy是基于twisted的异步框架 12.1 安装 首先需要安装相应的依赖库 lxml、wheel、pywin32 twisted此依赖安装时，需要从地址下载相应的版本,然后通过pip来安装 最后安装scrapy框架 12.2 创建项目 创建项目scrapy startproject 项目名 创建爬虫文件scrapy genspider 爬虫名 域名 12.3 运行项目scrapy crawl 爬虫名 12.4 项目基本架构└── day01(A) 外层项目目录 └── day01(A) 内层项目目录 └── spiders 放置爬虫的包 └── __init__.py └── tianting.py 爬虫文件 ​ └── items.py 定义要爬取的数据字段​ └── middlewares.py 中间件​ └── piplines.py 管道​ └── settings.py 配置文件(爬虫配置)​ └── scrapy.cfg 配置文件,跟部署相关 12.5 核心组件与数据流向核心组件 五大核心组件 1.引擎(Scrapy Engine)：整个框架的调度，负责各个组件之间的通信与数据的传递 2.爬虫(Spiders)：定义爬取行为和解析规则 3.调度器(Scheduler)：负责调度所有请求 4.下载器(Downloader)：负责爬取页面(与互联网交互，爬取页面的) 5.管道(Item Pipeline)：负责数据持久化 由于引擎负责各个组件之间的调度，所以，所有的组件在相互传递时，都需要经过引擎，比如，爬虫解析后需要将req给引擎，然后引擎再给调度器，注意，此时，调度器无法越过引擎去直接调用下载器，所以，需要将req再次返回给引擎，由引擎来调度下载器和互联网交互 数据流向 依据请求的生命周期 ​ 【调度器】 ​ ↑ ↓​ 2.req 3.req 4.req 【管道】 【引擎】 → 【下载器】 5.req → internet ←7.res ←6.res ​ ↑ ↓​ 1.req 8.res ​ 【爬虫】 9.爬虫经过引擎到达管道 12.6 组件分析 爬虫组件 xpath选择器，extract_first返回第一个数据，如果不加first，则是返回所有数据 scrapy.Spider:Spider爬虫类，自建的爬虫类必须继承这个 将数据item在组件中传递，注意不是return，而是yield 实例化item,两种方法，一种是实例化对象，另一种是直接yield 类名(字段名=值) class BlogSpider(scrapy.Spider): #爬虫名，爬虫唯一的身份标识，不可重复 name = 'blog' #域名的限定，限制了爬虫的范围，可以注释 # allowed_domains = ['baidu.com'] #起始url，当项目启动，自动对这个url发起请求 start_urls = ['http://baidu.com/'] #parse是默认的解析回调方法，如果发送一个请求，未指定回调解析，默认调用parse def parse(self,response): li_list = response.xpath('//ul[@id=\"menu-list\"]/li') for li in li_list: item = Test01Item() #名字 item['title'] = li.xpath('.//h2/a/@title').extract_first() #简介 item['brief'] = ''.join(li.xpath('./text()').extract()).replace('\\n','') #时间 item['date'] = re.findall(r'\\d+-\\d+-\\d+', li.xpath('.//p/text()').extract_first())[0] #链接 item['link'] = li.xpath('.//h2/a/@href').extract_first() yield item #第二种实例化方法 yield Test01Item(title=title) item Item 是保存爬取数据的容器，它的使用方法和字典类似,Item 多了额外的保护机制，可以避免拼写错误或者定义字段错误。建 Item需要继承 scrapy.Item类，并且定义类型为 scrapy.Field字段 注意！！items只能够通过字典的方式进行访问和添加 在爬虫组件中使用item容器，需要以下4步： 导包，将items导入 实例化items中的类对象 通过键值对的字典形式将数据添加进items 通过yield将items返回 class Test01Item(scrapy.Item): title = scrapy.Field() jianjie = scrapy.Field() #爬虫组件中： item = Test01Item() #名字 item['title'] = li.xpath('.//h2/a/@title').extract_first() #简介 item['brief'] = ''.join(li.xpath('./text()').extract()).replace('\\n','') #时间 item['date'] = re.findall(r'\\d+-\\d+-\\d+', li.xpath('.//p/text()').extract_first())[0] #链接 item['link'] = li.xpath('.//h2/a/@href').extract_first() yield item 管道(pipelines) 在管道中，主要实现数据的保存，在自定义类中，实现process_item方法，参数有item，spider 如果爬虫组件中，传出了item，那么就需要将item走过这个process_item方法 #数据库的连接 conn = pymongo.MongoClient('localhost',27017) db = conn.Longtan table = db.ss #自定义类 class TextPipline: #实现process_item方法,item就是爬虫组件中传来的item def process_item(self,item,spider): table.insert_one(dict(item)) return item settings 需要修改的是ROBOTSTXT_OBEY,UA, ITEM_PIPELINES = { 'test01.pipelines.TextPipline': 300, } 12.7 保存数据保存到json中 scrapy crawl 爬虫名 -o blog_data.json 另外，也可以每一个Item输出一行JSON,输出后缀改为jl，命令:scrapy crawl 爬虫名 -o blog_data.jl 此外，输出还支持csv、xml、pickle、marshal，还支持了远程ftp、s3等输出 注意，ftp输出需要正确配置用户名、密码、地址、输出路径，否则会报错 12.8 管道(Item Pipline)的使用 注意，pipline是需要被注册的 1. 在settings中，解封或者重新在底下写上 ITEM_PIPELINES = { 'test01.pipelines.TextPipline': 300, #项目名.pipelines.自定义的管道名: } # 300：代表着优先级，数字越小，代表着优先级越高 当Item生成后，它会自动被送到管道进行处理，我们常用管道来实现以下: 清理HTML数据 验证爬取数据，检查爬取字段 查重并丢弃重复内容 将爬取结果保存到数据库中 class Test01Pipeline: def process_item(self, item, spider): return item 12.9 整体流程 需要在items中，定义要爬取的字段 在爬虫组件中，定义要爬取的url和解析规则 在settings中，配置相关的参数 在pipeline中，自定义类,实现process_item方法在爬虫组件生成的item，通过了yield传递到process_item方法中 将数据保存在MongoDB、mysql等数据库中,也可以将数据保存到json等文件里 12.91 手动发送请求 srcapy框架在启动时，会自动对起始url发起请求，是因为爬虫组件继承的scrapy.Spider实现了一个start_requests方法，所以，想要手动发送请求，需要自己在类中实现此方法，这样，便不会再自动继承父类的方法，而是从自己类中实现方法 #手动发送get请求 #callback参数，是指定回调，如果不指定此参数，就会默认回调到parse yield scrapy.Request(url=url,callback=self.parse) #手动发送post请求 yield scrapy.FormRequest(url=next_page,formdata=data) 12.92 中间件篡改UA 需要用到的模块fake-useragent，用pip安装 #在middlewares中，TestIpUaDownloaderMiddleware下载器中间件中， #导包 from fake_useragent import UserAgent #实例化对象 ua = UserAgent() def process_request(self, request, spider): request.headers['User-Agent'] = ua.random return None ua.random #生成的ua是随机的 注意，需要把settings中的DOWNLOADER_MIDDLEWARES打开 篡改ipdef process_request(self, request, spider): #篡改ip request.meta['proxy'] = 'http://ip:port' return None selenium的使用 拦截res响应，在process_response方法中 使用selenium抓取数据 构建一个响应对象 替换原有的响应对象 注意！如果在中间件中实例化对象，那么，每次请求都会实例化对象，比较耗费资源，所以，最好在爬虫组件中实例化 #爬虫组件中 from selenium import webdriver #构建响应对象 from scrapy.http import HttpResponse bro = webdriver.Chrome(executable_path='设置绝对路径') #中间件中，使用方法process_response自带的spider参数，从爬虫组件可以直接传过去 bro = spider.bro bro.get(url=response.url) #设置滚动一屏脚本 js = 'window.scrollTo(0,document.body.scrollHeight)' #执行js脚本 bro.execute_script(js) #网页源码 html = bro.page_source #自己构建响应对象 myres = HttpResponse(url=response.url,body=html,encoding='utf-8',request=request) #替换原来的响应对象 return myres 根据需要添加判断是否使用selenium 12.93 meta传值 在爬虫组件中，如果回调解析方法之间相互传值，可以使用meta def parse(self,response): ... yield scrapy.Request(url=url,callback=self.da_parse,meta={'title':title}) def da_parse(self,response): title = response.meta['title'] 12.94 全站数据爬取 全站数据爬取就是爬取多页时自动帮助翻页，省去了写代码的时间 创建爬虫文件时，在genspider后，添加-t crawl再添加爬虫名和域名，这样，爬虫会成为一个全站爬虫 #链接提取器，根据给定的规则进行链接的提起 link = LinkExtractor(allow=r'Items/') rules = ( #规则解析器，根据给定的规则解析数据，follow是递归提取页码，就是逐级访问url，对各个url来追踪访问 Rule(link,callback='parse_item', follow=True), ) 12.95 增量式爬虫 增量式就是自写爬虫监控目标网站，如果目标有更新的数据，则将更新的数据爬取下来，已经爬取的数据，则通过去重的方式来忽略 而这其中，又分为两种去重方式，分别是根据url去重和数据指纹去重 from redis import Redis conn = Redis('127.0.0.1',6379) def detail_parse(self,response): name = li.xpath('//text()') content = li.xpath('//text()') #连起来生成指纹 data = name+content fp = md5(data.encode('utf-8')).hexdigest() #添加进redis的集合中 conn.sadd('ggsfp',fp) #如果返回结果为0，则证明已经添加过，如果为1，则没有添加过 对应day14视频 13.MongoDB数据库13.1 数据库操作 创建数据库并切换至该数据库下use 库名 查看当前数据库db 查看所有数据库，只能显示非空的数据库show dbs 创建表db.creaeteCollection('表名') 插入数据,如果没有这个表，会自动创建db.表名.insert({'字段':'值}) 查询所有数据db.表名.find() 查看所有表show tables 删除表db.表名.drop() 删除库db.表名.dropDatabase() 13.2 增加数据 字段多少是单独的那条数据，不一致也没有关系 例如： {‘name’:’玛卡巴卡’}—{‘name’:’海绵宝宝’,’age’:200} db.表名.insertOne({'字段名':'值}) db.表名.insert({'字段':'值}) db.表名.insertMany([{'字段':'值},{'字段':'值}]) 13.3 查询数据 简单查询db.表名.find() 分页，返回2条查询数据db.表名.find({}).limit(2) 排序，*1**是升序，-1***是降序db.表名.find().sort({'price':1}) 等值查询 等值查询（按照条件查询）db.表名.find({'price':4.5}) 非等值查询（范围查询） ({'price':{'$lt':500,'$gt':100}})#小于500，大于100 或查询db.表名.find({$or:[{'price':{'$lt':5.5}},{'price':{$gt:500}} ]}) > == $gt ({'price':{$gt:100}})#价格大于100的 &lt; == $lt ({'price':{$lt:100}})#价格小于100的 &lt;= == $lte ... >= == $gte ... != == $ne ... 模糊查询 db.表名.find({'字段名':{$regex:'正则表达式'}}) 13.4 修改数据db.表名.update({'price':2.5},{$set:{'price':500}}) 13.5 删除数据db.表名.remove({'name':'玛卡巴卡'})","categories":[{"name":"spider","slug":"spider","permalink":"http://godhearing.cn/categories/spider/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"spider","slug":"spider","permalink":"http://godhearing.cn/tags/spider/"}],"author":"天听"},{"title":"PayPal跨境支付","slug":"PayPal跨境支付","date":"2019-04-17T10:51:00.000Z","updated":"2019-04-17T10:51:00.000Z","comments":true,"path":"2019/04/17/paypal-kua-jing-zhi-fu/","link":"","permalink":"http://godhearing.cn/2019/04/17/paypal-kua-jing-zhi-fu/","excerpt":"","text":"什么是PayPal PayPal又称为贝宝，是一种外贸支付方式，目前在国际的贸易支付中有着极高的地位，它就好像中国的支付宝，都是第三方支付平台。 PayPal的优势是，其业务网络遍布全球。目前PayPal的庞大网络覆盖了全球200多个国家，可提供20多种语言服务，并接受100多种货币付款和56种货币提现。 需要注意的一点是，也是我个人感觉非常好的一个地方，PayPal的支付逻辑，是一个事务性的操作，一旦开始，除非获取到支付成功的返回凭证，否则一致就认为没有成功，支付宝就不是这样，它是支付时生成订单，无论是否付款，订单都已经生成了，这样的话，在付款的一瞬间，啪，网断了，支付宝有可能会出现错误，可能是支付状态未修改啊，或者三方卖家没有收到支付成功请求啊一类的。 但是，支付宝也有避免错误的方法，就是对所有唤起收银台交易的状态码10003发起轮询，轮询间隔未3秒，在让用户再次支付前，必须通过查询确认当前订单的 状态 注册PayPal和开发者平台官网地址 开发者平台 注册成功后，会默认创建两个账号，一个是个人的，一个商户的，如果不想用默认的，可以自己创建 进入应用管理，可以看到，它也给创建了默认的应用 然后获取Client ID和Secret 然后回来修改一下个人账户的余额和密码 开始操作pip install paypalrestsdk 先安装paypal的sdk 然后新建app，注册好路由和应用，然后view中的代码 import paypalrestsdk from django.http import HttpResponse from django.shortcuts import redirect Client_id = '你的Client_id' Secret = '你的Secret' def payment(request): paypalrestsdk.configure({ \"mode\": \"sandbox\", # sandbox代表沙盒 \"client_id\": Client_id, \"client_secret\": Secret, }) payment = paypalrestsdk.Payment({ \"intent\": \"sale\", \"payer\": { \"payment_method\": \"paypal\"}, \"redirect_urls\": { \"return_url\": \"http://192.168.1.157:8000/palpay/pay/\", # 支付成功跳转页面 \"cancel_url\": \"http://192.168.1.157:3000/paypal/cancel/\"}, # 取消支付页面 \"transactions\": [{ \"amount\": { \"total\": \"5.00\", # 付款金额 \"currency\": \"USD\"}, # 货币类型 \"description\": \"这是一个订单测试\"}]}) if payment.create(): print(\"Payment created successfully\") for link in payment.links: if link.rel == \"approval_url\": approval_url = str(link.href) print(\"Redirect for approval: %s\" % (approval_url)) return redirect(approval_url) else: print(payment.error) return HttpResponse(\"支付失败\") 启动项目，网页访问该视图，即可看到这样 点击继续，可以看到，回调的地址传来了三个参数 http://192.168.1.157:8000/palpay/pay/?paymentId=PAYID-L7BRGHA28162157FN083293V&amp;token=EC-1S659222KF8901217&amp;PayerID=QFV2E28KASNQS 支付id,token和支付者id 然后，我们写一个回调方法，需要通过支付者id进行确认验证支付 def payment_execute(request): paymentid = request.GET.get(\"paymentId\") #订单id payerid = request.GET.get(\"PayerID\") #支付者id payment = paypalrestsdk.Payment.find(paymentid) print(payment) if payment.execute({\"payer_id\": payerid}): print(\"Payment execute successfully\") return HttpResponse(\"支付成功\") else: print(payment.error) # Error Hash return HttpResponse(\"支付失败\") 然后点击支付即可看到，少了五块钱 然后有些时候需要对交易进行一些核对，也可以通过接口查看交易明细，也就是上面的payment","categories":[{"name":"PayPal","slug":"PayPal","permalink":"http://godhearing.cn/categories/PayPal/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"PayPal","slug":"PayPal","permalink":"http://godhearing.cn/tags/PayPal/"}],"author":"天听"},{"title":"基于Docker配置Elasticsearch全文检索","slug":"基于Docker配置Elasticsearch全文检索","date":"2019-03-24T14:41:00.000Z","updated":"2019-03-24T14:41:00.000Z","comments":true,"path":"2019/03/24/ji-yu-docker-pei-zhi-elasticsearch-quan-wen-jian-suo/","link":"","permalink":"http://godhearing.cn/2019/03/24/ji-yu-docker-pei-zhi-elasticsearch-quan-wen-jian-suo/","excerpt":"","text":"前言 什么是搜索，这是一个相当简单的问题了，就是根据一个搜索词来检索出所有包含该词的数据。我们一般使用mysql数据搜索都是通过模糊搜索来查询，但是这样就会面临一个性能问题，假如数据量超多，这样的搜索无异于自杀。 基于文档式的全文检索引擎相信大家都不陌生，Elasticsearch诞生的本意是为了解决文本搜索太慢的问题，ES会默认将所有的输入内容当作字符串来理解，对于字段类型是keyword或者text的数据比较友好。 我们已经提到，Elasticsearch专为字符串搜索而生，在建立索引的时候针对字符串进行了非常多的优化，在对字符串进行准确匹配或者前缀匹配等匹配的时候效率是很高的。 谈论到搜索引擎，就一定会涉及到两个概念，正向索引和反向索引。听上去这是两个完全不同的数据结构。但是实际上，正向索引就好比我们的书籍，每本书都有目录，这就是一种正向索引，能够通过文档去查找关键词。 而反向索引，和正向索引是完全相反的，它将关键词作为索引， 去查找哪个文档包含了这个关键词，就拿上边的例子，正向索引是通过楼层去找店铺，而反向索引，就是你知道店铺的某一个字，商场导航给你提供了包含这个字的所有店铺，这样就极大的缩小了查找范围。 Docker配置拉取镜像 docker pull elasticsearch:7.2.0 运行镜像 docker run --name es -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -d elasticsearch:7.2.0 容器别名我们就用缩写es来替代，通过 9200 端口并使用Elasticsearch的原生传输协议和集群交互。集群中的节点通过端口 9300 彼此通信。如果这个端口没有打开，节点将无法形成一个集群，运行模式先走单节点模式。 此时，我们如果要加一些功能，就得改一些配置，就好像django的Settings配置一样。 Docker提供了cp命令来拷贝容器内部的文件 我们只要拷贝elasticsearch.yml docker cp 容器id:/usr/share/elasticsearch/config/elasticsearch.yml ./elasticsearch.yml 也可以将文件拷贝路径指定为绝对路径 打开文件，可以加一些自己的配置 cluster.name: \"docker-cluster\" network.host: 0.0.0.0 http.cors.enabled: true http.cors.allow-origin: \"*\" 然后停止正在运行的容器，并且删除它 docker stop 容器id docker rm $(docker ps -a -q) 再次启动，只不过不同的是，这次我们需要通过-v命令把我们刚刚修改的文件配置挂载到容器内部去。 docker run --name es -v E:\\elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -d elasticsearch:7.2.0 这里需要注意一点，就是在Win10宿主机里需要单独设置一下共享文件夹，这里我设置的共享文件夹叫做es，如果是Centos或者Mac os就直接写真实物理路径即可。 随后，重启Docker，输入命令进入默认容器：docker-machine ssh default 在容器根目录能够看到刚刚设置的共享文件夹，就说明设置成功了。 另外还有一个需要注意的点，就是Elasticsearch存储数据也可以通过-v命令挂载出来，如果不对数据进行挂载，当容器被停止或者删除，数据也会不复存在，所以挂载后存储在宿主机会比较好一点，命令是： docker run --name es -v /es/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /es/data:/usr/share/elasticsearch/data -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -d elasticsearch:7.2.0 python配置首先安装pip3 install elasticsearch 建立检索实例 from elasticsearch import Elasticsearch es = Elasticsearch(hosts=[{\"host\":'Docker容器所在的ip', \"port\": 9200}]) 建立索引 # 建立索引 result = es.indices.create(index='godhearing', ignore=400) print(result) 删除索引 result = es.indices.delete(index='godhearing', ignore=[400, 404]) print(result) 插入数据 data = {'title': '天听', 'url': 'http://123.com','content':\"好耶耶耶\"} result = es.index(index='godhearing',body=data) print(result) index()方法会自动生成一个唯一id 也可以使用create()方法创建数据不同的是create()需要手动指定一个id 修改数据 data = {'content':\"啊哈哈哈哈哈哈哈嗝！\"} result = es.index(index='godhearing',body=data, id='插入数据时返回的id') print(result) 修改之后，仅剩余此字段 删除数据 result = es.delete(index='godhearing',id='插入数据时的ID') print(result) 查询数据 result = es.search(index='godhearing') print(result) 全文检索 mapping = { 'query': { 'match': { 'content': '嗝' } } } result = es.search(index='godhearing',body=mapping) print(result)","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}],"author":"天听"},{"title":"linux终端测试接口","slug":"linux终端测试接口","date":"2018-11-01T12:27:00.000Z","updated":"2018-11-01T12:27:00.000Z","comments":true,"path":"2018/11/01/linux-zhong-duan-ce-shi-jie-kou/","link":"","permalink":"http://godhearing.cn/2018/11/01/linux-zhong-duan-ce-shi-jie-kou/","excerpt":"","text":"前言 假如说，你只有一个linux终端来测试你的代码，你是不是瞬间觉得没有了梦想，发送参数，发送不同的请求，甚至查看返回结果，你面对的只有无穷无尽的黑色和白色的代码，是不是很绝望，今日带来一篇攻略，让你仅仅凭借一个终端来测试你的代码 命令 访问接口，除了写脚本测试之外，还可以直接使用终端来进行测试，具体我们要使用的命令，就是curl 这个命令可以访问网址，但是，他会返回的只有网页的源代码，所以，一般你访问完了，会看到这种情况 虽然可以查看到，但是也很不利于你的查看，所以，我们需要将curl命令后面，加上一个&gt;，紧接着，跟上你保存的文件名字，这样，他就会将访问的网页给你保存下来，像这样 curl http://baidu.com &gt; baidu.html 随后，你就会看到一个网页就保存下来了 其次，我们写的接口不只是get，还有其他的请求方法，这样该怎么做呢，只需添加一个X参数即可 curl -X POST http://127.0.0.1:8000/ ，这样你就发送了一个post请求，同样你也可以传参，使用d，不同参数之间使用&amp;来进行连接 curl -X POST -d'a=1&amp;b=2&amp;c=3' http://127.0.0.1:8000/ 再做个假如，你访问的是张图片，我们也可以通过 &gt; 来进行保存，然后查看 还有，如果在终端里发送文件，则需要-F参数，不过要注意的是，@一定不要少哦 curl -X POST -F 'avatar=@./ssss.jpg' http://127.0.0.1:8000/user/avatar/ 结语 这个场景呢，主要就是使用了windows系统，但是有些功能只能在linux下测试，但是我的linux云服务器又只有一个终端界面，出于无奈之举，只能使用这样的方法来进行。希望能帮到大家。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/tags/Linux/"}],"author":"天听"},{"title":"axios的封装","slug":"axios封装","date":"2018-10-07T12:25:00.000Z","updated":"2018-10-07T12:25:00.000Z","comments":true,"path":"2018/10/07/axios-feng-zhuang/","link":"","permalink":"http://godhearing.cn/2018/10/07/axios-feng-zhuang/","excerpt":"","text":"axios的封装 src文件夹下新建http文件夹，用来放网络请求相关的文件 src/http文件夹下，创建index.js文件，对axios进行封装 const&amp;nbsp;axios=require('axios');&amp;nbsp;&amp;nbsp;&amp;nbsp;//创建axios对象 axios.defaults.baseURL='http://127.0.0.1:8000/';&amp;nbsp;//vue请求后端地址 axios.defaults.timeout=1000;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;//多久超时 &amp;nbsp;axios.defaults.withCredentials=&amp;nbsp;true;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;//携带cookie需要添加 /** &amp;nbsp;*&amp;nbsp;设置请求传递数据的格式（看服务器要求的格式） &amp;nbsp;*&amp;nbsp;x-www-form-urlencoded &amp;nbsp;*&amp;nbsp;默认提交表单 &amp;nbsp;*&amp;nbsp;把数据对象序列化成&amp;nbsp;表单数据 &amp;nbsp;*/ //axios.default.headers['Content-Type']='application/x-www-form-urlencoded'; //axios.default.transformRequest=data&amp;nbsp;=>qs.stringify(data); /** &amp;nbsp;*设置默认提交JSON &amp;nbsp;*&amp;nbsp;把数据对象序列化成json字符串 &amp;nbsp;*/ axios.defaults.headers['Content-Type']='application/json'; axios.defaults.transformRequest=&amp;nbsp;data&amp;nbsp;=>JSON.stringify(data); //请求拦截器 axios.interceptors.request.use(config=>{ &amp;nbsp;&amp;nbsp;//从localStorage获取token &amp;nbsp;&amp;nbsp;let&amp;nbsp;token&amp;nbsp;=&amp;nbsp;localStorage.getItem('token'); &amp;nbsp;&amp;nbsp;//如果有token,就把token设置到请求头中Authorization字段中 &amp;nbsp;&amp;nbsp;token&amp;nbsp;&amp;&amp;(config.headers.Authorization=token); &amp;nbsp;&amp;nbsp;return&amp;nbsp;config; &amp;nbsp;&amp;nbsp;{,error=>{ &amp;nbsp;&amp;nbsp;return&amp;nbsp;Promise.reject(error); &amp;nbsp;&amp;nbsp;{ ); //响应拦截器 axios.interceptors.response.use(response=>{ &amp;nbsp;&amp;nbsp;//当响应码&amp;nbsp;2xx的情况，进入这里 &amp;nbsp;&amp;nbsp;return&amp;nbsp;response.data; &amp;nbsp;&amp;nbsp;{,error&amp;nbsp;=>&amp;nbsp;{ &amp;nbsp;&amp;nbsp;//当相应码不是2xx的情况，进入这里 &amp;nbsp;&amp;nbsp;return&amp;nbsp;error &amp;nbsp;&amp;nbsp;{ ); //get方法，对应get请求 //@params{String{&amp;nbsp;url[请求的url地址] //@params{Object{&amp;nbsp;params[请求时携带的参数] export&amp;nbsp;function&amp;nbsp;get(url,params,headers)&amp;nbsp;{ &amp;nbsp;&amp;nbsp;return&amp;nbsp;new&amp;nbsp;Promise((resolve,reject)=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;axios.get(url,{params,headers{).then(res=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resolve(res.data.ResultObj) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{).catch(err=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;reject(err.data) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{ &amp;nbsp;&amp;nbsp;) { //post方法，对应post请求 export&amp;nbsp;function&amp;nbsp;post(url,params,headers)&amp;nbsp;{ &amp;nbsp;&amp;nbsp;return&amp;nbsp;new&amp;nbsp;Promise((resolve,reject)=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;axios.post(url,params,headers).then((res)=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resolve(res.data) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{).catch((err)=>{ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;reject(err.data) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;{ &amp;nbsp;&amp;nbsp;) { export&amp;nbsp;default&amp;nbsp;axios; src/http目录下创建apis.js文件，用来写接口地址列表 //接口信息，生成请求方法 //请求后端django的地址 //引入get方法，post方法 import {get,post{ from './index' //用户登录 export const login = (params,headers) => post(\"/user/login/\",params,headers) CORS跨域#允许所有源来跨域 CORS_ORIGIN_ALLOW_ALL =True #或者设置跨域请求白名单 # CORS_ORIGIN_WHITELIST = ( # 'http://127.0.0.1:8080', # 'http://localhost:8080', # ) #允许携带cookie CORS_ALLOW_CREDENTALS = True 前后端联调思路 写完视图函数，使用postman进行接口测试，保证后端接口没有问题 在vue中写页面，向后端发送数据 const { data { = require('autoprefixer'); const axios = require('axios'); const { error { = require('shelljs'); axios.defaults.baseURL = 'http://192.168.56.100:8888' //vue请求后端地址 axios.defaults.timeout = 10000; // 超时设置 axios.defaults.withCredentials = true; // 跨域访问需要发送cookie时，一定要加上 axios.defaults.headers['Content-Type'] = 'application/json'; //设置默认提交json axios.defaults.transformRequest = data => JSON.stringify(data);//把数据对象序列化成json字符串 // 对请求拦截器，和响应拦截器进行封装 /* 请求拦截器，发送请求前需要调用这个函数 当没有登录时，直接跳转到登录页 请求发送前把token获取，设置到header中 */ axios.interceptors.request.use(config=>{ //从localStorage中获取token let token = localStorage.getItem('token'); // 如果有token，就把token设置到请求头中Authorization字段中 token &amp;&amp; (config.headers.Authorzation = token); return config; {,error => { return Promise.reject(error) {); /* 响应拦截器，当后端返回数据给vue时，会调用这个函数 每次返回403错误时，跳转到login */ axios.interceptors.response.use(response =>{ //当响应码是2xx的情况时，进入这里 return response.data; {,error =>{ //当响应码不是2xx的情况是，进入这里 return error {); //使用上面的axios对象，对get和post请求进行封装 /* get方法，对应get请求 @param {String{ url [请求的url地址] @param {Object{ params [请求时携带的参数] */ export function get(url,params,headers){ return new Promise((resolve,reject) =>{ axios.get(url,{params,headers{).then(res=>{ resolve(res.data.ResulObj) {).catch(err =>{ reject(err.data) {) {) { /* post方法，对应post请求 @param {String{ url [请求的url地址] @param {Object{ params [请求时携带的参数] */ export function post(url,params,headers){ return new Promise((resolve,reject) =>{ axios.post(url,params,headers).then(res=>{ resolve(res.data) {).catch(err =>{ reject(err.data) {) {) { //一定要导出函数 export default axios; def login(request): body_dict = json.loads(request.body) name = body_dict.get(\"name\") pwd = body_dict.get(\"pwd\") if not all([name,pwd]): resp = { 'code':1001, 'msg':'信息不全' { return JsonResponse(resp) if name == '天听' and pwd == '123456': res = { \"code\":0, \"msg\":\"登录成功\", \"data\":{ \"id\":1, \"name\":\"tianting\", \"age\":20 { { return JsonResponse(res) return JsonResponse({ \"code\":1002, \"msg\":'用户名或密码错误' {)","categories":[{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/categories/axios/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"},{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/tags/axios/"}],"author":"天听"},{"title":"Docker部署Django项目","slug":"Docker部署Django项目","date":"2018-07-21T14:00:00.000Z","updated":"2018-07-21T14:00:00.000Z","comments":true,"path":"2018/07/21/docker-bu-shu-django-xiang-mu/","link":"","permalink":"http://godhearing.cn/2018/07/21/docker-bu-shu-django-xiang-mu/","excerpt":"","text":"为什么要使用Docker 作为一种新兴的虚拟化方式，Docker跟传统的虚拟化方式相比具有众多的优势 首先，Docker的启动可以在秒级实现，这相比传统的虚拟机方式要快得多。 其次，Docker 对系统资源的利用率很高，一台主机上可以同时运行数千个 Docker 容器。 容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 简单来说，Docker是一个应用容器引擎，它包括了三个基本的概念，镜像(Image)，容器(Container)，仓库(Repository)，理解了这三个概念，就理解了Docker整个的生命周期 Docker镜像Docker镜像就是一个只读的模板，就好比于一个桶，桶里可以装任何东西，无论是一个程序，还是一个完整的ubuntu系统环境，里面仅安装了需要的其他应用程序，需要用什么，往桶里扔什么 镜像可以用来创建Docker容器 Docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用 docker load -i '镜像路径'用来引入镜像 docker pull mysql，拉去mysql镜像，当然不止mysql，几乎所有的工具都能通过pull来拉取镜像来搭建环境，例如mongo，redis等 Docker 容器 Dockers利用容器来运行应用 容器时从镜像创建的运行实例，它可以被启动、开始、停止、删除。每个容器都时相互隔离的，保证安全 docker run -it -p 80:80 镜像id：运行容器 -it：i以交互式模式运行容器,t为容器分配一个伪输入终端 -p：端口 80:80：端口映射宿主机port:容器port docker stop 容器id：关闭 docker ps:查看正在运行的容器 注意，启动时，使用的是镜像id，关闭时，使用的是容器id Docker 仓库 仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。 国内的公开仓库包括 Docker Pool 等，可以提供大陆用户更稳定快速的访问。 当然，用户也可以在本地网络内创建一个私有仓库。 当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。 Docker仓库的概念跟Git类似，注册服务器可以理解为Github这样的托管服务 部署Django 在安装好Docker之后，在宿主机安装gunicorn，容器内我们用异步的方式来启动Django pip isntall gunicorn gevent Django配置settings.py对应的应用: INSTALLED_APPS = [ 'gunicorn' ] 在Django项目的根目录编写gunicorn的配置文件：gunicorn.conf.py import multiprocessing bind = \"0.0.0.0:8000\" #绑定的ip与端口 workers = 1 #进程数 这里注意一点，ip必须是0.0.0.0，不要写成127.0.0.1,否则外部环境会访问不到容器内的服务 导出你的python对应的依赖列表: pip freeze > requirements.txt 在根目录编写Dockerfile文件 FROM python:3.7 WORKDIR /Project/mydjango COPY requirements.txt ./ RUN pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple COPY . . ENV LANG C.UTF-8 CMD [\"gunicorn\", \"mydjango.wsgi:application\",\"-c\",\"./gunicorn.conf.py\"] 打包 docker build -t '你的项目名' . 然后查看镜像 docker images 启动镜像服务 docker run -it --rm -p 8000:8000 你的项目名","categories":[{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"}],"author":"天听"},{"title":"工厂模式","slug":"工厂模式","date":"2018-07-14T16:00:00.000Z","updated":"2018-07-14T16:00:00.000Z","comments":true,"path":"2018/07/15/gong-han-mo-shi/","link":"","permalink":"http://godhearing.cn/2018/07/15/gong-han-mo-shi/","excerpt":"","text":"什么是工厂模式 所谓的工厂模式，是设计模式中比较常用的，这种类型的设计模式 在这种模式下，不会暴露创建逻辑，只是通过某个标识来确定实例化哪个工厂实例 简单的举个例子，如果你要买一辆汽车，不用知道他是怎样做出来的，只需要把参数传递进去，到时候来提车就好了。 无论是车还是什么，都不用管工厂内部是怎样实现的，只需要将参数传进去就可以 工厂模式从简到难，分为简单工厂模式，工厂方法模式以及抽象工厂模式 简单工厂模式简单工厂模式其实并不算是一种设计模式，更多的时候更像是一种编程习惯 定义一个工厂类，根据传入的参数返回不同的实例，被创建的实例具有共同的父类或接口 现实生活中，工厂是负责生产产品的；同样在设计模式中，简单工厂模式我们可以理解为负责生产对象的一个类，称为“工厂类”。 再举个栗子，假如，你要生产瓶子，客户说要什么样的瓶子，你就拿着这个瓶子的参数，去该瓶子的生产车间去取。 # 定义三个具体的工厂，他们用来生产具体的东西 class A: def __init__(self): self.name = 'A瓶子' class B: def __init__(self): self.name = 'B瓶子' class C: def __init__(self): self.name = 'C瓶子' # 工厂类 class factory(): def example(self,name): if name=='A': s = A() print(s.name) elif name == 'B': s = B() print(s.name) elif name == 'C': s = C() print(s.name) w = factory() w.example('B') 可以看到，只需要在工厂factory中，传入你想要什么瓶子，他就可以通过内部方法实例化某个工厂，从而达到你想要的某个东西 工厂模式 工厂方法模式，又称工厂模式、多态工厂模式和虚拟构造器模式，通过定义工厂父类负责定义创建对象的公共接口，而子类则负责生成具体的对象。 以上面的例子为例，假如要再多一个D瓶子，除了要新建一个D类，还要修改工厂类中的代码，这样就违背了软件设计中的开闭原则，即再扩展新的类时，尽量不要修改原有的代码 说的再简单点，就是工厂类被具体的工厂继承 # 工厂类 class factory(): @staticmethod def example(self): pass class A(factory): def __init__(self): self.name = 'A瓶子' def example(self): print(self.name) class B(factory): def __init__(self): self.name = 'B瓶子' def example(self): print(self.name) class C(factory): def __init__(self): self.name = 'C瓶子' def example(self): print(self.name) class D(factory): def __init__(self): self.name = 'D瓶子' def example(self): print(self.name) # 无论你加了多少个工厂，只需要实例化对应的那个具体工厂 w = D() w.example() 这样做的好处就是，在工厂类里，无需改动任何代码，只需要在需要的时候实例化某个具体工厂即可，便于维护 抽象工厂 提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。 抽象工厂模式中有工厂和产品簇的概念。而一簇的产品都是成套出现的。比如现在要给每个士兵发一套武器，包括枪和子弹。步枪和步枪子弹，手枪和手枪子弹。生产步枪的工厂就是步枪工厂，而生产手枪的工厂就是手枪工厂。步枪工厂和手枪工厂都是工厂，这就是一种抽象工厂的例子 用一个简单的代码来实现: import abc # 工厂类 class factory(): @staticmethod def Rifle(self): pass def Pistol(self): pass # M4工厂 class M4(category): def Rifle(self): return M4Rifle() def Pistol(self): return M4Pistol() # AK工厂 class AK(category): def Rifle(self): return AKRifle() def Pistol(self): return AKPistol() # M4工厂 class M4Rifle(): def __repr__(self): return 'M4A1步枪' class M4Pistol(): def __repr__(self): return 'M4A1手枪' class AKRifle(): def __repr__(self): return 'AK47步枪' class AKPistol(): def __repr__(self): return 'AK47手枪' w = M4().Rifle() print(w) 进入工厂类，无论是要AK系列的，还是M4系列的，都只实例化那个类就可以了，因为，无论是M4还是AK都能生产该类的手枪和步枪 虽然我也不知道他们是不是生产手枪 嗯…再画个图吧，low归low，但比较好理解","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"分布一致性算法(雪花算法)","slug":"分布一致性算法(雪花算法)","date":"2018-06-24T14:28:00.000Z","updated":"2018-06-24T14:28:00.000Z","comments":true,"path":"2018/06/24/fen-bu-yi-zhi-xing-suan-fa-xue-hua-suan-fa/","link":"","permalink":"http://godhearing.cn/2018/06/24/fen-bu-yi-zhi-xing-suan-fa-xue-hua-suan-fa/","excerpt":"","text":"什么是分布一致性 这是一个相当灵魂拷问的问题了，在我们应对千万级甚至亿级的资源访问时，一定会用到的一个词，就是分布式 因为一台服务器的承载力是有限的，而数据库更是有限的，数据表是更更更有限的，我们只能通过横向扩容的方式来进行数据存储量和查询速度的优化 可这样会造成一个问题，分库分表后，分布式系统中，唯一主键ID的生成问题，当我们使用mysql自增长主键时，他只在本表中是唯一的， 在分布式系统中，两张表都有一张ID为1的数据，那么显然就无法使用这个自增长了 如果我们说使用uuid，也可以凑活用，但是，第一他是无序的，第二，他占用空间巨大，耗费空间。 uuid只能适用于类似生成token令牌的场景 SnowFlake SnowFlake(雪花算法)，是Twitter提出来的一个算法，其目的是生成一个64bit的整数 1位标识符，始终是0 41位的时间戳 10位的机器标识码 前5位代表数据中心，后面5位是某个数据中心的机器ID 12位的递增序列 用来对同一个毫秒之内产生不同的ID，12位可以最多记录4095个，也就是在同一个机器同一毫秒最多记录4095个，多余的需要进行等待下毫秒。 python实操 首先安装pip install pysnowflake 然后终端启动snowflake服务 snowflake_start_server --worker=1 这里的worker就是当前节点的标识 上代码: import snowflake.client print(snowflake.client.get_guid()) # 多次输出一下，就会发现，有递增的连续性 将其转换成二进制码 print(bin(4431128039883018241)) 然后我们通过二进制码反推 第一位是标识符 往后41位是时间戳 从右方数，12位的递增序列，再数5位就是机器标识，这个机器标识就是某个节点的存储标识00001 但是啊，目前他是二进制，我们再将其转换为10进制 print(int('00001',2))","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"}],"author":"天听"},{"title":"ModelViewSet","slug":"ModelViewSet","date":"2018-06-18T12:41:00.000Z","updated":"2018-06-18T12:41:00.000Z","comments":true,"path":"2018/06/18/modelviewset/","link":"","permalink":"http://godhearing.cn/2018/06/18/modelviewset/","excerpt":"","text":"ModelViewSet是封装度最高的DRF的视图类。包含了怎删改查中的所有接口操作。 它继承自GenericViewSet、ListModelMixin、RetrieveModelMixin、CreateModelMixin、UpdateModelMixin、DestoryModelMixin。 ！！！！！注意，因为继承关系，必须在内部定义属性，queryset和serializer_class，因此，ModelViewSet通常结合ModelSerializer使用 所以，在使用ModelViewSet定义API时，我们只需要套用模板即可 使用视图集，可以将一系列逻辑相关的动作放到一个类中： list() 提供一组数据 retrieve() 提供单个数据 create() 创建数据 update() 更新数据 destory() 删除数据 视图集类不再实现get()、post()、put()、delete()方法，而是实现动作 action。 如 list()、retrieve()、create()、update()、destory() 请求 url 对应方法 备注 get 127.0.0.1:8000/app01/book/ list ListModelMixin get 127.0.0.1:8000/app01/book/{1{/ retrieve ….Mixin post 127.0.0.1:8000/app01/book/ create ….Mixin put 127.0.0.1:8000/app01/book/{1{/ update ….Mixin detete 127.0.0.1:8000/app01/book/{1{/ destroy ….Mixin get 127.0.0.1:8000/app01/book/ user_action useraction 自定义 post 127.0.0.1:8000/app01/book/ user_action useraction 自定义 #views中 class BookInfoViewSet(ModelViewSet): \"\"\"增删改查图书信息\"\"\" # 指定查询集 queryset = BookInfo.objects.all() # 指定序列化器 serializer_class = BookInfoModelSerializer 定义好了API视图后，需要在路由中，将请求方法与action进行绑定 from django.conf.urls import url from django.urls import path,include from . import views #导入路由控制类 from rest_framework.routers import SimpleRouter,DefaultRouter #实例化路由控制对象 router = DefaultRouter() #注册 router.register(r'book',views.BookInfoViewSet) #编写路由，固定写法，不清楚是否还有其他写法 urlpatterns = [ url(r'',include(router.urls)), ] action(自定义方法) DRF框架提供的action根本不能满足某些无良老板的奇葩需求,这时候，就需要自定义action 只需要在ModelViewSet定义的api类中，自定action函数后，再将自定义的函数在路由中绑定即可 def 函数名(self, request): ''' 自定义action return: JSON数据 ''' pass #示例： def latest(self, request): '''获取最后一条记录''' # 获取模型数据 book = BookInfo.objects.latest('id') # 获取序列化器对象 s = BookInfoModelSerializer(instance=book) return Response(s.data) #路由 urlpatterns = [ url(r'^books/$', views.BookInfoViewSet.as_view({'get':'action函数名'{)), #示例： url(r'^books/$', views.BookInfoViewSet.as_view({'get':'latest'{)), ] 之后，使用http://127.0.0.1:8000/app01/books/进行测试 或者使用装饰器方法： from rest_framework.decorators import action @action(methods=['get'],detail=False,url_path='bookw') def use(self,request): return Response({'name':'天听'{) ''' http://127.0.0.1:8000/app01/book/bookw/ 如果不加url_path，路由就成了http://127.0.0.1:8000/app01/book/use/ ''' 注意，此写法不需要再注册路由，需要在装饰器内部进行定义url_path如果不定义url_path，路由默认为函数名 detail=False表示不需要匹配主键的正则，函数不需要传入主键就设为False","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"数据库自定义字段","slug":"数据库自定义字段","date":"2018-04-26T14:01:00.000Z","updated":"2018-04-26T14:01:00.000Z","comments":true,"path":"2018/04/26/shu-ju-ku-zi-ding-yi-zi-duan/","link":"","permalink":"http://godhearing.cn/2018/04/26/shu-ju-ku-zi-ding-yi-zi-duan/","excerpt":"","text":"在以往的数据库使用中，我们通常在一个数据库里操作某些表，如果有很多字段不同但目的相同的表，则需要建立很多的表来完成需求，这时候，可以通过在一个字段里添加多个数据来实现 比如，工单的创建，假如有请假，外购两个工单，请假需要的字段为请假时间，请假理由，而外购的工单需要价格，理由，而两个工单的审批人也都不同，但是，要求两张工单都在同一张表里 相信很多入门的程序猿到这里已经要提刀砍人了，难道要把两个工单的全部字段建出来，然后根据不同的工单来添加哪些数据，不添加哪些数据吗？ 是的，上面的也是一个办法，但是，这样除了效率和辨识度的问题，还有一个最重要的问题就是，low 简单的来画个图吧 建立这样一张表，的确能够满足需求，只需要在创建外购的时候，请假的两个字段不填就好了(请忽略这张比较low的表) 而如果使用了自定义字段，就会是这样 怎么样，是不是简便的多了，只需要在取出该条数据的时候，多一个自定义字段的解析罢了，而且还节省空间和性能(请再次忽略这张low图) 推荐使用json数据类型储存自定义字段，这样无论是使用哪种语言来实现这项需求，都非常的简单","categories":[{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/categories/mysql/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"}],"author":"天听"},{"title":"DRF基本操作","slug":"DRF基本操作","date":"2018-04-07T14:28:00.000Z","updated":"2018-04-07T14:28:00.000Z","comments":true,"path":"2018/04/07/drf-ji-ben-cao-zuo/","link":"","permalink":"http://godhearing.cn/2018/04/07/drf-ji-ben-cao-zuo/","excerpt":"","text":"前言 DRF作为django的伴生框架，也封装了很多及其好用的东西 1.认证 2.权限 3.限流 4.序列化 5.分页 认证 Drf内置的四种API认证方式： 认证方式说明： **BasicAuthentication**每次提交请求的时候附加用户名和密码来进行认证 **TokenAuthentication**每次提交请求的时候在HTTP headers里附加Token进行认证 **SessionAuthentication**用户登录之后系统在cookies存入sessionid进行认证 **RemoteUserAuthentication**通过web服务器认证(apache/nginx这些) 我选择的是基于Token的认证，客户端登录之后维护一个token，每次请求附加到HTTP headers，还算是方便。 Drf还可以自定义认证方式，只要继承authentication.BaseAuthentication这个类然后重写authenticate方法就好了。 class MyAuthentication(BaseAuthentication): def authenticate(self, request): # 认证逻辑，如果认证通过，返回两个值 # 如果认证失败，抛出Authentication异常 token = request.GET.get('token') if token: user_token = UserToken.objects.filter(token=token).first() # 认证通过 if user_token: return user_token.user,token else: raise AuthenticationFailed('认证失败') else: raise AuthenticationFailed('请求地址中需要携带token') 然后在视图中使用即可 class StudentViewSet(viewsets.ModelViewSet): authentication_classes = [SessionAuthentication, BasicAuthentication,MyAuthentication] permission_classes = [IsAuthenticated] queryset = Student.objects.all() serializer_class = StudentSerializer 创建认证类：继承BaseAuthentication、重写authenticate方法 authenticate()返回值 None：当前认证不管，等下一个认证来执行 raise exceptions.AuthenticationFailed('用户认证失败') 有返回值元组形式：（元素1，元素2）元素1复制给request.user、元素2复制给request.auth 在settings.py中可以配置默认的认证方式，这里我添加了三个： REST_FRAMEWORK = { # 身份验证 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework.authentication.BasicAuthentication', 'rest_framework.authentication.TokenAuthentication', 'rest_framework.authentication.SessionAuthentication', ) } 权限API授权Drf的接口权限有以下几种： **AllowAny**：允许所有，登不登录无所谓 **IsAuthenticated**：登录了才能访问 **IsAdminUser**：管理员才能访问 **IsAuthenticatedOrReadOnly**：顾名思义，不登录只读，登录才能写入 **DjangoModelPermissions**：根据Django Auth的配置（权限细化到每个model） DjangoModelPermissionsOrAnonReadOnly **DjangoObjectPermissions**：配合第三方权限控制，细化到每个对象 一般来说小网站用到DjangoModelPermissions就是够用的，或者干脆简单一点，用IsAuthenticated和queryset限定请求的数据即可。 介绍完了基本概念，来看看代码中是如何操作的。 对于操作用户信息的viewset，我只用了permissions.IsAuthenticated这个权限，然后覆盖了ReadOnlyModelViewSet的get_queryset方法，把queryset变成只包括当前用户，这样就保证了一个用户只能操作自己的信息。 from rest_framework import authentication, permissions, viewsets class UserViewSet(viewsets.ReadOnlyModelViewSet): permission_classes = [permissions.IsAuthenticated] serializer_class = UserSerializer def get_queryset(self): return User.objects.filter(pk=self.request.user.pk) viewset的action同样可以使用权限，加在装饰器的参数上即可： @action(detail=True, methods=['GET'], permission_classes=[permissions.IsAuthenticated]) def some_actions(self, request, pk=None): dosomething return Response(SomeSerializer(some_data, many=True).data) 这里提一下装饰器的detail参数，这个代表了是对列表操作还是对单个对象操作，True就是对单个对象。 ApiView和ViewSet同样通过在类字段中加入authentication_classes和permission_classes实现认证和授权。 分页 PAGINATIONDrf和Django一样自带分页功能，很好用（当然也支持使用第三方的分页功能）。 首先进行配置（不配置的话使用默认配置），这里我设置每页显示十条记录： REST_FRAMEWORK = { # 设置分页 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', 'PAGE_SIZE': 10, } 使用得最多的ModelViewSet已经自带分页了，这个我们不用操心，不过如果自己定义了action来返回列表数据的话，就没有分页，这时候要用paginate_queryset方法来处理。 代码如下： @action(detail=False) def tag(self, request): queryset = SomeModel.objects.all().order_by('-add_time') page = self.paginate_queryset(queryset) if page is not None: return self.get_paginated_response(self.get_serializer(page, many=True).data) return Response(self.get_serializer(queryset, many=True).data) 可以看出Drf自动处理了不同页面的请求，不用像Django一样自己从GET或者POST数据里读取page，分页相关的方法直接在viewset对象里面，非常方便。 限流其实就是一个自定义的认证过程。 Drf内置有BaseThrottle、SimpleRateThrottle，后者是前者的之类。 BaseThrottle 需要自己写allow_request和wait方法，控制粒度更细 SimpleRateThrottle重写get_cache_key和设置scope名称就可以，更简单 实现1分钟内只能访问3次的限流SimpleRateThrottle代码如下： from rest_framework.throttling import SimpleRateThrottle class VisitThrottle(SimpleRateThrottle): '''匿名用户60s只能访问三次（根据ip）''' scope = 'throttle' #这里面的值，自己随便定义，settings里面根据这个值配置throttle def get_cache_key(self, request, view): #通过ip限制节流 return self.get_ident(request) class UserThrottle(SimpleRateThrottle): '''登录用户60s可以访问10次''' scope = 'userThrottle' #这里面的值，自己随便定义，settings里面根据这个值配置userThrottle def get_cache_key(self, request, view): return request.user.user_id BaseThrottle 代码如下： from rest_framework.throttling import BaseThrottle import time VISIT_RECORD = {} #保存访问记录 class VisitThrottle(BaseThrottle): '''60s内只能访问3次''' def __init__(self): self.history = None #初始化访问记录 def allow_request(self,request,view): #获取用户ip (get_ident) remote_addr = self.get_ident(request) ctime = time.time() #如果当前IP不在访问记录里面，就添加到记录 if remote_addr not in VISIT_RECORD: VISIT_RECORD[remote_addr] = [ctime,] #键值对的形式保存 return True #True表示可以访问 #获取当前ip的历史访问记录 history = VISIT_RECORD.get(remote_addr) #初始化访问记录 self.history = history #如果有历史访问记录，并且最早一次的访问记录离当前时间超过60s，就删除最早的那个访问记录， #只要为True，就一直循环删除最早的一次访问记录 while history and history[-1] < ctime - 60: history.pop() #如果访问记录不超过三次，就把当前的访问记录插到第一个位置（pop删除最后一个） if len(history) < 3: history.insert(0,ctime) return True def wait(self): '''还需要等多久才能访问''' ctime = time.time() return 60 - (ctime - self.history[-1]) 配置节流#全局 REST_FRAMEWORK = { # 设置全局节流 \"DEFAULT_THROTTLE_CLASSES\":['api.utils.throttle.UserThrottle'], #全局配置，登录用户节流限制（10/m） # 设置访问频率 \"DEFAULT_THROTTLE_RATES\":{ 'throttle':'3/m', #没登录用户3/m，throttle就是scope定义的值,通过IP地址 'userThrottle':'10/m', #登录用户10/m，userThrottle就是scope定义的值， 通过user_id } } # 局部：在类视图中添加 throttle_classes = [VisitThrottle,] 原文地址","categories":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"聊天室","slug":"聊天室","date":"2018-04-02T04:05:00.000Z","updated":"2018-04-02T04:05:00.000Z","comments":true,"path":"2018/04/02/liao-tian-shi/","link":"","permalink":"http://godhearing.cn/2018/04/02/liao-tian-shi/","excerpt":"","text":"最近，在写一个简单的基于websocket和django的聊天室，来分享一下我这个过程 首先是思路，聊天室说起来简单，但实际操作起来完全不是那么回事了，除了简单的连接之外，完全没有一点头绪 把聊天室的功能梳理了一下，然后拆开，分成了前端连接，前端发送，后端连接，实时推送消息，记录保存这五个方面 先来说一说前端连接的问题，其实也就是一个简单的websocket连接，不过是增加了一个断开重连 websocketlink(uid){ if('WebSocket' in window){ // 生成websocket链接 console.log('支持') var ws = new WebSocket('ws://192.168.1.157:8000/chat_room_websocket/?uid='+uid); // var ws = new WebSocket('ws://192.168.1.157:8000/chat_room_websocket/'); // 连接成功 ws.onopen = function(){ ws.send(uid); { // 收到数据 ws.onmessage=(evt)=>{ // 将获取信息打印 var received_msg = evt.data this.msglist.push(JSON.parse(received_msg)) console.log(evt) // 连接关闭 ws.onclose=()=>{ console.log('链接已关闭') this.websocketlink(localStorage.getItem('uid')) { // 连接报错 ws.error=()=>{ this.websocketlink(localStorage.getItem('uid')) { 这里，我把连接封装起来，在报错或者关闭时，重新连接 页面操作上，点击按钮，发送消息，这都是简单的请求API接口的操作，这里就不多详述了 后端连接，也是一个简单的dwebsocket连接 clients = {{ # 链接websocket接口 @accept_websocket def chat_room_websocket(request): if request.is_websocket(): while True: message = request.websocket.wait() if not message: break else: # 连接的用户的id uid = message.decode() # 加入到字典中 clients[uid] = request.websocket 和之前一样，定义公共变量，将连接的id和连接对象放进去 存储，发送消息不难，存储其实也不难，无论是存到mysql还是redis还是文件都可以自行选择，这里，我存的是自己的文件，根据用户的id生成文件，时间戳+消息追加性的存储 # 时间戳，毫秒，为了之后取消息记录，做一下准备 t = time.time() timestamp1 = int(round(t * 1000)) # 加个换行，直接存 s = str(timestamp1)+ ':' + msg + '\\n' with open('chat_record/%s.txt' % uid, 'a', encoding='utf-8') as f: f.write(s) 实时推送消息，如果有人在聊天室发了消息，却看不到，这就非常的伤脑筋，为了实现简单的推送消息，将公共变量中的所有连接，遍历一下，然后发送给所有人 # 遍历所有的连接用户 for client in clients: # 构造返回数据，需要编一下码 message = json.dumps({'username':username,'msg':msg{,ensure_ascii=False) # 发送消息 clients[client].send(message.encode('utf-8')) 后端推送，这里我是传了图片，为了更加的人性化。只发文字不发图片的聊天室是没有灵魂的 另外，我做了一个redis限流，这个可以忽略不计 另外的另外， 因为我前端稀碎， 有些多余的代码可以自行过滤一下 class SendMessage(APIView): def post(self,request): # 传图片和发送消息是两个接口，所以，获取用户id的方法也不一样，这里写的多余了 msg = request.data.get('msg') uid = request.data.get('uid') image = request.FILES.get('file') if image: uid = request.GET.get('uid') # redis限流 redis_client = get_redis_connection('chat_room') try: redis_client.get(uid) except Exception as e: print(e) return Response({'code': 1001, 'msg': '发送频繁'{) redis_client.hincrby(uid,'num') redis_client.expire(uid, 5) # 这里的名字是根据连接查了一下数据库，可以忽略 username = User.objects.filter(id=uid).first().username # 如果要获取两个人的聊天记录，在创建文件时，可以用两个人的id来组成文件名 # 构造时间戳 t = time.time() timestamp1 = int(round(t * 1000)) # 如果传的是消息，发送消息顺便存储到本地 if msg: for client in clients: message = json.dumps({'username':username,'msg':msg{,ensure_ascii=False) clients[client].send(message.encode('utf-8')) s = str(timestamp1)+ ':' + msg + '\\n' with open('chat_record/%s.txt' % uid, 'a', encoding='utf-8') as f: f.write(s) # 写图片文件,如果传了文件，则写入到文件中，然后拼接一个url返回 elif image: with open(os.path.join(CHAT_RECORD_ROOT, '', image.name), 'wb') as f: for chunk in image.chunks(): f.write(chunk) message = json.dumps({'username': username, 'msg':('http://192.168.1.157:8000/static/chat_record/'+image.name){, ensure_ascii=False) for client in clients: clients[client].send(message.encode('utf-8')) s = str(timestamp1)+':'+'http://192.168.1.157:8000/static/chat_record/'+str(image.name) + '\\n' with open('chat_record/%s.txt' % uid, 'a', encoding='utf-8') as f: f.write(s) return Response({'message': 'ok'{) 发送与渲染 判断：indexOf() —&gt;判断是否有某个元素，格式为 字符串&amp;&amp;字符串.indexOf(‘子串’) 如果不存在，则返回一个**-1** &lt;a-modal v-model='show' @ok=\"myok\"> &lt;span v-for=\"i in msglist\" :key=\"i.username\"> &lt;p v-if='i.msg &amp;&amp; i.msg.indexOf(\".jpg\")!=-1'>{{i.username{{:&lt;a-avatar :size=\"44\" :src=i.msg style='margin-left: 20px'>&lt;/a-avatar>&lt;/p> &lt;p v-else>{{i.username{{:{{i.msg{{&lt;/p> &lt;/span> &lt;span> &lt;a-form-item label=\"发送消息\" v-bind=\"formlayout\" > &lt;a-input v-model=\"msg\" /> &lt;/a-form-item> &lt;a-form-item v-bind=\"buttonlayout\"> &lt;a-button type='primary' @click=\"send_msg\">发送消息&lt;/a-button> &lt;a-upload name=\"file\" :multiple=\"true\" :action=uid :headers=\"headers\" @change=\"handleChange\" > &lt;a-button> &lt;a-icon type=\"upload\" /> 发送文件 &lt;/a-button> &lt;/a-upload> &lt;/a-form-item> &lt;/span> &lt;/a-modal> 一个简单的聊天室就做好啦","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"}],"author":"天听"},{"title":"Celery","slug":"Celery","date":"2018-02-13T14:25:00.000Z","updated":"2018-02-13T14:25:00.000Z","comments":true,"path":"2018/02/13/celery/","link":"","permalink":"http://godhearing.cn/2018/02/13/celery/","excerpt":"","text":"介绍：1.1 celery应用举例 Celery 是一个 基于python开发的分布式异步消息任务队列，通过它可以轻松的实现任务的异步处理，如果你的业务场景中需要用到异步任务，就可以考虑使用celery 你想对100台机器执行一条批量命令，可能会花很长时间 ，但你不想让你的程序等着结果返回，而是给你返回 一个任务ID,你过一段时间只需要拿着这个任务id就可以拿到任务执行结果， 在任务执行ing进行时，你可以继续做其它的事情 Celery 在执行任务时需要通过一个消息中间件来接收和发送任务消息，以及存储任务结果， 一般使用rabbitMQ or Redis 1.2 Celery有以下优点 简单：一单熟悉了celery的工作流程后，配置和使用还是比较简单的 高可用：当任务执行失败或执行过程中发生连接中断，celery 会自动尝试重新执行任务 快速：一个单进程的celery每分钟可处理上百万个任务 灵活： 几乎celery的各个组件都可以被扩展及自定制 1.3 Celery 特性 方便查看定时任务的执行情况, 如 是否成功, 当前状态, 执行任务花费的时间等. 可选 多进程, Eventlet 和 Gevent 三种模型并发执行. Celery 是语言无关的.它提供了python 等常见语言的接口支持. Celery组件2.1 Celery 扮演生产者和消费者的角色 Celery Beat : 任务调度器. Beat 进程会读取配置文件的内容, 周期性的将配置中到期需要执行的任务发送给任务队列. Celery Worker : 执行任务的消费者, 通常会在多台服务器运行多个消费者, 提高运行效率. Broker : 消息代理, 队列本身. 也称为消息中间件. 接受任务生产者发送过来的任务消息, 存进队列再按序分发给任务消费方(通常是消息队列或者数据库). Producer : 任务生产者. 调用 Celery API , 函数或者装饰器, 而产生任务并交给任务队列处理的都是任务生产者. Result Backend : 任务处理完成之后保存状态信息和结果, 以供查询. 2.2 celery架构图 2.3 产生任务的方式 发布者发布任务(WEB 应用) 任务调度按期发布任务(定时任务) 2.4 celery 依赖三个库: 这三个库, 都由 Celery 的开发者开发和维护. billiard : 基于 Python2.7 的 multisuprocessing 而改进的库, 主要用来提高性能和稳定性. librabbitmp : C 语言实现的 Python 客户端 kombu : Celery 自带的用来收发消息的库, 提供了符合 Python 语言习惯的, 使用 AMQP 协议的高级借口. 配置Celery安装celerypip install celery @ https://github.com/celery/celery/tarball/master 新建celery/main.py配置celery# celery_task/main.py import os from celery import Celery # 定义celery实例, 需要的参数, 1, 实例名, 2, 任务发布位置, 3, 结果保存位置 app = Celery('mycelery', broker='redis://127.0.0.1:6379/14', # 任务存放的地方 backend='redis://127.0.0.1:6379/15') # 结果存放的地方 @app.task def add(x, y): return x + y 测试启动celery'''1.启动celery''' #1.1 单进程启动celery celery -A main worker -l INFO #1.2 celery管理 celery multi start celery_test -A celery_test -l debug --autoscale=50,5 # celery并发数：最多50个，最少5个 ps auxww|grep \"celery worker\"|grep -v grep|awk '{print $2{'|xargs kill -9 # 关闭所有celery进程 1.使用celery异步发送短信1.1 在celery_task/mian.py中添加发送短信函数# celery项目中的所有导包地址, 都是以CELERY_BASE_DIR为基准设定. # 执行celery命令时, 也需要进入CELERY_BASE_DIR目录执行. CELERY_BASE_DIR = os.path.dirname(os.path.abspath(__file__)) @app.task(bind=True) def send_sms_code(self, mobile, datas): sys.path.insert(0, os.path.join(CELERY_BASE_DIR, '../syl')) # 在方法中导包 from libs.rl_sms import send_message # time.sleep(5) try: # 用 res 接收发送结果, 成功是:０， 失败是：－１ res = send_message(mobile, datas) except Exception as e: res = '-1' if res == '-1': # 如果发送结果是 -1 就重试. self.retry(countdown=5, max_retries=3, exc=Exception('短信发送失败')) 1.2 在verifications/views.py中添加celery发送短信视图函数class SmsCodeView(APIView): \"\"\"使用apiview的限流\"\"\" # 1. 所有人可以访问 permission_classes = (AllowAny,) def post(self, request): # 1. 获取参数 phone = request.data.get('phone') # 手机号 image_code = request.data.get('image_code') # 图片验证码 image_code_uuid = request.data.get('image_code_uuid') # 前端生成的uuid # 2. 检查参数 if not all([phone, image_code, image_code_uuid]): return Response({\"code\": 999, \"msg\": \"参数不全\"{) if not re.match(r'^1[3456789]\\d{9{$', phone): return Response({\"code\": 999, \"msg\": \"手机号码不正确\"{) # 3. 检查是否发送 redis_client = get_redis_connection('img_code') phone_exists = redis_client.get(phone) if phone_exists: return Response({\"code\": 999, \"msg\": \"频繁发送, 请稍后再试\"{) # 验证图形验证码 redis_image_code = redis_client.get(image_code_uuid) # bytes if redis_image_code: # bytes 转成 string redis_image_code = redis_image_code.decode() # 比较用户提供的图片内容是否和redis中保存的一致 if image_code.upper() != redis_image_code: return Response({'code': 999, 'msg': '图片验证码不正确'{) # 4. 发送 code = '%06d' % random.randint(0, 999999) # 随机6位验证码 from syl.settings import BASE_DIR sys.path.insert(0, os.path.join(BASE_DIR, '../celery_task')) from main import send_sms_code # 必须这么写, 从main中导包 send_sms_code.delay(phone, (code, \"5\")) print(code) # 5.使用 pipeline 批量操作 pl = redis_client.pipeline() # 实例化pipeline对象 pl.setex(phone, 60 * 5, code) # 存储phone:code, 5分钟有效期 pl.delete(image_code_uuid) # 从redis中删除这个图片验证码, 以防再次被使用 pl.execute() # 6. 返回结果 return Response({\"code\": 0, \"msg\": \"短信发送成功\"{) 1.3 添加路由urlpatterns = [ path('sms_codes/', views.SmsCodeView.as_view()), ] 2.测试接口 接口URL http://192.168.56.100:8888/user/sms_codes/ 请求携带参数 { \"phone\": 18538752511, \"image_code\":\"aed3\", # 前端生成的 图形验证码 \"image_code_uuid\":\"de8edce2-fc9f-11ea-9325-005056c00008\" # 前端生成的uuid { django添加检查用户名和手机号数量接口1.1 在user/urls.py中添加urlpatterns = [ path('count/', views.RegCountView.as_view()), # 查询用户名手机号使用量的视图, /user/count/ ] 1.2 在user/views.py中添加视图函数# 查询用户数量接口 class RegCountView(APIView): # 注册时需要验证的用户名和手机号是否使用 # 自定义权限类 permission_classes = (AllowAny,) def post(self, request): # 接收参数: 验证的内容type: username/phone, data: '用户名' 或者 '手机号', datatype = request.data.get('type') data = request.data.get('data') if not all([data, datatype]): return Response({'code': 999, 'msg': '参数不完整'{) if datatype == 'username': count = User.objects.filter(username=data).count() if datatype == 'phone': count = User.objects.filter(phone=data).count() return Response({'code': 0, 'msg': '查询成功', 'data': {'type': datatype, 'count': count{{) 测试接口 测试接口URL http://192.168.56.100:8888/user/count/ 完善注册接口修改user/views.py中完善视图函数# 注册接口 class RegisterView(APIView): \"\"\" 用户注册, 权限是: 匿名用户可访问 \"\"\" # 自定义权限类 permission_classes = (AllowAny,) def post(self, request): \"\"\" 接收用户名,密码,手机号和验证码, 前端校验两遍一致性, 注册成功后返回成功, 然后用户自行登录获取token 1. 用户名 2. 密码 3. 手机号 4. 验证码 :param request: :return: {'code':0,'msg':'注册成功'{ code: \"260361\" password: \"123123\" phone: \"13303479527\" username: \"liangxuepeng\" \"\"\" username = request.data.get('username') phone = request.data.get('phone') code = request.data.get('code') passwrod = request.data.get('password') if all([username, passwrod, phone, code]): pass else: return Response({'code': 999, 'msg': '参数不全'{) # rand_name = self.randomUsername() # 验证手机验证码 redis_client = get_redis_connection('verify_code') code_redis = redis_client.get(phone) if code_redis: code_redis = code_redis.decode() if not code == code_redis: return Response({'code': 999, 'msg': '手机验证码错误'{) user = User(username=username, phone=phone) user.set_password(passwrod) user.save() return Response({'code': 0, 'msg': '注册成功'{)","categories":[{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/categories/Celery/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/tags/Celery/"}],"author":"天听"},{"title":"Python的垃圾回收机制","slug":"python的垃圾回收机制","date":"2018-02-02T12:34:00.000Z","updated":"2018-02-02T12:34:00.000Z","comments":true,"path":"2018/02/02/python-de-la-ji-hui-shou-ji-zhi/","link":"","permalink":"http://godhearing.cn/2018/02/02/python-de-la-ji-hui-shou-ji-zhi/","excerpt":"","text":"浅谈一下python的内存管理机制 python采用的是引用计数机制为主，标记-清除和分代回收两种机制为辅的策略 在python中，每一个对象的核心就是一个结构体，它的内部有一个引用计数器，程序运行过程中会实时的更新ob_refcnt的值，来反映引用当前对象的名称数量，当某个对象的引用计数为0，那么它的内存就会被立即释放掉 对象存储 在Python中万物皆对象 不存在基本数据类型，0, 1.2, True, False, \"abc\"等，这些全都是对象 所有对象, 都会在内存中开辟一块空间进行存储 分代回收 分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为三代，分别为年轻代(0代)，中年代(1代)，老年代(2代)，他们对应的是三个链表，它们的垃圾收集频率与对象的存活时间的增大而减小 新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，以此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。 同时，分代回收是建立在标记清除技术基础之上。作为Python的辅助垃圾收集技术处理那些容器对象 垃圾回收有三种情况会触发垃圾回收： 调用gc.collect(),需要先导入gc模块。 当gc模块的计数器达到阀值的时候。 程序退出的时候。 gc模块gc模块提供一个接口给开发者设置垃圾回收的选项。上面说到，采用引用计数的方法管理内存的一个缺陷是循环引用，而gc模块的一个主要功能就是解决循环引用的问题。 多次赋值 对于整数和短小的字符，Python会进行缓存，不会创建多个相同对象 此时，被多次赋值，只会有多份引用 id和hex在python的内置函数中，可以通过id获取内存地址(10进制)，通过hex()可以查看16进制地址","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"Hexo解决引用本地图片无法显示","slug":"Hexo解决引用本地图片无法显示","date":"2018-01-24T13:06:00.000Z","updated":"2018-01-24T13:06:00.000Z","comments":true,"path":"2018/01/24/hexo-jie-jue-yin-yong-ben-di-tu-pian-wu-fa-xian-shi/","link":"","permalink":"http://godhearing.cn/2018/01/24/hexo-jie-jue-yin-yong-ben-di-tu-pian-wu-fa-xian-shi/","excerpt":"","text":"最近用起hexo，但是发现文章中引用本地图片时总是显示不出来 花费了许久时间才解决了这个问题，因此将一些解决经验整理出来，希望能帮助到大家 一、插件安装与配置首先我们需要安装一个图片路径转换的插件，这个插件名字是hexo-asset-image npm install https://github.com/CodeFalling/hexo-asset-image --save 打开/node_modules/hexo-asset-image/index.js，将内容更换为下面的代码 'use strict'; var cheerio = require('cheerio'); // http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string function getPosition(str, m, i) { return str.split(m, i).join(m).length; } var version = String(hexo.version).split('.'); hexo.extend.filter.register('after_post_render', function(data){ var config = hexo.config; if(config.post_asset_folder){ var link = data.permalink; if(version.length > 0 &amp;&amp; Number(version[0]) == 3) var beginPos = getPosition(link, '/', 1) + 1; else var beginPos = getPosition(link, '/', 3) + 1; // In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\". var endPos = link.lastIndexOf('/') + 1; link = link.substring(beginPos, endPos); var toprocess = ['excerpt', 'more', 'content']; for(var i = 0; i &lt; toprocess.length; i++){ var key = toprocess[i]; var $ = cheerio.load(data[key], { ignoreWhitespace: false, xmlMode: false, lowerCaseTags: false, decodeEntities: false }); $('img').each(function(){ if ($(this).attr('src')){ // For windows style path, we replace '\\' to '/'. var src = $(this).attr('src').replace('\\\\', '/'); if(!/http[s]*.*|\\/\\/.*/.test(src) &amp;&amp; !/^\\s*\\//.test(src)) { // For \"about\" page, the first part of \"src\" can't be removed. // In addition, to support multi-level local directory. var linkArray = link.split('/').filter(function(elem){ return elem != ''; }); var srcArray = src.split('/').filter(function(elem){ return elem != '' &amp;&amp; elem != '.'; }); if(srcArray.length > 1) srcArray.shift(); src = srcArray.join('/'); $(this).attr('src', config.root + link + src); console.info&amp;&amp;console.info(\"update link as:-->\"+config.root + link + src); } }else{ console.info&amp;&amp;console.info(\"no src attr, skipped...\"); console.info&amp;&amp;console.info($(this)); } }); data[key] = $.html(); } } }); 打开_config.yml文件，修改下述内容 post_asset_folder: true 然后，无论是自己创建MarkDown文件还是使用命令创建，如果传了图片，图片一定要保存到跟MarkDown文件名一致的文件夹中，然后在Markdown中通过相对路径来进行引用 好啦，问题解决的简单粗暴，希望大家能够相互帮助，在码农的世界里一起成长","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/tags/Hexo/"}],"author":"天听"},{"title":"Hexo搜索功能","slug":"Hexo搜索功能","date":"2018-01-24T03:21:00.000Z","updated":"2018-01-24T03:21:00.000Z","comments":true,"path":"2018/01/24/hexo-sou-suo-gong-neng/","link":"","permalink":"http://godhearing.cn/2018/01/24/hexo-sou-suo-gong-neng/","excerpt":"","text":"Hexo提供了一个插件来实现搜索的功能，但是，不知道什么原因，按照以下步骤操作，我的搜索功能还是没能实现 npm install hexo-generator-search --save 然后在hexo根目录的_config.yml下添加： search: path: search.xml field: post format: html limit: 100 在你的主题themes目录下找到_config.yml，然后添加: local_search: enable: true trigger: auto top_n_per_article: 1 一般来说，这时候就已经可以实现搜索功能了，但是我遇到的错误比较奇怪，之后，经过一系列的试验，终于找到了解决办法 遇到难点的朋友们，可以尝试一下 找到你博客主题的search.ejs模板文件，注意是ejs,修改下面代码 &lt;script type=\"text/javascript\"> $(function () { searchFunc(\"&lt;%= config.root %>\" + \"search.xml\", 'searchInput', 'searchResult'); }); &lt;/script> 也是改为json格式的文件 现在的网上，基本上遇到一个问题，几乎回答全是千篇一律，毫无营养可言，只要一条路走不通，就走不通了，非常的让人抓狂，但是，还有不少真正的大神在认真的在为小白解答，遇到这种情况，希望不要浮躁，慢慢的寻找解决办法","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/tags/Hexo/"}],"author":"天听"},{"title":"FastAPI学习之路-2 SQLAlchemy","slug":"FastAPI学习之路-2 SQLAlchemy","date":"2018-01-15T13:13:00.000Z","updated":"2018-01-15T13:13:00.000Z","comments":true,"path":"2018/01/15/fastapi-xue-xi-zhi-lu-2-sqlalchemy/","link":"","permalink":"http://godhearing.cn/2018/01/15/fastapi-xue-xi-zhi-lu-2-sqlalchemy/","excerpt":"","text":"SQLAlchemy简介 它是一个ORM(Object-Relational Mapping)框架，是python最好的ORM工具之一，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型 提示一点：ORM并非是非用不可，但是，为了代码的健壮性，我们最好还是使用ORM，一方面，为了提高开发效率，另一方面，为了使别人能够看懂代码。 安装安装sqlalchemy： pip install sqlalchemy 连接数据库 所有的关系型数据库，都可以通过这样连接 '数据库类型+数据库驱动名称://用户名:密码@机器地址:端口号/数据库名' 我们就拿mysql打个比方： 需要安装pymysql，因为引擎基于pymysql pip install pymysql # 首先，导包 from sqlalchemy import create_engine # 创建数据库连接 connect = create_engine('mysql+pymysql://root:mySQL@localhost:3306/test01') # 可以顺便指定一下编码方式 # create_engine('mysql+pymysql://root:mySQL@localhost:3306/test01?charset=utf8') 这样，我们就能连接上数据库了 描述表结构(模型) 要使用 ORM, 我们需要将数据表的结构用 ORM 的语言描述出来。SQLAlchmey 提供了一套 Declarative 系统来完成这个任务。我们以创建一个 user 表为例，看看它是怎么用 SQLAlchemy 的语言来描述的 from sqlalchemy.ext.declarative import declarative_base ModelBase = declarative_base() #&lt;-元类 class User(ModelBase): # 定义表名 __tablename__ = \"user\" # 声明字段 id = Column(Integer, primary_key=True) username = Column(String(length=255)) password = Column(String(length=255)) 这个模型，是对数据表结构的映射，无论是进行迁移文件还是操作数据库，都需要使用到它 一对多关系 一对多，也就是我们所谓的外键，假如，有个user表，要求每个user都要有一个角色，我们就需要另外建立一张角色表，来通过外键连接角色表，用来实现关联，一个角色可以被多个user关联，一个user只能有一个角色，这，就是一对多的关系 class User(ModelBase): # 定义表名 __tablename__ = \"user\" # 声明字段 id = Column(Integer, primary_key=True) username = Column(String(length=255)) password = Column(String(length=255)) role = Column(Integer,ForeignKey('role.id')) class Role(ModelBase): __tablename__ = 'role' id = Column(Integer, primary_key=True) name = Column(String(length=255)) isnn = relationship('User') 可以看到，我们定义了ForeignKey来指定关联哪张表，连接哪个字段 而“一”这里，提供了一个relationship方法来表明两个模型中的关系 而relationship有一个backref，来指定反向访问的属性名称，在我们的这个例子中，就是用来反向访问，都有哪些用户是这个角色 添加数据（Create）我们依赖上方的代码来进行添加 ⚠！！！在SQLAlchemy中，CRUD都是通过会话(session)进行的，所以我们必须要先创建会话，每一个SessionLocal实例就是一个数据库session from sqlalchemy.orm import sessionmaker # 创建DBSession类型 # flush()是指发送数据库语句到数据库，但数据库不一定执行写入磁盘；commit()是指提交事务，将变更保存到数据库文件 DBSession = sessionmaker(bind=connect,autoflush=False, autocommit=False, expire_on_commit=True) # 创建session对象 session = DBSession() # 创建新User对象(我们要添加的数据) new_user = User(id=1,username='Bob',password='123') # 添加到session session.add(new_user) # 提交即保存到数据库 session.commit() # 关闭session session.close() ok，执行一下，可以看到我们的数据库中已经有了这条数据 可见，关键是获取session，然后把对象添加到session，最后提交并关闭。`DBSession`对象可视为当前数据库连接。 查找数据（Retrieve） 重新得到数据 user = session.query(User).filter(User.id==5).one() print(type(user)) print('name',user.username) 这只是查找第一条 如果要获取多条，那么就可以将one换成all user = session.query(User).filter(User.id==5).all() print(type(user)) for i in user: print(i.username) 注意，因为获取多条，他的类型是列表，所以，我们需要处理一下 更新数据（Update） 首先呢，我们需要找出需要修改的数据，然后直接进行update修改 session.query(User).filter(User.id==1).update({'username':'haoye'{) session.commit() 删除数据（delete） 操作和update一致，只是最后换成dalete session.query(User).filter(User.id==1).delete() session.commit() 总结：最为简单的CRUD操作并没有什么难度，只是需要注意的是，为了节约内存资源，最好是做完操作之后，将session进行关闭 如果我们要学习除了django之外的web框架，几乎都离不开sqlachemy，所以，将sqlachemy学好收益还是非常大的","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://godhearing.cn/tags/SQLAlchemy/"}],"author":"天听"},{"title":"FastAPI学习之路-1","slug":"FastAPI学习之路-1","date":"2018-01-05T08:33:00.000Z","updated":"2018-01-05T08:33:00.000Z","comments":true,"path":"2018/01/05/fastapi-xue-xi-zhi-lu-1/","link":"","permalink":"http://godhearing.cn/2018/01/05/fastapi-xue-xi-zhi-lu-1/","excerpt":"","text":"FastAPI框架 FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，天生的支持异步(async) 它的主要优势在于，高效，几乎可与NodeJS和Go比肩的极高性能，自动生成交互式的文档，省去了开发之后写文档的步骤 安装好了话不多说，我们从头开始，边学边补充吧，争取细到不能再细 首先是安装，安装非常的简单 pip install fastapi pip install uvicorn 好啦，完成之后，做一个基础的API吧 创建py文件 from fastapi import FastAPI app = FastAPI() @app.get(\"/\") async def root(): return {\"message\": \"Hello World\"} from fastapi import FastAPI这是一个python类，提供了API的所有功能 app=FastAPI()将该类实例化 @app.get('/')定义路径操作装饰器，告诉FastAPI是正下方的功能负责处理该请求,可用的其他方式： @app:get() @app.post() @app:put() @app:delete() @app.options() @app:head() @app:patch() @app:trace() async def root():定义路径操作功能，带有async功能的函数 return {\"message\": \"Hello World\"{返回内容，默认是一个json格式，还可以返回dict、list、str、int等类型，还可以返回Pydantic模型 运行项目 在终端中输入uvicorn main:app --reload ``main：main.py文件 app：app = FastAPI()实例化的FastAPI对象 –reload`:在代码更改后重新启动服务器 第二种方法: 程序入口直接运行: import uvicorn if __name__ == '__main__': uvicorn.run(app=app) #还可以指定host，port等 声明类型 FastAPI的类型声明使用的是 : 做个例子: from typing import str def get_full_name(first_name,last_name): full_name = first_name.title() + \" \" + last_name.title() return full_name # 上面这是没加类型提示的 def get_full_name(first_name: str, last_name: str): # 这事加了类型提示的，添加了这个类型提示不会改变原来的运行结果 这个呢，大家把它当做习惯就好了，无论是否使用这个框架的人 ，都能看懂传参需要什么类型 Optional:可选类型，例：q:Optional[str] = Noneq类型可以为str也可以为None 不只是str，能够声明所有标准python类型 比如：int float bool bytes 或者是：dict list set tuple 统统可以使用typing库来声明 from typing import List 而，在List这些嵌套类型中，也可以声明其元素的类型，比如： def process_items(items:List[str]): Dict的key和value都可以各自声明 Optional:可选类型，例：q:Optional[str] = Noneq类型可以为str也可以为None 哦对，差点忘了，类，也是可以作为类型传递的 class Person: def __init__(selfm,name:str): self.name = name def get_person_name(one:Person): return one_person.name 交互式文档 交互式文档，在你启动的项目路径的docs下 127.0.0.1:8000/docs 在网页： 这就是交互式文档的所在了，注意交互二字 可以清楚的看到，传参，传的什么参，是否路径参数，一目了然 也可以进入redoc来进入标准的API文档 查询参数和字符串验证要注意，到目前，我们在函数参数中传的，都只是params，也就是路径上的那种 Query(查询参数)Query第一个参数用来定义默认值 可用于限制长度或者正则表达式 举个栗子: #q参数必须为字符串，默认值为None，如果为...,则这个参数必须给值，最小长度3，最大长度50 async def reds(q:str = Query(None,min_length=3,max+length=50) 注意!!!!此处只是参数，如果路径与参数一致，则不会生效 正则表达式 添加regex参数 async def reds(q:str = Query(None,min_length=3,max+length=50,regex='^nice')) 别名(alias)async def reds(q:str = Query(None,alias = 'asd')) 此时，参数名不再是q，而是asd，函数内部仍使用q 弃用参数(deprecated)# 我自己测试得出结论，只是提醒作用，并不是不可写入 # 写入Query中，可以改为弃用参数 deprecated = True 路径参数(path)首先导入path from fastapi import path #和Query使用方法类似，在参数定义之后添加 @app.get(\"/items/{item_id}\") async def read_items(item_id: int = Path(..., title=\"The ID of the item to get\")) #注意，由于是路径参数，所以不能为空，所以，应该用...声明 ge代表大于等于，le代表小于等于，gt代表大于，lt代表小于 #将*作为函数第一个参数的话，那么这个函数内所有参数都是关键字参数kwargs，即便没有默认值 举个例子，参数q不带默认值，而it参数带了path async def re(*,q:str,it:int = path(..., title=\"The ID of the item to get\")) 传递 * 作为函数的第一个参数。 Python 不会对该 * 做任何事情，但是它将知道之后的所有参数都应作为关键字参数（键值对），也被称为 kwargs，来调用。即使它们没有默认值。 pydanticFastAPI内置的数据模型 from pydantic import BaseModel class Item(BaseModel): name:str price:float tax:float = None from pydantic import EmailStr #还提供了EmailStr类型，邮件地址类型 类似Django内的Model 请求参数 参数中，如果参数=body(…)，则这个参数不会在url中，而是会出现在请求体中 栗子： class Item(BaseModel): name:str age:int price:float @app.post(\"/items/ss\") async def root(a:Item=Body(...,embed=True)): return {'item':a{ 嵌套请求体参数embed=True 参数A=body(...,embed=True) 则可以将参数A当做一个字典的键嵌套进请求体中，例： A = body(...,embed=True) { A:{ q:1 w:2 e:3 { { 不加embed效果: { q:1 w:2 e:3 { 响应模型 response_model=**参数，此参数写在路径中，而不是函数** response_model_exclude_unset = True，返回数据模型中有的字段，没有的字段不返回 response_model_exclude = {某字段{，返回时排除了某个字段 response_model_include = [某字段1,某字段2],返回时只包含其中的某字段 **是解包的意思 假如有多个BaseModel，数据类型有相同的情况下，可以使用.dict形式相互传输数据但是，前边加了** 响应状态码from fastapi import status #如果报错的话，则使用starlette引入 from starlette import status #可使用HTTP状态码 #错误处理： from fastapi import HTTPException #在路径中 @post(/itmes/, status_code=***)","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"}],"tags":[{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"}],"author":"天听"},{"title":"Git","slug":"git基本操作","date":"2017-12-31T12:27:00.000Z","updated":"2017-12-31T12:27:00.000Z","comments":true,"path":"2017/12/31/git-ji-ben-cao-zuo/","link":"","permalink":"http://godhearing.cn/2017/12/31/git-ji-ben-cao-zuo/","excerpt":"","text":"配置全局： git config --global user.name \"xxx\" git config --global user.email \"7618733+god_hearing@user.noreply.gitee.com\" 上传到仓库： 在码云或者github上创建仓库 仓库初始化(在本地)git init 新建readme.md文件:touch README.md 添加到本地缓冲区(.代表全部)git add . 此时，也可以查看缓冲区状态git status 添加注释git commit -m'注释信息' 一般，注释会使用+ - * 等符号作为前缀，代表增加，删除，改动 添加远程仓库地址git remote add 自定义缓存名(origin) 码云地址 提交代码到远程仓库git push origin master 如果提示报错，不妨先试验一下git push -u origin master -f 克隆到本地：git clone 地址 Git分支命令 在一个庞大的项目中，只依靠自己，是写不完所有的命令的，这时候，就需要集合众人之力，东拼西凑，将项目做出来，如果所有人都将自己的想法给上传到线上，那么，很大的几率会发生冲突，因为一个人一个想法，不可能全部都一致，这时候，就需要分支站出来了 所谓的分支，就像是支线任务，你需要将所有的支线的全部做完，然后汇总到主线任务，才叫完成。 假若每个人都在自己做自己的支线任务，互不影响，最后再汇总，这样不仅会减少错误，还会很方便的分工合作。 Git的分支，无论是在github还是gitee，使用方式是一样的 git pull：将所有的更新拉到本地仓库 git branch -r：查看所有远程分支 git checkout 分支名：切换分支 git branch：查看当前在哪个分支 git merge 分支名：本地合并 需要注意的是，在本地合并，一定要先确认在哪个分支下，merge后的分支名，是将哪个分支合并过来，一般会在主分支下，逐一合并子分支。 所以，首要任务，就是要保证子分支的代码没有任何的错误，然后再合并","categories":[{"name":"Git","slug":"Git","permalink":"http://godhearing.cn/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://godhearing.cn/tags/Git/"},{"name":"Markdown","slug":"Markdown","permalink":"http://godhearing.cn/tags/Markdown/"}],"author":"天听"},{"title":"递归","slug":"递归","date":"2017-11-24T12:56:00.000Z","updated":"2017-11-24T12:56:00.000Z","comments":true,"path":"2017/11/24/di-gui/","link":"","permalink":"http://godhearing.cn/2017/11/24/di-gui/","excerpt":"","text":"什么是递归 递归，简单来说，就是自己调用自己，指在函数的定义中调用函数自身的方法 调用分为直接调用和间接调用，直接调用是指在函数体中调用自身，间接调用是调用别的函数，而这些别的函数又调用函数本身。它主要是把大问题变成小问题，使得代码更加简洁。理解递归需要有一定的抽象能力。 使用递归时，需要注意的是： 递归就是在过程或函数里调用自身 必须有一个明确的递归结束条件，称为递归出口 切勿忘记递归的出口，否则就是无限循环 著名的德罗斯特效应就是递归的一种视觉形式 递归的过程分为，递归前进段，递归边界条件，递归返回段 递归的最大层次限制是998次，但是可以通过sys(System)模块下的setrecursionlimit方法来进行递归层次的扩大 递归函数的优点是定义简单，逻辑清晰，但是，过深的调用会导致栈溢出 递归实现高斯求和 代码实现高斯求和，如果是按照正常方式实现，那么就是这样的 def sum_number(n): total = 0 for i in range(1,n+1): total += i return total 非常的简单，但是这样做，流程是这样的，假设我们传参5 同样是计算5，我们用递归实现： def sum_numbers(n): if n &lt;= 0: return 0 return n+sum_numbers(n-1) 可以看到，我们并不是从1开始加到5，而是逆向的思维，从5开始，逐渐的递减，直到边界条件触发，也就是，如果为0，则停止递归 但是，这样会造成一个后果，就是内存的耗费问题，因为递归需要同时保存成千上百个调用记录，很容易发生”栈溢出”错误（stack overflow） 尾递归 对于尾递归来说，由于只存在一个调用记录，所以永远不会发生”栈溢出”错误。 如果按照上方的递归，计算需要保存n个调用记录，复杂度为O(n) 也就是： 5 + sum_numbers(4) 5 + (4 + sum_numbers(3)) 5 + (4 (3 + sum_numbers(2)) 5 + (4 (3 (2 + sum_numbers(1)) 在内存中，需要存储5次，非常的耗费 让我们用尾递归来实现一下，看一下有什么不同 def tail_sum(n,result=0): if n &lt;= 0: return result else: tail_sum(n-1,result+n) 我们给定义了一个最终数result，每次调用，不再存储到内存中，而是将结果存到result中，这样，只在运行结束的瞬间，存储一次，其他时间，只是存储了状态，如下： tail_sum(5,0) tail_sum(4,5) tail_sum(3,9) tail_sum(2,12) tail_sum(1,14) tail_sum(0,15) 再实现一下斐波那契 def feibonacci(n): if n &lt;= 2: return 1 else: return feibonacci(n-1) + feibonacci(n-2) 总结一下， 递归最核心的思想是：每一次递归，整体问题都要比原来减小，并且递归到一定层次时，要能直接给出结果！","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"生成器和迭代器","slug":"生成器和迭代器","date":"2017-11-20T07:33:00.000Z","updated":"2017-11-20T07:33:00.000Z","comments":true,"path":"2017/11/20/sheng-cheng-qi-he-die-dai-qi/","link":"","permalink":"http://godhearing.cn/2017/11/20/sheng-cheng-qi-he-die-dai-qi/","excerpt":"","text":"迭代器 迭代，是访问集合内元素的一种方式。要理解迭代器，首先要理解可迭代对象，所谓可迭代对象，通俗的 说就是可以被for循环遍历的对象就是可迭代对象 任何内置了iter方法的，都是可迭代的对象，例如，list、set、dict、tuple等都是可迭代对象 迭代的概念 相信大家都玩过游戏，每一次游戏更新迭代，都是新增了一些功能，调整了什么，迭代就是这个概念，在python中，上一次输出的结果为下一次输入的初始值，重复的过程就称为迭代。 就好像我们游戏更新一样，都是继上一次的版本为初始的情况下，增加一些新的东西，同理，每次迭代的结果是下一次迭代的初始值。 那么，问题来了，为什么要有迭代器，因为，对于没有索引的数据类型，必须提供一种不依赖索引的迭代方式，这个方式就是迭代器。 迭代器定义迭代器，就是可迭代对象执行iter方法，得到的结果就是迭代器，迭代器对象有next方法。它是一个状态的对象，它能在你调用你next()方法的时候返回容器中的下一个值，如果容器中没有更多的元素了，则会抛出StopIteration(停止迭代异常) 我们常用的for循环，其实它的本质就是： 利用iter函数得到函数 利用next函数依次取值 捕获异常 手写一个迭代器 class Csmt: def __init__(self): # 初始化可迭代对象和num self.name = [] self.num = 0 def add(self,na): # 添加元素 self.name.append(na) def __iter__(self): # 调用iter方法，返回自身 return self def __next__(self): # 如果 num小于name的长度 if self.num &lt; len(self.name): # 从0开始，取出可迭代对象中的元素 ret = self.name[self.num] # 同时，往后移一位 self.num += 1 # 返回 return ret else: # 没有元素，抛出异常 raise StopIteration c = Csmt() c.add('AAA') c.add('BBB') c.add('CCC') for i in c: print(i) 生成器 生成器是一个特殊的迭代器，它的实现更简单，yield，就是生成器实现next()方法的关键，它作为生成器执行的暂停恢复点，可以对yield表达式进行赋值，也可以将yield表达式的值返回 也就是说，yield是一个语法糖，内部实现了支持迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态 也就是说，一边循环，一边计算的机制，就是生成器。 为什么要有生成器假如你有一个海量的列表，你要读取中间的某些元素，那后面的绝大多数元素占用的空间都白白浪费了，如果使用生成器，在得到你需要的数据之后就停止，就没有必要创建完整的list，从而节省大量的空间。 生成器如何定义第一种方式 在函数中添加yield关键字，这个函数就是生成器 第二种方式 将列表推导式的[ ] 变成 ( ) 获取生成器的数据 利用for循环获取迭代器数据 利用while和异常捕获 利用list、tuple类型转换 生成器唤醒方式 由于生成器是函数暂停执行实现的，那么，我们唤醒生成器就需要使用next()来取值 或者使用__next__魔法方法 还可以使用send()函数，它可以将数据作为参数传递到生成器内部，需要注意的一点是，send不能作为第一次唤醒时使用 实现生成器a = (i for i in range(1,11)) print(a) for i in a: print(i) ''' &lt;generator object &lt;genexpr> at 0x000001BEBA62E448> 1，2，3，4，5，6，7，8，9，10 ''' 好啦，分享到此结束","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"}],"author":"天听"},{"title":"Redis中pipline的使用","slug":"redis的pipline使用","date":"2017-11-03T12:20:00.000Z","updated":"2017-11-03T12:20:00.000Z","comments":true,"path":"2017/11/03/redis-de-pipline-shi-yong/","link":"","permalink":"http://godhearing.cn/2017/11/03/redis-de-pipline-shi-yong/","excerpt":"","text":"redis发送数据原理 Redis是建立在TCP协议基础上的CS架构，客户端client对redis server采取请求响应的方式交互。 一般来说客户端从提交请求到得到服务器相应，需要传送两个tcp报文。 设想这样的一个场景，你要批量的执行一系列redis命令，例如执行100次get key，这时你要向redis 请求100次+获取响应100次。如果能一次性将100个请求提交给redis server，执行完成之后批量的获 取相应，只需要向redis请求1次，然后批量执行完命令，一次性结果，性能是不是会好很多呢？ 未使用pipeline执行N条命令 使用了pipeline执行N条命令 pipeline性能代码展示 from django_redis import get_redis_connection redis_client = get_redis_connection('default') '''普通方法执行''' for i in range(99999): redis_client.set(i,i) '''使用pipeline执行''' p1 = redis_client.pipeline()# 实例化一个pipeline对象 for i in range(99999): p1.set(i,i) p1.execute()","categories":[{"name":"Redis","slug":"Redis","permalink":"http://godhearing.cn/categories/Redis/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"redis","slug":"redis","permalink":"http://godhearing.cn/tags/redis/"}],"author":"天听"},{"title":"Django配置","slug":"Django配置","date":"2017-11-01T14:32:00.000Z","updated":"2017-11-01T14:32:00.000Z","comments":true,"path":"2017/11/01/django-pei-zhi/","link":"","permalink":"http://godhearing.cn/2017/11/01/django-pei-zhi/","excerpt":"","text":"前言 django作为python中的重型web框架，在开始学习django的时候，一定是非常痛苦的，连文件配置都搞不清，更不要提写功能了，今天这篇文章主要讲解一下django的配置问题 settings settings作为django的配置文件，其中的东西可以说是面面俱到的影响着整个项目，这个文件不需要创建，在你创建项目的时候，他自然会被创建，那让我们来看一下，这个settings中，可以配置什么 SECRET_KEY，每个django项目都会有自己的密钥，方便了使用jwt加密等 DEBUG，这个呢，是开发人员的DEBUG模式，也就是开发模式，正常情况下，在项目上线之前，要把这个改为False，否则一旦你网站全部的url泄露出去，那后果可是不堪设想 ALLOWED_HOSTS，这个是为了限定请求中的host值，以防止黑客构造包来发送请求，只有在列表中的host才能访问 注意，本人在这里强烈建议不要使用*通配符去配置，另外当DEBUG设置为False的时候，必须配置这个配置，否则会抛出异常。 后面所跟的属性值是一个字符串列表值，这个字符串列表值表示当下这个Django站点可以提供的host/domain(主机/域名)。这是一种安全措施，通过使用伪造的HTTP主机标头提交请求来防止攻击者中毒缓存并触发带有恶意主机链接的密码重置电子邮件，即使在许多看似安全的Web服务器配置下也是如此 INSTALLED_APPS，一般就是存放app和一些django的插件，比如跨域或者DRF MIDDLEWARE：听名字就知道，这是中间件层，一些自定义的中间件也要在这里注册 ROOT_URLCONF,URL是Web服务的入口，用户通过浏览器发送过来的任何请求，都是发送到一个指定的URL地址，然后被响应，决定哟啊使用的根URLconf模块，通常，这是ROOT_URLFONF设置的值，但是如果传入的HttpRequest对象具有urlconf属性(由中间件设置),则其值将被用于代替ROOT_URLCONF设置，通俗的讲，就是你可以自定义项目入口url是哪个文件 TEMPLATES：模板，在做一些前后端不分离的项目时，会使用到。 WSGI_APPLICATION：wsgi的配置，一般不需要动 DATABASES:数据库，这里可以连接关系型数据库，例如Mysql，PosrgrelSQL AUTH_PASSWORD_VALIDATORS：弱密码校验，这里会有四个自带的校验器，当然也可以自己写校验器，只需要按照下述格式添加进去就可以做统一校验 然后剩下的这些，都是本地化的一些配置 LANGUAGE_CODE = 'zh-hans' # 语言，这里改为了中文 TIME_ZONE = 'Asia/Shanghai' # 时区，改为亚洲上海即可 USE_I18N = True USE_L10N = True USE_TZ = False # 弃用格林威治时间 STATIC_ROOT，新增的，该目录下面的文件会被当成静态文件处理，与STATIC_ROOT搭配使用的还有STATIC_URL，一般默认采用/static/，用于你指定静态目录的URL。 STATICFILES_DIRS，制定了一个工程里面哪个目录存放了与这个工程相关的静态文件，这是一个列表 STATIC_URL = '/static/' STATICFILES_DIRS=[ os.path.join(BASE_DIR,'static') ]","categories":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"DRF操作","slug":"DRF操作","date":"2017-11-01T12:27:00.000Z","updated":"2017-11-01T12:27:00.000Z","comments":true,"path":"2017/11/01/drf-cao-zuo/","link":"","permalink":"http://godhearing.cn/2017/11/01/drf-cao-zuo/","excerpt":"","text":"1.DRF初始化DRF六中常用操作 1.认证 2.权限 3.限流 4.序列化 5.分页 6.版本 1.1安装DjangoRestFrameworkpip install djangorestframework==3.11.1 pip install django-filter==2.3.8 #过滤器 pip install markdown # markdown support for the browsable API 123 1.2在syl/settings.py中注册INSTALLED_APPS = [ 'django_filters', 'rest_framework' ] 1234 1.3 在settings.py中配置# 过滤器 # 1,安装 django-filter # 2,注册应用 # 3,配置settings, 在view里配置可过滤的字段 # 4,使用 查询字符串携带过滤信息 REST_FRAMEWORK = { # 文档报错： AttributeError: ‘AutoSchema’ object has no attribute ‘get_link’ # 用下面的设置可以解决 'DEFAULT_SCHEMA_CLASS': 'rest_framework.schemas.AutoSchema', # 默认设置是: # 'DEFAULT_SCHEMA_CLASS': 'rest_framework.schemas.openapi.AutoSchema', # 异常处理器 # 'EXCEPTION_HANDLER': 'user.utils.exception_handler', # Base API policies 'DEFAULT_RENDERER_CLASSES': [ 'rest_framework.renderers.JSONRenderer', 'rest_framework.renderers.BrowsableAPIRenderer', ], 'DEFAULT_PARSER_CLASSES': [ 'rest_framework.parsers.JSONParser', 'rest_framework.parsers.FormParser', 'rest_framework.parsers.MultiPartParser' ], # 1.认证器（全局） 'DEFAULT_AUTHENTICATION_CLASSES': [ 'rest_framework.authentication.SessionAuthentication', # 使用session时的认证器 'rest_framework.authentication.BasicAuthentication' # 提交表单时的认证器 ], #2.权限配置（全局）： 顺序靠上的严格 'DEFAULT_PERMISSION_CLASSES': [ # 'rest_framework.permissions.IsAdminUser', # 管理员可以访问 # 'rest_framework.permissions.IsAuthenticated', # 认证用户可以访问 # 'rest_framework.permissions.IsAuthenticatedOrReadOnly', # 认证用户可以访问, 否则只能读取 # 'rest_framework.permissions.AllowAny', # 所有用户都可以访问 ], #3.限流（防爬虫） 'DEFAULT_THROTTLE_CLASSES': [ 'rest_framework.throttling.AnonRateThrottle', 'rest_framework.throttling.UserRateThrottle', ], #3.1限流策略 'DEFAULT_THROTTLE_RATES': { 'user': '100/hour', # 认证用户每小时100次 'anon': '3/day', # 未认证用户每天能访问3次 {, 'DEFAULT_CONTENT_NEGOTIATION_CLASS': 'rest_framework.negotiation.DefaultContentNegotiation', 'DEFAULT_METADATA_CLASS': 'rest_framework.metadata.SimpleMetadata', 'DEFAULT_VERSIONING_CLASS': None, #4.分页（全局）：全局分页器, 例如 省市区的数据自定义分页器, 不需要分页 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', # 每页返回数量 'PAGE_SIZE': 10, # 默认 None #5.过滤器后端 'DEFAULT_FILTER_BACKENDS': [ 'django_filters.rest_framework.DjangoFilterBackend', # 'django_filters.rest_framework.backends.DjangoFilterBackend', 包路径有变化 ], #5.1过滤排序（全局）：Filtering 过滤排序 'SEARCH_PARAM': 'search', 'ORDERING_PARAM': 'ordering', 'NUM_PROXIES': None, #6.版本控制：Versioning 接口版本控制 'DEFAULT_VERSION': None, 'ALLOWED_VERSIONS': None, 'VERSION_PARAM': 'version', # Authentication 认证 # 未认证用户使用的用户类型 'UNAUTHENTICATED_USER': 'django.contrib.auth.models.AnonymousUser', # 未认证用户使用的Token值 'UNAUTHENTICATED_TOKEN': None, # View configuration 'VIEW_NAME_FUNCTION': 'rest_framework.views.get_view_name', 'VIEW_DESCRIPTION_FUNCTION': 'rest_framework.views.get_view_description', 'NON_FIELD_ERRORS_KEY': 'non_field_errors', # Testing 'TEST_REQUEST_RENDERER_CLASSES': [ 'rest_framework.renderers.MultiPartRenderer', 'rest_framework.renderers.JSONRenderer' ], 'TEST_REQUEST_DEFAULT_FORMAT': 'multipart', # Hyperlink settings 'URL_FORMAT_OVERRIDE': 'format', 'FORMAT_SUFFIX_KWARG': 'format', 'URL_FIELD_NAME': 'url', # Encoding 'UNICODE_JSON': True, 'COMPACT_JSON': True, 'STRICT_JSON': True, 'COERCE_DECIMAL_TO_STRING': True, 'UPLOADED_FILES_USE_URL': True, # Browseable API 'HTML_SELECT_CUTOFF': 1000, 'HTML_SELECT_CUTOFF_TEXT': \"More than {count{ items...\", # Schemas 'SCHEMA_COERCE_PATH_PK': True, 'SCHEMA_COERCE_METHOD_NAMES': { 'retrieve': 'read', 'destroy': 'delete' {, { 1.4创建user/serializer.py写序列化器# -*- coding: utf-8 -*- from rest_framework import serializers from user.models import User def address_validate(data): # 独立校验器 # raise serializer.ValidationError(‘请填写实际地址’) # 有错就抛出异常 # 没错就返回数据 return data class UserSerializer (serializers.ModelSerializer): # 1.独立校验器：重新设定字段，替掉模型中的设定，重新设定地址的长度最小为5 address = serializers.CharField(max_length=255,min_length=5,validators=[address_validate]) #2.单一字段验证，验证地址 def validate_address(self,data): if data == '测试': raise serializers.ValidationError('请填写实际地址') # 有错就抛出异常 return data # 没错返回结果 def validate_phone(self,data): # 不符合手机号格式 # raise serializer.ValidationError(\"手机号格式不正确\") model =self.root.Meta.model num = model.object.filter(phone=data).count() if num > 0: raise serializers.ValidationError('手机好已存在') return data # 3. 所以属性验证器 def validate(self,attrs): # attrs:{“user”:“zhangsan”,\"phone\":\"17563734847\",...{ # 所有属性验证器 # self.context中有request和view上下午 # attrs 是需要序列化的数据 # raise serializer.ValidationsError('xxx.错误') # 有问题报错 return attrs # 没问题返回数据 class Meta: model = User # fields = ('id') # 临时添加字段也需要写在这里 fields = '__all__' # 所有字段 # exclude = ['id'] # 排除id字段 read_only_fields = ('',) # 指定字段为 read_only, # 扩展address：extra_kwargs = {{ # 局部替换某些字段，或者新增设定 extra_kwargs = { 'address':{ 'min_length' :5 ,# 给地址增加最小长度限制 'default' :'默认测试地址', # 增加默认值 { { 2.DRF认证、权限、限流、分页、过滤、序列化 排序# -*- coding: utf-8 -*- from django.urls import include,path from user import views from rest_framework.routers import SimpleRouter,DefaultRouter # 自动生产路由方法，必须使用视图集 # router = SimpleRouter（） # 没有跟根路由 /user/ 无法识别 router = DefaultRouter() # 有跟路由 router.register(r'user',views.UserViewSet) # 配置路由 urlpatterns = [ path('index/',views.index), path('api-auth/',include('rest_framework.urls',namespace='res_framework')) # 认证地址 ] urlpatterns+= router.urls # 模块地址 2.2 编写user/views.pyfrom django.shortcuts import render from django.http import HttpResponse from django_filters.rest_framework import DjangoFilterBackend from rest_framework import viewsets from rest_framework.authentication import BasicAuthentication,SessionAuthentication from rest_framework.decorators import action from rest_framework.filters import OrderingFilter from rest_framework.permissions import AllowAny,IsAdminUser,IsAuthenticated,IsAuthenticatedOrReadOnly from rest_framework.response import Response from rest_framework.throttling import UserRateThrottle from rest_framework.pagination import PageNumberPagination from rest_framework.views import APIView from rest_framework.permissions import BasePermission,SAFE_METHODS from user.models import User from user.serializer import UserSerializer # Create your views here. def index(request): #需要认证才能访问的视图 return HttpResponse('HELLO') # 分页（局部）: 自定义分液器，局部 class PageNum(PageNumberPagination): # 查询字符串中代表每页返回数据量的参数名，默认值：None page_size_query_param = 'page_size' # 查询字符串中代表页码的参数名，有默认值：page # page_query_param = 'page' # 一页中最多的结果条数 max_page_size = 2 # 自定义权限(局部） class MyPermission(BasePermission): def has_permission(self, request, view): print(view.kwargs.get('pk'),request.user.id) '''判断用户对模型有没有访问权限''' # 任何用户对使用此类权限的视图都有访问权限 print(request) if request.user.is_superuser: # 管理员对用户模型有访问权限 return True elif view.kwargs.get('pk') == str(request.user.id): # 携带的id和用户的id相同时有访问权限 return True return False def has_object_permission(self, request, view, obj): '''获取单个数据时，判断用户对某个数据对象时否有访问权限''' if request.user.id == obj.id: return True return False class UserViewSet(viewsets.ModelViewSet): \"\"\" 完成产品的增删改查 \"\"\" queryset = User.objects.all() serializer_class = UserSerializer # 优先使用 get_serializer_class 返回的序列化器 # # 1.认证： 自定义认证类，自定义会覆盖全局配置 # authentication_classes = (BasicAuthentication,SessionAuthentication) # # 2.权限认证： 自定义权限类 # permission_classes = (MyPermission) #3.分页： 自定义分页器 覆盖全局配置 pagination_class = PageNum # 4.限流：自定义限流类 parser_classes = [UserRateThrottle] #5. 过滤： 指定过滤方法类，拍下方法类，一个或多个 filter_backends = (DjangoFilterBackend,OrderingFilter) # 同时支持过滤和排序 # 5.1 指定排序字段，不设置，排序功能不起效 ordering_fileds = ('date_joined','id') # ?ordering = -id # 5.2指定过过滤字段，不设置，过滤功能不起效 filter_fields = ('username','phone','is_active') #?username = tom&amp;phone=is_active=tur # # 根据不同的请求，获得不同的序列化器 # def get_serializer_class(self): # if self.action == 'unactived': # return UserUnActiveSerializer # else: # return UserSerializer","categories":[{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"}],"author":"天听"},{"title":"倒排索引与全文检索","slug":"倒排索引与全文检索","date":"2017-10-30T13:40:00.000Z","updated":"2017-10-30T13:40:00.000Z","comments":true,"path":"2017/10/30/dao-pai-suo-yin-yu-quan-wen-jian-suo/","link":"","permalink":"http://godhearing.cn/2017/10/30/dao-pai-suo-yin-yu-quan-wen-jian-suo/","excerpt":"","text":"倒排索引 一个未经处理的数据库中，一般是以文档ID作为索引，文档内容作为记录 而倒排索引指的是，将单词或记录作为索引，将文档ID作为记录，这样便可以方便地通过索引来查找到其所在的文档 例如： 简单来说，普通的查询检索是通过文档查找关键词，而倒排索引就是通过关键词找到文档 流程：将数据库中的结构化数据数据转换为非结构化数据 然后将非结构化数据转化为分词结构 Django使用haystack haystack是django的开源搜索框架，该框架支持 Solr,Elasticsearch,Whoosh, Xapian搜索引擎，不用更改代码，直接切换引擎，减少代码量。 搜索引擎使用Whoosh，这是一个由纯Python实现的全文搜索引擎，没有二进制文件等，比较小 巧，配置比较简单，当然性能自然略低。 中文分词Jieba，由于Whoosh自带的是英文分词，对中文的分词支持不是太好，故用jieba替换 whoosh的分词组件。 配置与使用：syl/settings.py 全文检索配置 '''1.注册app ''' INSTALLED_APPS = [ 'haystack', # haystack要放在应用的上面 ] '''2.模板路径 ''' TEMPLATES = [ { 'DIRS':[os.path.join(BASE_DIR,'templates')], { ] '''3.全文检索配置''' HAYSTACK_SEARCH_RESULTS_PER_PAGE = 15 # 搜索出多条数据时需要分页 HAYSTACK_CONNECTIONS = { 'default': { 'ENGINE': 'course.whoosh_cn_backend.MyWhooshEngine', 'PATH': os.path.join(BASE_DIR, 'whoosh_index'), # 指定倒排索引存放位置 { { # # ES引擎 # HAYSTACK_CONNECTIONS = { # 'default': { # 'ENGINE': 'haystack.backends.elasticsearch_backend.ElasticsearchSearchEngine', # 'URL': 'http://10.211.55.15:9200/', # Elasticsearch服务器ip地址，端口号固 定为9200 # 'INDEX_NAME': 'syl', # Elasticsearch建立的反向索引库的名称 # {, # { # 添加此项，当数据库改变时，会自动更新索引，非常方便 HAYSTACK_SIGNAL_PROCESSOR = 'haystack.signals.RealtimeSignalProcessor' 子应用下创建索引文件apps/course/search_indexes.py # apps/course/search_indexes.py # 文件名必须是 search_indexes.py from haystack import indexes from .models import Course # 修改此处，类名为模型类的名称+Index，比如模型类为GoodsInfo,则这里类名为GoodsInfoIndex(其 实可以随便写) class CourseIndex(indexes.SearchIndex, indexes.Indexable): \"\"\" Course索引类 \"\"\" # text为索引字段 # document = True，这代表haystack和搜索引擎将使用此字段的内容作为索引进行检索 # use_template=True 指定根据表中的那些字段建立索引文件的说明放在一个文件中 text = indexes.CharField(document=True, use_template=True) # 对那张表进行查询 def get_model(self): # 重载get_model方法，必须要有 \"\"\"返回建立索引的模型类\"\"\" return Course # 返回这个model # 建立索引的数据 def index_queryset(self, using=None): \"\"\"返回要建立索引的数据查询集\"\"\" # 这个方法返回什么内容，最终就会对那些方法建立索引，这里是对所有字段建立索引 return self.get_model().objects.all() 指定索引模板文件 templates/search/indexes/course/course_text.txt 创建文件路径命名必须这个规范：templates/search/indexes/应用名称/模型类名称 text.txt {{object.id{{ {{object.title{{ {{object.desc{{ 修改为jieba分词中的中文分析器 # apps/course/whoosh_cn_backend.py # 更换 text 字段的 分析方式, 变为jieba分词中的中文分析器 from haystack.backends.whoosh_backend import WhooshEngine, WhooshSearchBackend from whoosh.fields import TEXT from jieba.analyse import ChineseAnalyzer class MyWhooshSearchBackend(WhooshSearchBackend): def build_schema(self, fields): (content_field_name, schema) = super().build_schema(fields) # 指定whoosh使用jieba进行分词 schema._fields['text'] = TEXT(stored=True, analyzer=ChineseAnalyzer(), field_boost=fields.get('text').boost, sortable=True) return (content_field_name, schema) class MyWhooshEngine(WhooshEngine): backend = MyWhooshSearchBackend 课程全文检索接口视图函数 # course/views.py from syl import settings from django.core.paginator import InvalidPage, Paginator from haystack.forms import ModelSearchForm from django.http import JsonResponse # 如果settings.py中配置就是用settings中配置的，否则就每页15条 RESULTS_PER_PAGE = getattr(settings, 'HAYSTACK_SEARCH_RESULTS_PER_PAGE', 15) def course_index_search(request): query = request.GET.get('q', None) page = int(request.GET.get('page', 1))# 第几页 page_size = int(request.GET.get('page_size', RESULTS_PER_PAGE))# 每页多少条 if query: form = ModelSearchForm(request.GET, load_all=True) # 将查询条件传递给查询对 象 if form.is_valid(): results = form.search() # 查询出来的最终数据 else: results = [] else: return JsonResponse({\"code\": 404, \"msg\": 'No file found！', \"data\": []{) # 对结果集进行分页 paginator = Paginator(results, page_size) try: page = paginator.page(page) # 从分好的页中拿第几页 except InvalidPage: # 如果分页出错 return JsonResponse({\"code\": 404, \"msg\": 'No file found！', \"data\": []{) jsondata = [] for result in page.object_list: # 分页后的课程查询结果 data = { 'id': result.object.id, 'title': result.object.title, 'desc': result.object.desc, 'img': request.scheme + '://' + request.META['HTTP_HOST'] + result.object.img.url, # 'follower': result.object.follower, 'learner': result.object.learner, 'status': result.object.status, 'course_type': result.object.course_type.id { jsondata.append(data) result = { \"code\": 200, \"msg\": 'Search successfully！', \"data\": {\"count\": page.paginator.count, \"results\": jsondata{ { return JsonResponse(result) syl/urls.py添加路由 urlpatterns = [ path('search/', course_index_search), ] 命令创建倒排索引 python manage.py rebuild_index","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}],"author":"天听"},{"title":"浅谈进程和线程","slug":"浅谈进程和线程","date":"2017-09-15T02:58:00.000Z","updated":"2017-09-15T02:58:00.000Z","comments":true,"path":"2017/09/15/qian-tan-jin-cheng-he-xian-cheng/","link":"","permalink":"http://godhearing.cn/2017/09/15/qian-tan-jin-cheng-he-xian-cheng/","excerpt":"","text":"前言 相信大家在刚开始接触三程的时候会懵逼，进程和线程是操作系统的基本概念，但是他们比较抽象，不容易掌握。今天来细细的解释一下进程和线程，关于协程之后有时间会独自出一篇。 多进程 在谈到进程的时候，大家总说的一句话就是，进程是操作系统分配资源的最小单位，这句话理论上没有什么问题，但是对新手不友好。 首先我们一点点的说，从什么是进程开始吧 进程呢，就是一个将一个程序放入到内存中，从CPU和内存中申请进程资源，从而运行起来，这个运行起来的程序，就是进程，举个例子，进程就好像一座超市，而CPU呢，就好像给超市供电的发电站，假设发电站电力有限，只能供给一座超市，当一个超市正常运行时，其他的超市就得停，就是说，单个CPU只能一次执行一个任务。而一个CPU想要执行多个进程，就需要时间片轮询，什么意思呢，系统会分给每个进程一个时间片，在这个时间片内，是轮到这个进程执行的，然后多个进程轮流执行，由于轮的很快，人的肉眼根本分辨不出来，所以，他们看起来是一起执行的，但是实际上，同一时间只有一个进程在执行。这也就是我们常说的并发，同一时间点，只有一个进程在运行，他们只是因为快速的切换达到多任务的效果 每家超市的货物库存都是独立的，也就是说，每个进程的资源不是共享的。 多线程 线程是系统调度的最小单位，他运行在进程中，就拿上边的例子来举例，每个超市中的收银台，就好像一个个的线程，每个线程都是共享进程的资源的。 并且，他们是一起执行的，并不是轮流执行，这也就是所谓的并行。 谈到线程，就不得不说子线程，子线程就好像超市排队结账的人，他们都是要通过主线程。 如果你没有对线程进行阻塞，就会造成，你排着排着队，超市下班点到了，所有人都走了，你排队的人一脸懵逼，这时候，就需要守护线程登场，在主线程要结束的时候，先看看超市中还有没有人，等到没人了，主线程才结束。 由于系统中，线程的启动成本是比较高的，因为他涉及到与操作系统的交互，在这种情况下，使用线程池可以很好的提升性能，在系统启动时创建大量空闲的线程，程序只要将一个函数提交给线程池，线程池就会启动一个空闲的线程来执行它，当该函数执行结束后，该线程并不会死亡，而是再次返回到线程池中变成空闲状态，等待执行下一个函数。 此外，使用线程池可以有效的控制系统中并发线程的数量，当系统中包含有大量的并发线程时，会导致系统性能急剧下降，甚至可能会导致python解释器崩溃。 还有一点是，线程的执行是有顺序的，但是，它可以被其他线程抢占，当多个线程对同一个数据做修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对资源加锁。 补充，python解释器默认是Cpython，它上面有一个全局解释器锁GIL，它的作用是，限制多线程同时执行，保证同一时间内只有一个线程在执行，GIL并不是python的特性，注意，python和python解释器是两个概念。 所以python也被人诟病是伪多线程，而解决这个问题，可以使用协程或者多进程，也可以换其他解释器，比如PyPy。","categories":[{"name":"多任务","slug":"多任务","permalink":"http://godhearing.cn/categories/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"}],"tags":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"多任务","slug":"多任务","permalink":"http://godhearing.cn/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"}],"author":"天听"},{"title":"Restful风格接口","slug":"Restful风格接口","date":"2017-08-02T08:24:00.000Z","updated":"2017-08-02T08:24:00.000Z","comments":true,"path":"2017/08/02/restful-feng-ge-jie-kou/","link":"","permalink":"http://godhearing.cn/2017/08/02/restful-feng-ge-jie-kou/","excerpt":"","text":"什么是restful风格API 网络应用程序，分为前端和后端两个部分。当前的发展趋势，就是前端设备层出不穷（手机、平板、桌面电脑、其他专用设备……）。因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信。这导致API构架的流行，甚至出现API First的设计思想。RESTful API是目前比较成熟的一套互联网应用程序的API设计理论。 协议API与用户的通信协议，总是使用HTTPS协议 域名应该尽量将API部署在专用域名之下，如何确定API很简单，不会有进一步扩展，可以考虑在主域名下 版本应该将API的版本号放在URL中，另一个做法是，将版本号放在http头信息中，但不如放在url方便和直观 路径路径又称为终点，代表了API的具体网址，每个网址代表一种资源，所以网址中不能有动词，只能有名词，而且所用的名词往往和数据库的表格名相互对应，一般来说，每个数据库中的表都是同种记录的集合，所以API中的名词也要使用复数 举个栗子： # 有一个API提供商品信息 https://api.example.com/v1/goods https://api.example.com/v1/categorys HTTP动词对于资源的具体操作类型， 由HTTP动词表示 GET(SELECT):从服务器中取出资源(一项或多项) POST(CREATE):在服务器新建一个资源 PUT(UPDATE):在服务器更新资源(客户端提供改变后的完整资源) PATCH(UPDATE):在服务器更新资源(客户端提供改变的属性) DELETE(DELETE):从服务器删除资源 HEAD:获取资源的元数据 OPTIONS:获取信息，关于资源的哪些属性是客户端可以改变的 拿上边的商品信息举例子 GET /goods:列出所有商品 POST /goods:新建商品 GET /goods/ID:获取某个指定商品的信息 PUT /goods/ID:更新某个指定商品的信息(全部信息) PATCH /goods/ID:更新某个商品的信息(部分信息) DELETE /goods/ID:删除某个商品 GET /categorys/ID/good:列出商品类ID下所有商品 DELETE /categorys/ID/good/ID:删除某个指定商品类下的某个指定商品 过滤信息如果记录数量很多，服务器不可能都将它们返回给用户，API应该提供参数，过滤返回结果 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /categorys/ID/goods 与 GET /goods?categorys_id=ID 的含义是相同的。 状态码 服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词） 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 错误处理如果状态码是4xx，就应该向用户返回出错信息，一般来说，返回的信息中将error作为键名，出错信息作为键值即可 { \"error\":\"Invalid API key\" { 返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 面向文档RESTful API最好做到面向文档，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么，比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。 {\"link\": { \"rel\": \"collection https://www.example.com/goods\", \"href\": \"https://api.example.com/goods\", \"title\": \"List of goods\", \"type\": \"application/vnd.yourformat+json\" {{ 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。 所以，API有四个级别，分别为： level 0 ：面向过程，只是把HTTP当做一个传输的通道，没有把HTTP当做一种传输协议 level 1：面向资源，通过参数判断 level 2：面向标签，真正将HTTP作为了一种传输协议，最直观的一点就是Level2使用了HTTP动词，GET/PUT/POST/DELETE/PATCH....,这些都是HTTP的规范 level 3：面向文档,使用者只需要知道如何获取资源的入口，之后的每个URI都可以通过请求获得，无法获得就说明无法执行那个请求 所以，我们写API时，要朝着最高级别迈进，才能逐渐的成长起来","categories":[{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"天听"}],"categories":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/categories/Golang/"},{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/categories/FastAPI/"},{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/categories/Book/"},{"name":"python","slug":"python","permalink":"http://godhearing.cn/categories/python/"},{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/categories/Docker/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"cors","slug":"cors","permalink":"http://godhearing.cn/categories/cors/"},{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/categories/Nginx/"},{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/categories/Linux/"},{"name":"Casbin","slug":"Casbin","permalink":"http://godhearing.cn/categories/Casbin/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/categories/mysql/"},{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/categories/Vue/"},{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/categories/Hexo/"},{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/categories/axios/"},{"name":"文件","slug":"文件","permalink":"http://godhearing.cn/categories/%E6%96%87%E4%BB%B6/"},{"name":"bigchaindb","slug":"bigchaindb","permalink":"http://godhearing.cn/categories/bigchaindb/"},{"name":"PostgrelSQL","slug":"PostgrelSQL","permalink":"http://godhearing.cn/categories/PostgrelSQL/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/categories/Django/"},{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/categories/Celery/"},{"name":"spider","slug":"spider","permalink":"http://godhearing.cn/categories/spider/"},{"name":"PayPal","slug":"PayPal","permalink":"http://godhearing.cn/categories/PayPal/"},{"name":"Git","slug":"Git","permalink":"http://godhearing.cn/categories/Git/"},{"name":"Redis","slug":"Redis","permalink":"http://godhearing.cn/categories/Redis/"},{"name":"多任务","slug":"多任务","permalink":"http://godhearing.cn/categories/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://godhearing.cn/tags/Golang/"},{"name":"bug","slug":"bug","permalink":"http://godhearing.cn/tags/bug/"},{"name":"微服务","slug":"微服务","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"FastAPI","slug":"FastAPI","permalink":"http://godhearing.cn/tags/FastAPI/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://godhearing.cn/tags/SQLAlchemy/"},{"name":"Book","slug":"Book","permalink":"http://godhearing.cn/tags/Book/"},{"name":"python","slug":"python","permalink":"http://godhearing.cn/tags/python/"},{"name":"Linux","slug":"Linux","permalink":"http://godhearing.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://godhearing.cn/tags/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://godhearing.cn/tags/Redis/"},{"name":"算法","slug":"算法","permalink":"http://godhearing.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文检索","slug":"全文检索","permalink":"http://godhearing.cn/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://godhearing.cn/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"cors","slug":"cors","permalink":"http://godhearing.cn/tags/cors/"},{"name":"定时任务","slug":"定时任务","permalink":"http://godhearing.cn/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"Nginx","slug":"Nginx","permalink":"http://godhearing.cn/tags/Nginx/"},{"name":"测试","slug":"测试","permalink":"http://godhearing.cn/tags/%E6%B5%8B%E8%AF%95/"},{"name":"pycharm","slug":"pycharm","permalink":"http://godhearing.cn/tags/pycharm/"},{"name":"Casbin","slug":"Casbin","permalink":"http://godhearing.cn/tags/Casbin/"},{"name":"mysql","slug":"mysql","permalink":"http://godhearing.cn/tags/mysql/"},{"name":"Vue","slug":"Vue","permalink":"http://godhearing.cn/tags/Vue/"},{"name":"Hexo","slug":"Hexo","permalink":"http://godhearing.cn/tags/Hexo/"},{"name":"Django","slug":"Django","permalink":"http://godhearing.cn/tags/Django/"},{"name":"axios","slug":"axios","permalink":"http://godhearing.cn/tags/axios/"},{"name":"RSA","slug":"RSA","permalink":"http://godhearing.cn/tags/RSA/"},{"name":"文件","slug":"文件","permalink":"http://godhearing.cn/tags/%E6%96%87%E4%BB%B6/"},{"name":"bigchaindb","slug":"bigchaindb","permalink":"http://godhearing.cn/tags/bigchaindb/"},{"name":"MobaXerm","slug":"MobaXerm","permalink":"http://godhearing.cn/tags/MobaXerm/"},{"name":"PostgrelSQL","slug":"PostgrelSQL","permalink":"http://godhearing.cn/tags/PostgrelSQL/"},{"name":"WebHook","slug":"WebHook","permalink":"http://godhearing.cn/tags/WebHook/"},{"name":"Celery","slug":"Celery","permalink":"http://godhearing.cn/tags/Celery/"},{"name":"支付","slug":"支付","permalink":"http://godhearing.cn/tags/%E6%94%AF%E4%BB%98/"},{"name":"设计模式","slug":"设计模式","permalink":"http://godhearing.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"spider","slug":"spider","permalink":"http://godhearing.cn/tags/spider/"},{"name":"PayPal","slug":"PayPal","permalink":"http://godhearing.cn/tags/PayPal/"},{"name":"Git","slug":"Git","permalink":"http://godhearing.cn/tags/Git/"},{"name":"Markdown","slug":"Markdown","permalink":"http://godhearing.cn/tags/Markdown/"},{"name":"redis","slug":"redis","permalink":"http://godhearing.cn/tags/redis/"},{"name":"多任务","slug":"多任务","permalink":"http://godhearing.cn/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"}]}